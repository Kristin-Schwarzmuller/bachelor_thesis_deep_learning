{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 1.14.0 includes GPU support.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "if tf.test.is_gpu_available() & tf.test.is_built_with_cuda():\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    #from tensorflow.python.client import device_lib\n",
    "    #print(device_lib.list_local_devices())\n",
    "\n",
    "from keras import callbacks, backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from keras.layers.advanced_activations import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis=-1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anpassung Generatoren (zur Verwendung ImageDataGenerator mit Functional API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(e_gen, a_gen):\n",
    "    while True:\n",
    "        x_e, y_e = e_gen.next()\n",
    "        x_a, y_a = a_gen.next()\n",
    "        yield x_e, [y_e, y_a]\n",
    "        \n",
    "def prepare_test_data(e_gen, a_gen):\n",
    "    while True:\n",
    "        x_e, y_e = e_gen.next()\n",
    "        x_a, y_a = a_gen.next()\n",
    "        yield x_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Klassifizierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes():\n",
    "    df = pd.read_csv(_CSV_FILE)\n",
    "    df_shuffled = df.sample(frac=1, random_state=1)\n",
    "\n",
    "    df_shuffled['Elevation'] = df_shuffled['Elevation'].apply(lambda x: str(x))\n",
    "    df_shuffled['Azimuth'] = df_shuffled['Azimuth'].apply(lambda x: str(x))\n",
    "\n",
    "    df_train = df_shuffled[0:int(_TRAIN_SAMPLES*0.8 // _BATCH_SIZE * _BATCH_SIZE)]   \n",
    "    df_valid = df_shuffled.drop(df_shuffled.index[0:df_train.shape[0]])[0:int(_TRAIN_SAMPLES*0.2 // _BATCH_SIZE * _BATCH_SIZE)]\n",
    "    df_test  = df_shuffled[df_shuffled.shape[0] - _TEST_SAMPLES:df_shuffled.shape[0]]\n",
    "    \n",
    "    return df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    \n",
    "    with tf.device('/gpu:1'):\n",
    "\n",
    "        df_train, df_valid, df_test = create_dataframes()\n",
    "\n",
    "        if _USE_DATA_AUGMENTATION:\n",
    "            train_data_generator = ImageDataGenerator(\n",
    "                rescale=1./255\n",
    "                ,width_shift_range=0.1\n",
    "                ,height_shift_range=0.1\n",
    "                ,zoom_range=0.1\n",
    "                ,brightness_range=(0.25, 0.75)\n",
    "                ,fill_mode='nearest'\n",
    "            )\n",
    "        else:\n",
    "            train_data_generator = ImageDataGenerator(\n",
    "                rescale=1./255\n",
    "            )\n",
    "\n",
    "        valid_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        e_train_gen = train_data_generator.flow_from_dataframe(\n",
    "            dataframe=df_train,\n",
    "            directory=_IMAGE_DIR,\n",
    "            x_col='Filename',\n",
    "            y_col='Elevation',\n",
    "            class_mode='sparse',\n",
    "            target_size=(224, 224),\n",
    "            color_mode='rgb',\n",
    "            shuffle=True,\n",
    "            #shuffle=False,\n",
    "            seed=77,\n",
    "            batch_size=_BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        a_train_gen = train_data_generator.flow_from_dataframe(\n",
    "            dataframe=df_train,\n",
    "            directory=_IMAGE_DIR,\n",
    "            x_col='Filename',\n",
    "            y_col='Azimuth',\n",
    "            class_mode='sparse',\n",
    "            target_size=(224, 224),\n",
    "            color_mode='rgb',\n",
    "            shuffle=True,\n",
    "            #shuffle=False,\n",
    "            seed=77,\n",
    "            batch_size=_BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        e_val_gen = valid_data_generator.flow_from_dataframe(\n",
    "            dataframe=df_valid,\n",
    "            directory=_IMAGE_DIR,\n",
    "            x_col='Filename',\n",
    "            y_col='Elevation',\n",
    "            class_mode='sparse',\n",
    "            target_size=(224, 224),\n",
    "            color_mode='rgb',\n",
    "            shuffle=False,\n",
    "            seed=777,\n",
    "            batch_size=_BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        a_val_gen = valid_data_generator.flow_from_dataframe(\n",
    "            dataframe=df_valid,\n",
    "            directory=_IMAGE_DIR,\n",
    "            x_col='Filename',\n",
    "            y_col='Azimuth',\n",
    "            class_mode='sparse',\n",
    "            target_size=(224, 224),\n",
    "            color_mode='rgb',\n",
    "            shuffle=False,\n",
    "            seed=777,\n",
    "            batch_size=_BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        return e_train_gen, a_train_gen, e_val_gen, a_val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data():\n",
    "    \n",
    "    df_train, df_valid, df_test = create_dataframes()\n",
    "    \n",
    "    test_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    e_test_gen = test_data_generator.flow_from_dataframe(\n",
    "        dataframe=df_test,\n",
    "        directory=_IMAGE_DIR,\n",
    "        x_col='Filename',\n",
    "        y_col='Elevation',\n",
    "        class_mode='sparse',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        shuffle=False,\n",
    "        seed=777,\n",
    "        batch_size=_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    a_test_gen = test_data_generator.flow_from_dataframe(\n",
    "        dataframe=df_test,\n",
    "        directory=_IMAGE_DIR,\n",
    "        x_col='Filename',\n",
    "        y_col='Azimuth',\n",
    "        class_mode='sparse',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        shuffle=False,\n",
    "        seed=777,\n",
    "        batch_size=_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    return e_test_gen, a_test_gen, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Modell (Angepasst für Klassifizierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    lr = _LEARNING_RATE\n",
    "    \n",
    "    cnn = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    fc_input = Input(shape=(7, 7, 512))\n",
    "    fc = Flatten()(fc_input)\n",
    "\n",
    "    fc = Dense(units=_FIRST_NEURON, activation=None, kernel_initializer=glorot_uniform(seed=1))(fc)\n",
    "    fc = LeakyReLU(alpha = _LEAKY_ALPHA)(fc)\n",
    "    if _DROPOUT_RATE > 0.0:\n",
    "        fc = Dropout(rate=_DROPOUT_RATE)(fc)\n",
    "    \n",
    "    hidden_neuron_fraction = _FIRST_NEURON\n",
    "    for i in range(_HIDDEN_LAYERS):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc = Dense(units=hidden_neuron_fraction, activation=None, kernel_initializer=glorot_uniform(seed=1))(fc)\n",
    "        fc = LeakyReLU(alpha = 0.1)(fc)\n",
    "        if _DROPOUT_RATE > 0.0:\n",
    "            fc = Dropout(rate=_DROPOUT_RATE)(fc)\n",
    "        \n",
    "    elevation_output = Dense(18, activation='softmax', name='elevation_output')(fc)\n",
    "    azimuth_output = Dense(72, activation='softmax', name='azimuth_output')(fc)\n",
    "\n",
    "    fc = Model(inputs=fc_input, outputs=[elevation_output, azimuth_output])\n",
    "    \n",
    "    cnn_output = cnn.get_layer('block5_pool').output\n",
    "    full_output = fc(cnn_output)\n",
    "    full_model = Model(inputs=cnn.input, outputs=full_output)\n",
    "    \n",
    "    if _IS_FINETUNING:\n",
    "        lr = lr * 1e-2\n",
    "        full_model.load_weights(_MODEL_TO_LOAD)\n",
    "        for layer in cnn.layers[:15]:\n",
    "            layer.trainable = False\n",
    "            #print(layer.name, layer.trainable)\n",
    "    else:\n",
    "        cnn.trainable = False\n",
    "        for layer in cnn.layers[:25]:\n",
    "            layer.trainable = False\n",
    "            #print(layer.name, layer.trainable)\n",
    "    \n",
    "    full_model.compile(\n",
    "        optimizer=_OPTIMIZER(lr=lr)\n",
    "        ,loss='sparse_categorical_crossentropy'\n",
    "        ,loss_weights=[1., 1.]\n",
    "        ,metrics=['accuracy'])\n",
    "    \n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dateisystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RUN = '96'\n",
    "_LOSS = 'MSE'\n",
    "_DATASET_NAME = 'Dataset_2019-08-13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Vorsicht, Verzeichnis existiert bereits!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3a7b3309f0ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0m_LOG_DIR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'E:\\\\tkroiss\\\\{}_Klassifizierung_{}\\\\'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_RUN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_LOSS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LOG_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Vorsicht, Verzeichnis existiert bereits!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LOG_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Vorsicht, Verzeichnis existiert bereits!"
     ]
    }
   ],
   "source": [
    "_IMAGE_DIR = 'D:\\\\tkroiss\\\\Datasets\\\\{}\\\\'.format(_DATASET_NAME)\n",
    "_CSV_FILE = _IMAGE_DIR + 'images.csv'\n",
    "\n",
    "# _DEVICE = 'RTX' if(os.environ[\"CUDA_VISIBLE_DEVICES\"]=='0') else 'TITAN'\n",
    "\n",
    "_LOG_DIR = 'E:\\\\tkroiss\\\\{}_Klassifizierung_{}\\\\'.format(_RUN, _LOSS)\n",
    "assert(os.path.exists(_LOG_DIR)) == False, 'Vorsicht, Verzeichnis existiert bereits!'\n",
    "os.makedirs(_LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_SAMPLES = 10000\n",
    "_BATCH_SIZE = 64\n",
    "_LEARNING_RATE = 1e-3\n",
    "_DROPOUT_RATE = 0.25\n",
    "_FIRST_NEURON = 1024\n",
    "_HIDDEN_LAYERS = 2\n",
    "_OPTIMIZER = Adam\n",
    "_ACTIVATION = 'leakyrelu'\n",
    "_LEAKY_ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_IS_FINETUNING = False\n",
    "_USE_DATA_AUGMENTATION = False\n",
    "_MODEL_TO_LOAD = None\n",
    "_TRAIN_SAMPLES = 20000\n",
    "_NUM_EPOCHS = 1\n",
    "\n",
    "e_train_gen, a_train_gen, e_val_gen, a_val_gen = create_train_data()\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath=_LOG_DIR + 'Best_Weights_FC.hdf5'\n",
    "        ,monitor='val_accuracy'\n",
    "        ,verbose=1\n",
    "        ,save_best_only=False\n",
    "        ,mode='min')\n",
    "    \n",
    "csv_logger = callbacks.CSVLogger(\n",
    "        filename=_LOG_DIR + 'Logger_FC.csv'\n",
    "        ,separator=','\n",
    "        ,append=False)\n",
    "\n",
    "startTime = datetime.now()\n",
    "history = model.fit_generator(\n",
    "    generator=prepare_train_data(e_train_gen, a_train_gen)\n",
    "    ,steps_per_epoch=e_train_gen.n//e_train_gen.batch_size\n",
    "    ,validation_data=prepare_train_data(e_val_gen, a_val_gen)\n",
    "    ,validation_steps=e_val_gen.n//e_val_gen.batch_size\n",
    "    ,callbacks=[csv_logger, checkpointer]\n",
    "    ,epochs=_NUM_EPOCHS\n",
    ")\n",
    "print(\"Time taken:\", datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (Fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_IS_FINETUNING = True\n",
    "_USE_DATA_AUGMENTATION = True\n",
    "#_MODEL_TO_LOAD = _LOG_DIR + 'Best_Weights_FC.hdf5'\n",
    "_MODEL_TO_LOAD = _LOG_DIR + 'Best_Weights_CNN_Epoche_0-12.hdf5'\n",
    "_TRAIN_SAMPLES = 100000\n",
    "_NUM_EPOCHS = 188\n",
    "\n",
    "#del model\n",
    "\n",
    "e_train_gen, a_train_gen, e_val_gen, a_val_gen = create_train_data()\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "checkpointer = callbacks.ModelCheckpoint(\n",
    "    filepath=_LOG_DIR + 'Best_Weights_CNN.hdf5'\n",
    "    ,monitor='val_accuracy'\n",
    "    ,verbose=1\n",
    "    ,save_weights_only=False\n",
    "    ,save_best_only=False\n",
    "    ,mode='min'\n",
    ")\n",
    "    \n",
    "csv_logger = callbacks.CSVLogger(\n",
    "    filename=_LOG_DIR + 'Logger_CNN.csv'\n",
    "    ,separator=','\n",
    "    ,append=False\n",
    ")\n",
    "\n",
    "lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss'\n",
    "    ,factor=0.1\n",
    "    ,patience=10\n",
    "    ,verbose=1\n",
    "    ,mode='min'\n",
    "    ,min_delta=0.0001\n",
    ")\n",
    "\n",
    "early_stopper = callbacks.EarlyStopping(\n",
    "    monitor='val_loss'\n",
    "    ,min_delta=0\n",
    "    ,patience=15\n",
    "    ,verbose=1\n",
    "    ,mode='min'\n",
    ")\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    startTime = datetime.now()\n",
    "    history = model.fit_generator(\n",
    "        generator=prepare_train_data(e_train_gen, a_train_gen)\n",
    "        ,steps_per_epoch=e_train_gen.n//e_train_gen.batch_size\n",
    "        ,validation_data=prepare_train_data(e_val_gen, a_val_gen)\n",
    "        ,validation_steps=e_val_gen.n//e_val_gen.batch_size\n",
    "        ,callbacks=[checkpointer\n",
    "                    ,csv_logger\n",
    "                    ,lr_reducer\n",
    "                    ,early_stopper\n",
    "                   ]\n",
    "        ,epochs=_NUM_EPOCHS\n",
    "    )\n",
    "    print(\"Time taken:\", datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prognose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 validated image filenames belonging to 18 classes.\n",
      "Found 10000 validated image filenames belonging to 72 classes.\n",
      "156/156 [==============================] - 45s 286ms/step\n"
     ]
    }
   ],
   "source": [
    "_IS_FINETUNING = True\n",
    "_TRAIN_SAMPLES = 0\n",
    "_MODEL_TO_LOAD = _LOG_DIR + 'Best_Weights_CNN.hdf5'\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "e_test_gen, a_test_gen, df_test = create_test_data()\n",
    "\n",
    "predictions = model.predict_generator(generator=prepare_test_data(e_test_gen, a_test_gen)\n",
    "                                      ,steps=e_test_gen.n//e_test_gen.batch_size\n",
    "                                      ,verbose=1\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = e_test_gen.n//e_test_gen.batch_size*e_test_gen.batch_size\n",
    "df_result = pd.DataFrame({'Filename': df_test['Filename'][0:num_predictions],\n",
    "                          'Elevation_true': df_test['Elevation'][0:num_predictions],\n",
    "                          'Elevation_pred': [(np.argmax(elem) + 1)*5 for elem in predictions[0][:]],\n",
    "                          'Elevation_err': None,\n",
    "                          'Azimuth_true': df_test['Azimuth'][0:num_predictions],\n",
    "                          'Azimuth_pred': [(np.argmax(elem) + 1)*5 for elem in predictions[1][:]],\n",
    "                          'Azimuth_err': None,}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_ele_sum = 0\n",
    "err_azi_sum = 0\n",
    "\n",
    "for index, row in df_result.iterrows():\n",
    "    err_ele = abs((row['Elevation_pred'] - int(row['Elevation_true'])))\n",
    "    df_result.at[index, 'Elevation_err'] = err_ele\n",
    "    err_ele_sum = err_ele_sum + err_ele\n",
    "        \n",
    "    err_azi = min(abs(row['Azimuth_pred'] - int(row['Azimuth_true'])), abs(360 - abs(row['Azimuth_pred'] - int(row['Azimuth_true']))))\n",
    "    df_result.at[index, 'Azimuth_err'] = err_azi\n",
    "    err_azi_sum = err_azi_sum + err_azi\n",
    "    \n",
    "err_ele_avg = err_ele_sum/num_predictions\n",
    "err_azi_avg = err_azi_sum/num_predictions\n",
    "    \n",
    "print('Fehler Elevation: {}'.format('%.3f'%err_ele_avg))\n",
    "print('Fehler Azimut: {}'.format('%.2f'%err_azi_avg))\n",
    "\n",
    "df_result.to_csv(_LOG_DIR + 'Prognosen_ErrE_{}_ErrA_{}.csv'.format('%.2f'%err_ele_avg, '%.2f'%err_azi_avg), index=False)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
