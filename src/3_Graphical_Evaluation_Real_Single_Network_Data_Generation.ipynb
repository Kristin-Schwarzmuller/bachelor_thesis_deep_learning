{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation<a name = \"Top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links\n",
    "\n",
    "<ol>\n",
    "    <li><a href = #setup>Setup</a></li>\n",
    "    <li><a href = #store>Save File</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from numpy import array\n",
    "from numpy.random import seed\n",
    "import random\n",
    "\n",
    "import ntpath\n",
    "\n",
    "import copy\n",
    "import re\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s = 1):\n",
    "    os.environ['PYTHONHASHSEED']='0'\n",
    "\n",
    "    seed(s)\n",
    "    tf.random.set_seed(s)\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "s = 1\n",
    "set_seed(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(activation, leaky_alpha, dropout):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation=activation_layer, input_shape=(224,224,Global.num_image_channels)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(units = 2, activation=activation_layer)\n",
    "        #Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(activation, leaky_alpha, dropout_rate, first_neuron, hidden_layers, init):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape=(224,224,Global.num_image_channels), filters = 64, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", kernel_initializer = init, activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = first_neuron, kernel_initializer = init))\n",
    "    model.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        model.add(Dropout(rate = dropout_rate))\n",
    "        \n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(hidden_layers):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        model.add(Dense(units = hidden_neuron_fraction, kernel_initializer = init))\n",
    "        model.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            model.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    model.add(Dense(units = 2, kernel_initializer = init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_trained(activation, leaky_alpha, dropout, hidden_layer, first_neuron):\n",
    "    \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    if(Global.net_architecture == 'preVGG16'):\n",
    "        cnn = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "    elif(Global.net_architecture == 'preRESNET'):\n",
    "        cnn = ResNet50(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "    else:\n",
    "        print('ERROR NET SPELLED WRONG')\n",
    "    for layer in cnn.layers[:15]:\n",
    "        layer.trainable = False\n",
    "        #print(layer.name, layer.trainable)\n",
    "    \n",
    "    model.add(cnn)\n",
    "    \n",
    "    fc = Sequential()\n",
    "    fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512) \n",
    "\n",
    "    fc.add(Dense(units = first_neuron , kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.add(activation_layer)\n",
    "    if dropout > 0.0:\n",
    "        fc.add(Dropout(rate = dropout))\n",
    "\n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(hidden_layer):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.add(activation_layer)\n",
    "        if dropout > 0.0:\n",
    "            fc.add(Dropout(rate = dropout))\n",
    "\n",
    "    fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    \n",
    "    model.add(fc)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(activation, leaky_alpha):\n",
    "         \n",
    "    activation_layer = ReLU()\n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "        \n",
    "    model = Sequential()\n",
    "    cnn = ResNet50(include_top=False, weights=None, input_tensor=None, input_shape=(224, 224, Global.num_image_channels))\n",
    "    for layer in cnn.layers:\n",
    "        layer.trainable = True\n",
    "    x = cnn.output\n",
    "\n",
    "    x = AveragePooling2D((7, 7), padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units = 512, activation = activation_layer)(x)\n",
    "    x = Dense(units = 256, activation = activation_layer)(x)\n",
    "    x = Dense(units = 64, activation = activation_layer)(x)\n",
    "    x = Dense(units = 2, activation = activation_layer)(x)\n",
    "    model = Model(cnn.input, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolut_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenpipeline f체r Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data():\n",
    "    \n",
    "    df = pd.read_csv(Global.csv_file)\n",
    "    df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "    df_test = df_shuffled[df_shuffled.shape[0] - Global.num_sample : df_shuffled.shape[0]]\n",
    "    \n",
    "    test_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "\n",
    "    test_generator = test_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_test,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        color_mode= Global.image_channels,\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        seed=1,\n",
    "        shuffle = False,\n",
    "        batch_size = 1\n",
    "    )\n",
    "    \n",
    "    return test_generator, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsset-Typ nach String Converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell f체r Inferenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(parameters, prediction_runs = 1):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    test_generator, df_test = setup_data()\n",
    "\n",
    "    if(Global.net_architecture == 'ALEX'):\n",
    "        model = alexnet(parameters['activation'], parameters['leaky_alpha'], parameters['dropout_rate'])\n",
    "    elif(Global.net_architecture =='VGG16'):\n",
    "        model = vgg16(activation = parameters['activation'], \n",
    "                      leaky_alpha = parameters['leaky_alpha'], \n",
    "                      dropout_rate = parameters['dropout_rate'],\n",
    "                      first_neuron = parameters['first_neuron'],\n",
    "                      hidden_layers = parameters['hidden_layers'],\n",
    "                      init = parameters['initializer'])\n",
    "    elif(Global.net_architecture == 'RESNET'):\n",
    "        model = resnet(parameters['leaky_alpha'], parameters['leaky_alpha'])\n",
    "    elif(Global.net_architecture == 'preVGG16' or Global.net_architecture == 'preRESNET'):\n",
    "        if(Global.image_channels == 'rgba'):\n",
    "            print('Error: Please use rgb as input channel!!!!!!')\n",
    "            return null \n",
    "        model = pre_trained(parameters['activation'], parameters['leaky_alpha'], parameters['dropout_rate'], parameters['hidden_layers'], parameters['first_neuron'])\n",
    "\n",
    "    else:\n",
    "        print('Wrong net architecture!')\n",
    "        \n",
    "    model.load_weights(parameters['model_to_load'])\n",
    "    ##print('Using Optimizer: {} with Learning Rate: {}'.format(parameters['optimizer'][0], parameters['learning_rate']))\n",
    "    #model.compile(\n",
    "    #    optimizer = parameters['optimizer'][0](lr = parameters['learning_rate']),\n",
    "    #    loss = Global.loss_function, \n",
    "    #    metrics = get_Reduction_Metric(Global.reduction_metric)\n",
    "    #)\n",
    "    model.compile(\n",
    "        optimizer = parameters['optimizer'](lr = lr_normalizer(parameters['learning_rate'], parameters['optimizer'])), \n",
    "        loss = Global.loss_function, \n",
    "        metrics = get_reduction_metric(Global.reduction_metric)\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    test_generator.reset()\n",
    "\n",
    "    #print(\"Predicting using these values:\\nTest Data: {}\\nUsing Loss: {} on Dataset: {}\".format(, parameters['dataset_name']))\n",
    "    print('Net: {}'.format(Global.net_architecture))\n",
    "    print('Image Channels: {}'.format(Global.image_channels))\n",
    "    duration = 0    \n",
    "    \n",
    "    print('test_generator: {}'.format(test_generator))\n",
    "    steps = test_generator.n // test_generator.batch_size\n",
    "    \n",
    "    print('test_generator.n: {}'.format(test_generator.n))\n",
    "    print('test_generator.batch_size: {}'.format(test_generator.batch_size))\n",
    "    print('steps: {}'.format(steps))\n",
    "    for n in range(prediction_runs):\n",
    "        startTime = datetime.now()\n",
    "        #predictions = model.predict_generator(generator = test_generator, steps = test_generator.n // test_generator.batch_size, verbose = 0)\n",
    "        predictions = model.predict(x = test_generator, steps = test_generator.n // test_generator.batch_size, verbose = 0)\n",
    "        duration = (datetime.now() - startTime).total_seconds()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    time_per_prediction = duration / prediction_runs\n",
    "    time_per_image = time_per_prediction / len(predictions)\n",
    "    print(\"Prediction repeated {} times at a total time of {}sec. \\nAverage Time per Prediction: {} sec. Average Time per Image: {} sec\".format(prediction_runs, duration, time_per_prediction, time_per_image))\n",
    "\n",
    "    del model\n",
    "    return predictions, df_test, duration, time_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(dataframe, model_to_load):\n",
    "    params = copy.deepcopy(p)\n",
    "    params['model_to_load'] = model_to_load\n",
    "    params['optimizer'] = make_optimizer(\"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\")\n",
    "    params['learning_rate'] = dataframe.iloc[0]['lr']\n",
    "    params['first_neuron'] = dataframe.iloc[0]['first_neuron']\n",
    "    params['dropout_rate'] = dataframe.iloc[0]['dropout']\n",
    "    params['activation'] = dataframe.iloc[0]['activation']\n",
    "    params['leaky_alpha'] = dataframe.iloc[0]['leaky_alpha']\n",
    "    params['hidden_layers'] = dataframe.iloc[0]['hidden_layers']\n",
    "    params['initializer'] = make_initializer(dataframe.iloc[0]['initializer'])\n",
    "    # -------------------------------------------------------------\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radians $\\rightarrow$ Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_degree(angle_in_rad):\n",
    "    return angle_in_rad * 180 / np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree $\\rightarrow$ Radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_radians(angle_in_deg):\n",
    "    return angle_in_deg * np.pi / 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sph채rische $\\rightarrow$ Karthesische Koordinaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spheric_cartesian_polar(phi_d, theta_d):\n",
    "    x = np.sin(np.radians(90.0 - theta_d)) * np.cos(np.radians(phi_d))\n",
    "    y = np.sin(np.radians(90.0 - theta_d)) * np.sin(np.radians(phi_d))\n",
    "    z = np.cos(np.radians(90.0 - theta_d))\n",
    "    return array([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spheric_cartesian_elevation(phi_d, theta_d):\n",
    "    x = np.cos(np.radians(theta_d)) * np.cos(np.radians(phi_d))\n",
    "    y = np.cos(np.radians(theta_d)) * np.sin(np.radians(phi_d))\n",
    "    z = np.sin(np.radians(theta_d))\n",
    "    return array([x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorlength(vector):\n",
    "    return np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculated Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angular_error(deg_e_phi, deg_e_theta):\n",
    "    return np.degrees(np.arccos(np.cos(np.radians(deg_e_phi)) * np.cos(np.radians(deg_e_theta))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalarprodukt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDot(a, b):\n",
    "    dot = 0;\n",
    "    it = np.nditer(a, flags=['f_index'])\n",
    "    for x in it:\n",
    "        dot = dot + (x * b[it.index])\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_angular_error_elevation(predicted_deg_vector, true_deg_vector):    \n",
    "    c_predicted = spheric_cartesian_elevation(predicted_deg_vector[0], predicted_deg_vector[1])\n",
    "    c_true = spheric_cartesian_elevation(true_deg_vector[0], true_deg_vector[1])\n",
    "    \n",
    "    len_prediction = vectorlength(c_predicted)\n",
    "    len_true = vectorlength(c_true)\n",
    "    \n",
    "    cos_angle = np.dot(c_true, c_predicted) / len_prediction / len_true\n",
    "    \n",
    "    return abs(np.degrees(np.arccos(cos_angle)))\n",
    "\n",
    "def dot_angular_error_polar(predicted_deg_vector, true_deg_vector):\n",
    "    c_predicted = spheric_cartesian_polar(predicted_deg_vector[0], predicted_deg_vector[1])\n",
    "    c_true = spheric_cartesian_polar(true_deg_vector[0], true_deg_vector[1])\n",
    "    \n",
    "    len_prediction = vectorlength(c_predicted)\n",
    "    len_true = vectorlength(c_true)\n",
    "    \n",
    "    cos_angle = np.dot(c_true, c_predicted) / len_prediction / len_true\n",
    "    \n",
    "    return abs(np.degrees(np.arccos(cos_angle)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Initializer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_initializer(initializer):\n",
    "    \n",
    "    if initializer == '<tensorflow.python.ops.init_ops_v2.GlorotUniform object at 0x000001B31F19FDC8>':\n",
    "        return glorot_uniform(seed=s)\n",
    "    elif initializer == '<tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x000001B31F19F688>':\n",
    "        return glorot_normal(seed=s)\n",
    "    elif initializer == '<tensorflow.python.ops.init_ops_v2.VarianceScaling object at 0x000001B31F19F608>':\n",
    "        return he_uniform(seed=s)\n",
    "    elif initializer == '<tensorflow.python.ops.init_ops_v2.VarianceScaling object at 0x000001B31F19F788>':\n",
    "        return he_normal(seed=s)\n",
    "    elif initializer == '<tensorflow.python.ops.init_ops_v2.VarianceScaling object at 0x000001B31F19FC08>':\n",
    "        return lecun_uniform(seed=s)\n",
    "    elif initializer == '<tensorflow.python.ops.init_ops_v2.VarianceScaling object at 0x000001B36A6AF648>':\n",
    "        return lecun_normal(seed=s)\n",
    "    else:\n",
    "        assert(False, 'Initializer yet unknown - Please modify get_init to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduction_metric(metric):\n",
    "    \n",
    "    if metric == 'mean_absolut_error':\n",
    "        return [mean_absolut_error]\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normierte sph채rische Koordinaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_df, eval_predictions, save_dir = None, save_to_file = False):\n",
    "    num_predictions = eval_df.shape[0]\n",
    "    \n",
    "    df_result = pd.DataFrame({\n",
    "        'Filename': eval_df['Filename'][0:num_predictions],\n",
    "        'Elevation_true': eval_df['Elevation'][0:num_predictions],\n",
    "        'Elevation_pred': eval_predictions[:, 0],\n",
    "        'Elevation_err': None,\n",
    "        'Azimuth_true': eval_df['Azimuth'][0:num_predictions],\n",
    "        'Azimuth_pred': eval_predictions[:, 1],\n",
    "        'Azimuth_err': None,\n",
    "        'Angular_MAE': None,\n",
    "        'dot_angular_err_elevation': None,\n",
    "        'dot_angular_err_polar': None,\n",
    "    })\n",
    "\n",
    "    angular_calculated = ''\n",
    "\n",
    "    #df_result['Elevation_pred'] = eval_predictions[:, 0] #todo nach oben? \n",
    "    #df_result['Azimuth_pred'] = eval_predictions[:, 1]\n",
    "\n",
    "    for index, row in df_result.iterrows():\n",
    "        #print('=======================================================================')\n",
    "        predicted_azimuth = row['Azimuth_pred']\n",
    "        predicted_elevation = row['Elevation_pred']\n",
    "\n",
    "        true_azimuth = row['Azimuth_true']\n",
    "        true_elevation = row['Elevation_true']\n",
    "\n",
    "        # azimuth and elevation error\n",
    "        error_elevation = abs((predicted_elevation - true_elevation))\n",
    "        df_result.at[index, 'Elevation_err'] = error_elevation\n",
    "\n",
    "        error_azimuth = min(abs(predicted_azimuth - true_azimuth), abs(360 - abs(predicted_azimuth - true_azimuth)))\n",
    "        df_result.at[index, 'Azimuth_err'] = error_azimuth\n",
    "\n",
    "        # dot angular error\n",
    "        predicted_direction = array([predicted_azimuth, predicted_elevation])\n",
    "        true_direction = array([true_azimuth, true_elevation])\n",
    "\n",
    "        error_direction_elevation = dot_angular_error_elevation(predicted_direction, true_direction)\n",
    "        df_result.at[index, 'dot_angular_err_elevation'] = error_direction_elevation\n",
    "        error_direction_polar = dot_angular_error_polar(predicted_direction, true_direction)\n",
    "        df_result.at[index, 'dot_angular_err_polar'] = error_direction_polar\n",
    "\n",
    "\n",
    "        # Angular Mean Absolute Error\n",
    "        y_pred = K.constant(predicted_direction)\n",
    "        y_true = K.constant (true_direction)\n",
    "        mae = mean_absolut_error(y_true, y_pred)\n",
    "        df_result.at[index, 'Angular_MAE'] = mae\n",
    "        #print('=======================================================================')\n",
    "\n",
    "    error_elevation_avg = np.mean(df_result['Elevation_err'], axis = 0)\n",
    "    error_azimuth_avg = np.mean(df_result['Azimuth_err'], axis = 0)\n",
    "    \n",
    "    \n",
    "    # Box'n Whiskers Diagram Data - Angular Error\n",
    "    e_angular_median = np.quantile(df_result['dot_angular_err_elevation'], 0.5)\n",
    "    e_angular_lower_q = np.quantile(df_result['dot_angular_err_elevation'], 0.25)\n",
    "    e_angular_upper_q = np.quantile(df_result['dot_angular_err_elevation'], 0.75)\n",
    "    e_angular_min = np.amin(df_result['dot_angular_err_elevation'])\n",
    "    e_angular_max = np.amax(df_result['dot_angular_err_elevation'])\n",
    "    \n",
    "    max_angular_error = df_result.loc[df_result['dot_angular_err_elevation'] == e_angular_max]\n",
    "    max_angular_error_file = max_angular_error.iloc[0]['Filename']\n",
    "    max_angular_error_theta_true = max_angular_error.iloc[0]['Elevation_true']\n",
    "    max_angular_error_phi_true = max_angular_error.iloc[0]['Azimuth_true']\n",
    "    max_angular_error_theta_pred = max_angular_error.iloc[0]['Elevation_pred']\n",
    "    max_angular_error_phi_pred = max_angular_error.iloc[0]['Azimuth_pred']\n",
    "    \n",
    "    \n",
    "    # Box'n Whiskers Diagram Data - Mean Absolute Error\n",
    "    e_mae_median = np.quantile(df_result['Angular_MAE'], 0.5)\n",
    "    e_mae_lower_q = np.quantile(df_result['Angular_MAE'], 0.25)\n",
    "    e_mae_upper_q = np.quantile(df_result['Angular_MAE'], 0.75)\n",
    "    e_mae_min = np.amin(df_result['Angular_MAE'])\n",
    "    e_mae_max = np.amax(df_result['Angular_MAE'])\n",
    "    \n",
    "    max_mae_error = df_result.loc[df_result['Angular_MAE'] == e_mae_max]\n",
    "    max_mae_error_file = max_mae_error.iloc[0]['Filename']\n",
    "    max_mae_error_theta_true = max_mae_error.iloc[0]['Elevation_true']\n",
    "    max_mae_error_phi_true = max_mae_error.iloc[0]['Azimuth_true']\n",
    "    max_mae_error_theta_pred = max_mae_error.iloc[0]['Elevation_pred']\n",
    "    max_mae_error_phi_pred = max_mae_error.iloc[0]['Azimuth_pred']\n",
    "    \n",
    "    # Avg Angular Error\n",
    "    error_dot_angular_elevation_avg = np.mean(df_result['dot_angular_err_elevation'], axis = 0)\n",
    "    angular_variance = np.var(df_result['dot_angular_err_elevation'], axis = 0)\n",
    "    avg_angular_lower_mean = np.mean(df_result['dot_angular_err_elevation'][df_result['dot_angular_err_elevation'] < error_dot_angular_elevation_avg])\n",
    "    avg_angular_upper_mean = np.mean(df_result['dot_angular_err_elevation'][df_result['dot_angular_err_elevation'] > error_dot_angular_elevation_avg])\n",
    "    \n",
    "    # Avg MAE Error\n",
    "    avg_mae_mean = np.mean(df_result['Angular_MAE'])\n",
    "    avg_mae_lower_mean = np.mean(df_result['Angular_MAE'][df_result['Angular_MAE'] < avg_mae_mean])\n",
    "    avg_mae_upper_mean = np.mean(df_result['Angular_MAE'][df_result['Angular_MAE'] > avg_mae_mean])\n",
    "    \n",
    "\n",
    "    print('Durchschnnittlicher Fehler Elevation{}: {:.1f}'.format(angular_calculated, error_elevation_avg))\n",
    "    print('Durchschnnittlicher Fehler Azimut{}: {:.1f}'.format(angular_calculated, error_azimuth_avg))\n",
    "    print()\n",
    "    print('Durchschnnittlicher Winkelfehler{} (Elevation): {:.1f}'.format(angular_calculated, error_dot_angular_elevation_avg))\n",
    "    print('Varianz Winkelfehler{}: {:.1f}'.format(angular_calculated, angular_variance))\n",
    "    print()\n",
    "    print('Box-Whisker Angular: max - {:.1f}, upper - {:.1f}, median - {:.1f}, lower - {:.1f}, min - {:.1f}'.format(e_angular_max, e_angular_upper_q, e_angular_median, e_angular_lower_q, e_angular_min))\n",
    "    print('Maxium Angular Error on: {}, True ({:.1f}, {:.1f}), Pred ({:.1f}, {:.1f})'.format(max_angular_error_file, max_angular_error_phi_true, max_angular_error_theta_true, max_angular_error_phi_pred, max_angular_error_theta_pred))\n",
    "    print('Box-Whisker MAE: max - {:.1f}, upper - {:.1f}, median - {:.1f}, lower - {:.1f}, min - {:.1f}'.format(e_mae_max, e_mae_upper_q, e_mae_median, e_mae_lower_q, e_mae_min))\n",
    "    print('Maxium MAE Error on: {}, True ({:.1f}, {:.1f}), Pred ({:.1f}, {:.1f})'.format(max_mae_error_file, max_mae_error_phi_true, max_mae_error_theta_true, max_mae_error_phi_pred, max_mae_error_theta_pred))\n",
    "    print()\n",
    "    print('Avg Angular Range: lower - {:.1f}, avg - {:.1f}, upper - {:.1f}'.format(avg_angular_lower_mean, error_dot_angular_elevation_avg, avg_angular_upper_mean))\n",
    "    print('Avg MAE Range: lower - {:.1f}, avg - {:.1f}, upper - {:.1f}'.format(avg_mae_lower_mean, avg_mae_mean, avg_mae_upper_mean))\n",
    "    \n",
    "        \n",
    "    df_avg = pd.DataFrame({\n",
    "        'Avg_Elevation_Err': [error_elevation_avg],\n",
    "        'Avg_Azimuth_Err': [error_azimuth_avg],\n",
    "        'Avg_Angular_Err': [error_dot_angular_elevation_avg],\n",
    "        'Angular_Variance': [angular_variance],\n",
    "        'Avg_Inference_Time': [image_time],\n",
    "        'box_angular_medium': [e_angular_median],\n",
    "        'box_angular_lower': [e_angular_lower_q],\n",
    "        'box_angular_upper': [e_angular_upper_q],\n",
    "        'box_angular_min': [e_angular_min],\n",
    "        'box_angular_max': [e_angular_max],\n",
    "        'box_mae_medium': [e_mae_median],\n",
    "        'box_mae_lower': [e_mae_lower_q],\n",
    "        'box_mae_upper': [e_mae_upper_q],\n",
    "        'box_mae_min': [e_mae_min],\n",
    "        'box_mae_max': [e_mae_max],\n",
    "        'avg_angular_lower_mean': [avg_angular_lower_mean],\n",
    "        'avg_angular_upper_mean': [avg_angular_upper_mean],\n",
    "        'avg_mae_mean': [avg_mae_mean],\n",
    "        'avg_mae_lower_mean': [avg_mae_lower_mean],\n",
    "        'avg_mae_lower_mean': [avg_mae_upper_mean],\n",
    "        'max_angular_error_file': [max_angular_error_file],\n",
    "        'max_angular_error_theta_true': [max_angular_error_theta_true],\n",
    "        'max_angular_error_phi_true': [max_angular_error_phi_true],\n",
    "        'max_angular_error_theta_pred': [max_angular_error_theta_pred],\n",
    "        'max_angular_error_phi_pred': [max_angular_error_phi_pred],\n",
    "        'max_mae_error_file': [max_mae_error_file],\n",
    "        'max_mae_error_theta_true': [max_mae_error_theta_true],\n",
    "        'max_mae_error_phi_true': [max_mae_error_phi_true],\n",
    "        'max_mae_error_theta_pred': [max_mae_error_theta_pred],\n",
    "        'max_mae_error_phi_pred': [max_mae_error_phi_pred]\n",
    "    })\n",
    "\n",
    "    if(not os.path.exists(Global.evaluation_path)):\n",
    "        os.makedirs(Global.evaluation_path)\n",
    "    df_result.to_csv(Global.evaluation_path + 'Model-Testset-{}_Prediction_Results.csv'.format(Global.dataset), index=False)\n",
    "    df_avg.to_csv(Global.evaluation_path + 'Model-Testset-{}_Average_Results.csv'.format(Global.dataset), index=False)\n",
    "    \n",
    "    if save_to_file:\n",
    "        df_result.to_csv(save_dir + 'Prognosen.csv')#.format('%.2f'%error_elevation_avg, '%.2f'%error_azimuth_avg), index=False)\n",
    "        #df_result.to_csv(save_dir + 'Prognosen_ErrE_{}_ErrA_{}.csv'.format('%.2f'%error_elevation_avg, '%.2f'%error_azimuth_avg), index=False)\n",
    "    \n",
    "    return df_result, df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model <a name = \"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Setup\n",
    "<p><a href = #Top>Top</a> \n",
    "<p><a href = #store>Save File</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required format of parameters parameter for _model_predict_ (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'model_to_load':[],\n",
    "    'optimizer':[],\n",
    "    'learning_rate':[],\n",
    "    'first_neuron':[],\n",
    "    'dropout_rate':[],\n",
    "    'activation':[],\n",
    "    'leaky_alpha':[],\n",
    "    'hidden_layers':[],\n",
    "    'initializer':[],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct for global parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class global_parameter:\n",
    "    #========================================================\n",
    "    # just change this, everything else will automaticlly adjusted\n",
    "    \n",
    "    net_architecture = 'VGG16' # 'ALEX' vs 'VGG16' vs 'RESNET' vs 'preVGG16' vs 'preRESNET'\n",
    "    image_channels: str = 'rgb' # 'rgb' vs 'rgba'\n",
    "    mode: str = 'REAL' # 'SYNTH' vs 'REAL' \n",
    "    #======================================================== \n",
    "    \n",
    "    loss_function: str = 'mean_squared_error'\n",
    "    reduction_metric: str = 'mean_absolut_error'\n",
    "    monitor_value: str = 'val_mean_absolut_error'\n",
    "            \n",
    "    dataset: str = '201129_2031' #'201019_2253_final' vs 201129_2031'\n",
    "    device: str = 'RTX_2080_Ti'\n",
    "    data_augmentation: bool = True\n",
    "    image_dir: str = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(dataset)\n",
    "    num_sample: int = 10000\n",
    "    \n",
    "    # Default values for rgb -> later ajusted when rgba\n",
    "    num_image_channels: int = 3\n",
    "    csv_file_name: str = 'labels_ks_RGB.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    model_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(net_architecture, dataset, image_channels)\n",
    "    #results: str = '\\\\..\\\\{}_{}_Results.csv'.format(net_architecture, dataset)\n",
    "    #results_man: str = '\\\\..\\\\ALEX_201019_2253_final_Results_manual.csv'\n",
    "    \n",
    "    run: str = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    evaluation_path: str = model_dir + 'Evaluation_{}\\\\'.format(mode)\n",
    "    talos_results: str = 'Talos_Results_Fine_Idx_{}_{}.csv'.format(net_architecture, image_channels) \n",
    "    model_path: str = model_dir + 'CNN_{}_Model_and_Weights_{}.hdf5'.format(net_architecture, image_channels)\n",
    "    \n",
    "Global = global_parameter\n",
    "\n",
    "if(Global.image_channels == 'rgba'):\n",
    "    Global.num_image_channels = 4\n",
    "    Global.csv_file_name = 'labels_ks_RGBD.csv'\n",
    "    Global.csv_file = Global.image_dir + Global.csv_file_name\n",
    "    model_dir = '..\\\\output\\\\{}_{}_{}\\\\'.format(Global.net_architecture, Global.dataset, Global.image_channels)\n",
    "    #results: str = '\\\\..\\\\{}_{}_Results.csv'.format(Global.net_architecture, Global.dataset)\n",
    "    talos_results = 'Talos_Results_Fine_Idx_{}_{}.csv'.format(Global.net_architecture, Global.image_channels)\n",
    "    model_path = Global.model_dir + 'CNN_{}_Model_and_Weights_{}.hdf5'.format(Global.net_architecture, Global.image_channels)\n",
    "    print(Global.csv_file)\n",
    "    \n",
    "if(Global.net_architecture == 'preVGG16' or Global.net_architecture == 'preRESNET'):\n",
    "    n = 198 # 198 vs 210\n",
    "    Global.model_dir = '..\\\\output\\\\SYNTH_Regression_MSE\\\\201129_2031_Angular_Top_1_Custom-MAE_{}\\\\Synth_TD\\\\'.format(Global.net_architecture)\n",
    "    Global.talos_results = 'Talos_Results_Fine_Idx{}.csv'.format(n)\n",
    "    Global.model_path = Global.model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(n, 80000)\n",
    "    evaluation_path: str = Global.model_dir + 'Evaluation_{}_{}\\\\'.format(Global.mode, Global.run)\n",
    "        \n",
    "if(Global.mode == 'REAL'):\n",
    "    Global.image_dir = '..\\\\dataset_mm\\\\2020-05-28\\\\'\n",
    "    Global.csv_file = Global.image_dir + 'images_real.csv'\n",
    "    Global.num_sample = 861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test to see if RGBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "df2 = pd.read_csv(Global.csv_file)\n",
    "print(df2.head(5))\n",
    "img_nm = df2.iloc[0]['Filename']\n",
    "\n",
    "img = Image.open(Global.image_dir + img_nm)\n",
    "img.show()\n",
    "\n",
    "for i in range(224):\n",
    "    for j in range(224):\n",
    "        colors = img.getpixel((i,j))\n",
    "        print(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Trainiert mit: Synthetische Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = Global.model_dir + Global.talos_results\n",
    "dataframe = pd.read_csv(network_file)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_params = load_params(dataframe, Global.model_path)\n",
    "print(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    result_df, avg_df = evaluate(df_test, predictions,  save_dir = Global.evaluation_path, save_to_file = True)\n",
    "    mse_results.append([network_file, result_df, avg_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Global.evaluation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation <a name = \"store\">\n",
    "<p></a><a href = #Top>Top</a>\n",
    "<p><a href = #setup>Setup</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = '..\\\\output\\\\Regression\\\\{}__{}__{}_Graphical_Evaluation\\\\'.format(Global.net_architecture, Global.image_channels, Global.mode)\n",
    "\n",
    "if(not os.path.exists(eval_dir)):\n",
    "    os.makedirs(eval_dir)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(eval_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(eval_dir + '{}_{}_{}_Results.pickle'.format(Global.net_architecture, Global.image_channels, Global.mode), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mse_results, fp) # uncomment if you are REALLY sure to overwrite the file\n",
    "\n",
    "with open(eval_dir + '{}_{}_{}_Results.pickle'.format(Global.net_architecture, Global.image_channels, Global.mode), \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
