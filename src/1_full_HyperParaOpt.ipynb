{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kristins Alex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U git+https://github.com/keras-team/keras git+https://github.com/keras-team/keras-applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  1 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4645685237463011593\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 20264236482\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6039782007414038193\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\"\n",
      "]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "import talos as ta\n",
    "from talos.model import lr_normalizer\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess_input\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess_input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s = 1):\n",
    "    os.environ['PYTHONHASHSEED']='0'\n",
    "\n",
    "    seed(s)\n",
    "    tf.random.set_seed(s)\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    \n",
    "set_seed()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(activation, leaky_alpha, dropout):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation=activation_layer, input_shape=(224,224,Global.num_image_channels)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(units = 2, activation=activation_layer)\n",
    "        #Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_old(activation, leaky_alpha):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape=(224,224,Global.num_image_channels), filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    #model.add(Dense(units = 2, activation = \"softmax\"))\n",
    "    model.add(Dense(units = 2, activation=activation_layer))\n",
    "\n",
    "    #opt = Adam(lr = 0.001)\n",
    "    #model.compile(optimizer = opt, loss= keras.losses.categorical_crossentropy, metrics = ['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_fc_like_RESNET(activation, leaky_alpha):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    cnn = VGG16(include_top=False, weights=None, input_tensor=None, input_shape=(224, 224, Global.num_image_channels))\n",
    "    for layer in cnn.layers:\n",
    "        layer.trainable = True\n",
    "    x = cnn.output\n",
    "\n",
    "    x =  AveragePooling2D((7, 7), padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units = 512, activation = activation_layer)(x)\n",
    "    x = Dense(units = 256, activation = activation_layer)(x)\n",
    "    x = Dense(units = 64, activation = activation_layer)(x)\n",
    "    x = Dense(units = 2, activation = activation_layer)(x)\n",
    "    model = Model(cnn.input, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(activation, leaky_alpha, dropout_rate, first_neuron, hidden_layers):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    cnn = VGG16(include_top=False, weights=None, input_tensor=None, input_shape=(224, 224, Global.num_image_channels))\n",
    "    for layer in cnn.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model.add(cnn)\n",
    "\n",
    "    \n",
    "    fc = Sequential()\n",
    "    fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512)\n",
    "    \n",
    "    fc.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    print('Number Hidden Layers {}'.format(hidden_layers))\n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(hidden_layers):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    model.add(fc)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_he_uniform(activation, leaky_alpha):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape=(224,224,Global.num_image_channels), filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 4096, activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(Dense(units = 512, activation = activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "    model.add(Dense(units = 2, activation=activation_layer, kernel_initializer = he_uniform(seed = 1)))\n",
    "\n",
    "    return model\n",
    "\n",
    "kernel_initializer = he_uniform(seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet(activation, leaky_alpha):\n",
    "         \n",
    "    activation_layer = ReLU()\n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "        \n",
    "    model = Sequential()\n",
    "    cnn = MobileNetV2(include_top=False, weights=None, input_tensor=None, input_shape=(224, 224, Global.num_image_channels))\n",
    "    for layer in cnn.layers:\n",
    "        layer.trainable = True\n",
    "    x = cnn.output\n",
    "\n",
    "    x =  AveragePooling2D((7, 7), padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units = 512, activation = activation_layer)(x)\n",
    "    x = Dense(units = 256, activation = activation_layer)(x)\n",
    "    x = Dense(units = 64, activation = activation_layer)(x)\n",
    "    x = Dense(units = 2, activation = activation_layer)(x)\n",
    "    model = Model(cnn.input, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(activation, leaky_alpha):\n",
    "         \n",
    "    activation_layer = ReLU()\n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "        \n",
    "    model = Sequential()\n",
    "    cnn = ResNet50(include_top=False, weights=None, input_tensor=None, input_shape=(224, 224, Global.num_image_channels))\n",
    "    for layer in cnn.layers:\n",
    "        layer.trainable = True\n",
    "    x = cnn.output\n",
    "\n",
    "    x =  AveragePooling2D((7, 7), padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units = 512, activation = activation_layer)(x)\n",
    "    x = Dense(units = 256, activation = activation_layer)(x)\n",
    "    x = Dense(units = 64, activation = activation_layer)(x)\n",
    "    x = Dense(units = 2, activation = activation_layer)(x)\n",
    "    model = Model(cnn.input, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_net(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    K.clear_session()\n",
    "    set_seed()\n",
    "    \n",
    "    train_generator, valid_generator = create_data_pipline(params['batch_size'], params['samples'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    " \n",
    "    if(Global.net_architecture == 'ALEX'):\n",
    "        model = alexnet(params['activation'], params['leaky_alpha'], params['dropout'])\n",
    "    elif(Global.net_architecture =='VGG16'):\n",
    "        model = vgg16(params['activation'], params['leaky_alpha'], params['dropout'], params['first_neuron'], params['hidden_layers'])\n",
    "        #model = vgg16(params['activation'], params['leaky_alpha'])\n",
    "    elif(Global.net_architecture == 'RESNET'):\n",
    "        model = resnet(params['leaky_alpha'], params['leaky_alpha'])\n",
    "    elif(Global.net_architecture == 'MOBILENET'):\n",
    "        model = mobilenet(params['leaky_alpha'], params['leaky_alpha'])\n",
    "    else:\n",
    "        print('Wrong net architecture!')\n",
    "            \n",
    "    model.compile(\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer'])), \n",
    "        loss = Global.loss_function, \n",
    "        metrics = get_reduction_metric(Global.reduction_metric)\n",
    "    )\n",
    "    #print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        epochs = params['epochs'],\n",
    "        validation_data = valid_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        workers = 8\n",
    "    )\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolut_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduction_metric(metric):\n",
    "    \n",
    "    if metric == 'mean_absolut_error':\n",
    "        return [mean_absolut_error]\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct for global parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class global_parameter:\n",
    "    #========================================================\n",
    "    # just change this, everything else will automaticlly adjusted\n",
    "    \n",
    "    net_architecture = 'MOBILENET' # 'ALEX' vs 'VGG16' vs 'RESNET'\n",
    "    image_channels: str = 'rgb' # 'rgb' vs 'rgba'\n",
    "    #======================================================== \n",
    "    loss_function: str = 'mean_squared_error'\n",
    "    reduction_metric: str = 'mean_absolut_error'\n",
    "    monitor_value: str = 'val_mean_absolut_error'\n",
    " \n",
    "    dataset: str = '201129_2031'\n",
    "    device: str = 'RTX_2080_Ti'\n",
    "    data_augmentation: bool = False\n",
    "    num_image_channels: int = 3\n",
    "    image_dir: str = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(dataset)\n",
    "    \n",
    "    csv_file_name: str = 'labels_ks_RGB.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(net_architecture, dataset, image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(net_architecture, dataset)\n",
    "\n",
    "        \n",
    "Global = global_parameter\n",
    "\n",
    "if(Global.image_channels == 'rgba'):\n",
    "    Global.num_image_channels = 4\n",
    "    Global.csv_file_name: str = 'labels_ks_RGBD.csv'\n",
    "    Global.csv_file: str = Global.image_dir + Global.csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(Global.net_architecture, Global.dataset, Global.image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(Global.net_architecture, Global.dataset)\n",
    "        \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_pipline(batch_size, num_samples):\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(Global.csv_file)\n",
    "    df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "    df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "    df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "    \n",
    "    if Global.data_augmentation:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            #rescale = 1./255,\n",
    "            #preprocessing_function=vgg16_preprocess_input,\n",
    "            preprocessing_function=mobilenet_preprocess_input,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.5, 1.0), \n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            #rescale = 1./255\n",
    "            #preprocessing_function=vgg16_preprocess_input\n",
    "            preprocessing_function=mobilenet_preprocess_input\n",
    "        )\n",
    "        \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = True,\n",
    "        seed = 1,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "        \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        #rescale = 1./255\n",
    "        #preprocessing_function=vgg16_preprocess_input,\n",
    "        preprocessing_function=mobilenet_preprocess_input,\n",
    "    )\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = False,\n",
    "        seed = 1,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if(not os.path.exists(Global.target_dir)):\n",
    "    os.makedirs(Global.target_dir)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(Global.target_dir))\n",
    "\n",
    "device_file = open(Global.target_dir + '{}.txt'.format(Global.device), \"a+\")\n",
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "#\n",
    "#hyper_parameter = {\n",
    "#    'samples': [20000],\n",
    "#    'epochs': [1],\n",
    "#    'batch_size': [32, 64],\n",
    "#    'optimizer': [Adam],\n",
    "#    'lr': [1, 2, 5],\n",
    "#    'first_neuron': [1024, 2048, 4096],\n",
    "#    'dropout': [0.25, 0.50],\n",
    "#    'activation': ['leakyrelu', 'relu'],\n",
    "#    'hidden_layers': [0, 1, 2, 3, 4],\n",
    "#    'leaky_alpha': [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "#}\n",
    "\n",
    "hyper_parameter = {\n",
    "    'samples': [20000],\n",
    "    'epochs': [1],\n",
    "    'batch_size': [32],#, 64], \n",
    "    'optimizer': [Adam],\n",
    "    'lr': [1, 2],#, 5],\n",
    "    'first_neuron':  [1024, 2048, 4096],\n",
    "    'dropout': [0.25, 0.50],\n",
    "    'activation': ['relu', 'leakyrelu'],\n",
    "    'hidden_layers': [0, 1, 2, 4],#, 3, 4],\n",
    "    'leaky_alpha': [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/96 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 1114s 2s/step - loss: 22680.7227 - mean_absolut_error: 112.6748 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631or: 11 - ETA: 31:34 - loss: 22459.9960 - - ETA: 18s - loss - ETA: 15s - loss: 22490.8343 - mean_absolut\n",
      "Time taken: 0:18:35.718714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                             | 1/96 [18:39<29:32:42, 1119.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 110s 219ms/step - loss: 22659.1242 - mean_absolut_error: 112.6114 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:50.838587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▋                                                                             | 2/96 [20:34<21:21:57, 818.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 21640.3604 - mean_absolut_error: 101.2527 - val_loss: 21575.5164 - val_mean_absolut_error: 100.2231 mea - ETA: 16s - loss: 21382.4822 - mean_absolut\n",
      "Time taken: 0:01:52.524758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▍                                                                            | 3/96 [22:31<15:42:10, 607.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 108s 217ms/step - loss: 22676.4609 - mean_absolut_error: 112.6438 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:49.705107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▎                                                                           | 4/96 [24:25<11:44:53, 459.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 227ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:54.932745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████▏                                                                           | 5/96 [26:25<9:02:21, 357.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 231ms/step - loss: 22682.1844 - mean_absolut_error: 112.6575 - val_loss: 22697.2094 - val_mean_absolut_error: 112.66312\n",
      "Time taken: 0:01:57.122040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████                                                                           | 6/96 [28:26<7:10:10, 286.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 21668.4316 - mean_absolut_error: 101.6189 - val_loss: 1003145.2971 - val_mean_absolut_error: 769.8338\n",
      "Time taken: 0:01:56.603034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▊                                                                          | 7/96 [30:27<5:51:37, 237.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 226ms/step - loss: 21679.4166 - mean_absolut_error: 101.5682 - val_loss: 21576.3491 - val_mean_absolut_error: 100.2362\n",
      "Time taken: 0:01:54.698619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▋                                                                         | 8/96 [32:26<4:55:46, 201.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 227ms/step - loss: 21668.4194 - mean_absolut_error: 101.6288 - val_loss: 21571.7085 - val_mean_absolut_error: 100.2021\n",
      "Time taken: 0:01:54.773871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▌                                                                        | 9/96 [34:25<4:16:32, 176.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 226ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:54.585822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▏                                                                      | 10/96 [36:24<3:48:41, 159.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 112s 223ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:53.144462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████████                                                                      | 11/96 [38:22<3:28:13, 146.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 229ms/step - loss: 22680.7344 - mean_absolut_error: 112.6488 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:56.075596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▉                                                                     | 12/96 [40:23<3:14:42, 139.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 229ms/step - loss: 22643.3734 - mean_absolut_error: 112.5581 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631445 - mean_absolut_error: 112.2 - ETA: 31s - loss: 22407.7\n",
      "Time taken: 0:01:55.758071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████████▋                                                                    | 13/96 [42:23<3:04:33, 133.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 118s 236ms/step - loss: 21601.2128 - mean_absolut_error: 100.8947 - val_loss: 21571.1708 - val_mean_absolut_error: 100.2047\n",
      "Time taken: 0:01:59.327954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|███████████▌                                                                   | 14/96 [44:27<2:58:22, 130.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 111s 223ms/step - loss: 22682.1844 - mean_absolut_error: 112.6575 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:52.838338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████████████▎                                                                  | 15/96 [46:24<2:50:50, 126.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 231ms/step - loss: 22676.4609 - mean_absolut_error: 112.6438 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631s: 22347\n",
      "Time taken: 0:01:56.879434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▏                                                                 | 16/96 [48:25<2:46:39, 124.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 117s 234ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:58.375272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█████████████▉                                                                 | 17/96 [50:28<2:43:43, 124.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 117s 234ms/step - loss: 22680.7344 - mean_absolut_error: 112.6488 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:58.400495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|██████████████▊                                                                | 18/96 [52:31<2:41:04, 123.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 226ms/step - loss: 21625.6619 - mean_absolut_error: 101.2243 - val_loss: 21577.3593 - val_mean_absolut_error: 100.2508- mean_absolut\n",
      "Time taken: 0:01:54.355916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████▋                                                               | 19/96 [54:30<2:37:02, 122.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 231ms/step - loss: 22691.9797 - mean_absolut_error: 112.7100 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:56.995800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████▍                                                              | 20/96 [56:31<2:34:39, 122.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 231ms/step - loss: 21725.8327 - mean_absolut_error: 101.9828 - val_loss: 21572.7341 - val_mean_absolut_error: 100.2141\n",
      "Time taken: 0:01:56.781520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████▎                                                             | 21/96 [58:32<2:32:17, 121.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 231ms/step - loss: 21657.0673 - mean_absolut_error: 101.2877 - val_loss: 21572.2801 - val_mean_absolut_error: 100.2004\n",
      "Time taken: 0:01:56.998892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████████████▋                                                           | 22/96 [1:00:34<2:30:07, 121.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 22643.3734 - mean_absolut_error: 112.5581 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:55.410889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████████████▍                                                          | 23/96 [1:02:34<2:27:23, 121.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 231ms/step - loss: 22716.0617 - mean_absolut_error: 112.7380 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.116272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████████████▎                                                         | 24/96 [1:04:35<2:25:30, 121.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 233ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.904981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|████████████████████                                                         | 25/96 [1:06:38<2:23:53, 121.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 21683.5515 - mean_absolut_error: 101.5303 - val_loss: 21571.0978 - val_mean_absolut_error: 100.2080\n",
      "Time taken: 0:01:57.690270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|████████████████████▊                                                        | 26/96 [1:08:40<2:22:03, 121.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 231ms/step - loss: 22662.1391 - mean_absolut_error: 112.6134 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:56.852829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|█████████████████████▋                                                       | 27/96 [1:10:41<2:19:53, 121.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 226ms/step - loss: 22667.5250 - mean_absolut_error: 112.6288 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:54.318543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██████████████████████▍                                                      | 28/96 [1:12:40<2:16:53, 120.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 118s 235ms/step - loss: 21683.5329 - mean_absolut_error: 101.6735 - val_loss: 21574.7605 - val_mean_absolut_error: 100.2101\n",
      "Time taken: 0:01:59.217907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████▎                                                     | 29/96 [1:14:44<2:15:51, 121.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 22690.7867 - mean_absolut_error: 112.7058 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.687429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|████████████████████████                                                     | 30/96 [1:16:46<2:14:00, 121.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 22690.7867 - mean_absolut_error: 112.7058 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:56.650701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████████████████▊                                                    | 31/96 [1:18:47<2:11:48, 121.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 7709.7472 - mean_absolut_error: 62.8516 - val_loss: 5760.6718 - val_mean_absolut_error: 56.4967\n",
      "Time taken: 0:01:52.506802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████████████████▋                                                   | 32/96 [1:20:44<2:08:19, 120.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 119s 237ms/step - loss: 22664.8266 - mean_absolut_error: 112.6150 - val_loss: 22697.2094 - val_mean_absolut_error: 112.66311\n",
      "Time taken: 0:02:00.183736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|██████████████████████████▍                                                  | 33/96 [1:22:49<2:07:41, 121.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 233ms/step - loss: 22664.8266 - mean_absolut_error: 112.6150 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.786972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███████████████████████████▎                                                 | 34/96 [1:24:51<2:05:52, 121.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 21755.8029 - mean_absolut_error: 102.2976 - val_loss: 21573.2428 - val_mean_absolut_error: 100.1983\n",
      "Time taken: 0:01:55.325234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|████████████████████████████                                                 | 35/96 [1:26:51<2:03:13, 121.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 22667.5250 - mean_absolut_error: 112.6288 - val_loss: 22697.2094 - val_mean_absolut_error: 112.66311646 - mean_absolut_err\n",
      "Time taken: 0:01:57.665661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████▉                                                | 36/96 [1:28:53<2:01:32, 121.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 119s 238ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:02:00.523804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|█████████████████████████████▋                                               | 37/96 [1:30:58<2:00:32, 122.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 117s 234ms/step - loss: 22671.7188 - mean_absolut_error: 112.6425 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:58.359412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████▍                                              | 38/96 [1:33:01<1:58:33, 122.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 22643.3734 - mean_absolut_error: 112.5581 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631: 22848.0192 - mean_absolut_\n",
      "Time taken: 0:01:57.478885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████▎                                             | 39/96 [1:35:03<1:56:19, 122.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 233ms/step - loss: 7317.4758 - mean_absolut_error: 61.5419 - val_loss: 5759.8992 - val_mean_absolut_error: 56.4474t_error: 56.48\n",
      "Time taken: 0:01:57.846182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████                                             | 40/96 [1:37:06<1:54:15, 122.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 21697.2368 - mean_absolut_error: 101.8847 - val_loss: 21581.4902 - val_mean_absolut_error: 100.3009\n",
      "Time taken: 0:01:57.324752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████▉                                            | 41/96 [1:39:07<1:52:03, 122.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 112s 224ms/step - loss: 22667.5250 - mean_absolut_error: 112.6288 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:53.652112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████████████████████▋                                           | 42/96 [1:41:06<1:48:54, 121.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 21734.4448 - mean_absolut_error: 102.1274 - val_loss: 21573.3476 - val_mean_absolut_error: 100.1981\n",
      "Time taken: 0:01:55.797688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████████████████████▍                                          | 43/96 [1:43:06<1:46:44, 120.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 21611.3365 - mean_absolut_error: 100.8884 - val_loss: 21588.5798 - val_mean_absolut_error: 100.3675\n",
      "Time taken: 0:01:55.549475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████████████████████▎                                         | 44/96 [1:45:06<1:44:32, 120.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 118s 236ms/step - loss: 21698.6707 - mean_absolut_error: 101.7415 - val_loss: 21574.4189 - val_mean_absolut_error: 100.2038\n",
      "Time taken: 0:01:59.302202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████████████████████████████████████                                         | 45/96 [1:47:10<1:43:20, 121.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 22680.7344 - mean_absolut_error: 112.6488 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.456273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████████████████████▉                                        | 46/96 [1:49:12<1:41:26, 121.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 112s 224ms/step - loss: 21702.8393 - mean_absolut_error: 101.7226 - val_loss: 21572.8970 - val_mean_absolut_error: 100.2144\n",
      "Time taken: 0:01:53.254270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████████████████████▋                                       | 47/96 [1:51:10<1:38:25, 120.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 229ms/step - loss: 22680.7344 - mean_absolut_error: 112.6488 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:56.227151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████▌                                      | 48/96 [1:53:10<1:36:27, 120.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:56.751701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████▎                                     | 49/96 [1:55:12<1:34:36, 120.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 227ms/step - loss: 22680.7344 - mean_absolut_error: 112.6488 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:55.033232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████████████████████                                     | 50/96 [1:57:11<1:32:18, 120.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 229ms/step - loss: 22669.3938 - mean_absolut_error: 112.6347 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:55.764951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|████████████████████████████████████████▉                                    | 51/96 [1:59:11<1:30:16, 120.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 22667.5250 - mean_absolut_error: 112.6288 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:56.607018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████████████████████████▋                                   | 52/96 [2:01:12<1:28:25, 120.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 233ms/step - loss: 22636.5188 - mean_absolut_error: 112.5606 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.821135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|██████████████████████████████████████████▌                                  | 53/96 [2:03:15<1:26:49, 121.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 229ms/step - loss: 21612.5910 - mean_absolut_error: 100.9032 - val_loss: 21580.9224 - val_mean_absolut_error: 100.2947\n",
      "Time taken: 0:01:56.198935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████████████████████████▎                                 | 54/96 [2:05:16<1:24:43, 121.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 118s 235ms/step - loss: 21778.4986 - mean_absolut_error: 102.5038 - val_loss: 21571.7913 - val_mean_absolut_error: 100.2018\n",
      "Time taken: 0:01:59.145655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████                                 | 55/96 [2:07:19<1:23:14, 121.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 233ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.939724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|████████████████████████████████████████████▉                                | 56/96 [2:09:22<1:21:20, 122.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 111s 223ms/step - loss: 22680.7344 - mean_absolut_error: 112.6488 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:52.977985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████████████████████████████████████████████▋                               | 57/96 [2:11:19<1:18:25, 120.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 112s 224ms/step - loss: 22650.9273 - mean_absolut_error: 112.6217 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:53.339518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████▌                              | 58/96 [2:13:17<1:15:53, 119.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 112s 224ms/step - loss: 22683.0711 - mean_absolut_error: 112.6695 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:53.688798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|███████████████████████████████████████████████▎                             | 59/96 [2:15:15<1:13:35, 119.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 22677.3672 - mean_absolut_error: 112.6578 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:55.487812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████▏                            | 60/96 [2:17:15<1:11:42, 119.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 117s 235ms/step - loss: 21610.5446 - mean_absolut_error: 101.1289 - val_loss: 21575.1391 - val_mean_absolut_error: 100.2168\n",
      "Time taken: 0:01:58.851020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████▉                            | 61/96 [2:19:19<1:10:23, 120.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 231ms/step - loss: 21645.4741 - mean_absolut_error: 101.3431 - val_loss: 21571.5818 - val_mean_absolut_error: 100.2025\n",
      "Time taken: 0:01:57.048838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████▋                           | 62/96 [2:21:20<1:08:31, 120.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 225ms/step - loss: 22682.1844 - mean_absolut_error: 112.6575 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:54.105747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████████████████████████████▌                          | 63/96 [2:23:19<1:06:07, 120.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 112s 224ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:53.701139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|███████████████████████████████████████████████████▎                         | 64/96 [2:25:17<1:03:48, 119.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 231ms/step - loss: 22680.7344 - mean_absolut_error: 112.6488 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.134338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████████████████████████▏                        | 65/96 [2:27:19<1:02:07, 120.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 229ms/step - loss: 22636.5188 - mean_absolut_error: 112.5606 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:55.804079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████▉                        | 66/96 [2:29:19<1:00:07, 120.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 231ms/step - loss: 22667.5250 - mean_absolut_error: 112.6288 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.141207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████████████████████████████████████████████████████▏                       | 67/96 [2:31:21<58:19, 120.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 22650.9273 - mean_absolut_error: 112.6217 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631n_absolut_error:\n",
      "Time taken: 0:01:56.639398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████████████████████████████████████████████████████▉                       | 68/96 [2:33:22<56:22, 120.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 227ms/step - loss: 22677.3672 - mean_absolut_error: 112.6578 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:55.005981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|████████████████████████████████████████████████████████▊                      | 69/96 [2:35:21<54:11, 120.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 226ms/step - loss: 21699.0396 - mean_absolut_error: 101.5815 - val_loss: 21571.1140 - val_mean_absolut_error: 100.2052\n",
      "Time taken: 0:01:54.300750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|█████████████████████████████████████████████████████████▌                     | 70/96 [2:37:20<51:58, 119.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 225ms/step - loss: 21706.0892 - mean_absolut_error: 101.8205 - val_loss: 21574.5150 - val_mean_absolut_error: 100.2055\n",
      "Time taken: 0:01:54.140301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|██████████████████████████████████████████████████████████▍                    | 71/96 [2:39:19<49:49, 119.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 21679.4011 - mean_absolut_error: 101.5548 - val_loss: 21571.6062 - val_mean_absolut_error: 100.2110\n",
      "Time taken: 0:01:56.532706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████▎                   | 72/96 [2:41:20<47:59, 120.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 233ms/step - loss: 21703.9419 - mean_absolut_error: 101.9230 - val_loss: 21573.6241 - val_mean_absolut_error: 100.1976\n",
      "Time taken: 0:01:57.912561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████████████████████████████████                   | 73/96 [2:43:22<46:16, 120.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 21606.1743 - mean_absolut_error: 100.8743 - val_loss: 21588.9780 - val_mean_absolut_error: 100.37083s - loss: 21429.8356 - mean_absolut_\n",
      "Time taken: 0:01:57.368561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████████████████████████████████▉                  | 74/96 [2:45:24<44:23, 121.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 225ms/step - loss: 21663.4127 - mean_absolut_error: 101.4142 - val_loss: 1610214.0485 - val_mean_absolut_error: 952.4460\n",
      "Time taken: 0:01:54.151282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████▋                 | 75/96 [2:47:23<42:07, 120.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 117s 233ms/step - loss: 21663.4115 - mean_absolut_error: 101.3337 - val_loss: 21579.3409 - val_mean_absolut_error: 100.2764\n",
      "Time taken: 0:01:58.097743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|██████████████████████████████████████████████████████████████▌                | 76/96 [2:49:25<40:20, 121.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 21623.9729 - mean_absolut_error: 101.2850 - val_loss: 21572.2789 - val_mean_absolut_error: 100.2130\n",
      "Time taken: 0:01:56.535703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████▎               | 77/96 [2:51:26<38:20, 121.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 227ms/step - loss: 21687.6246 - mean_absolut_error: 101.5748 - val_loss: 21571.9844 - val_mean_absolut_error: 100.2122\n",
      "Time taken: 0:01:54.875524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████████████████████████████████████████████████████████████▏              | 78/96 [2:53:26<36:10, 120.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 21681.1259 - mean_absolut_error: 101.6294 - val_loss: 21574.3665 - val_mean_absolut_error: 100.2028\n",
      "Time taken: 0:01:57.684844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|█████████████████████████████████████████████████████████████████              | 79/96 [2:55:28<34:17, 121.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 229ms/step - loss: 22676.4609 - mean_absolut_error: 112.6438 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631loss: 22581.634\n",
      "Time taken: 0:01:56.201576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|█████████████████████████████████████████████████████████████████▊             | 80/96 [2:57:29<32:15, 120.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 225ms/step - loss: 21650.2071 - mean_absolut_error: 101.2799 - val_loss: 21571.1799 - val_mean_absolut_error: 100.2088\n",
      "Time taken: 0:01:54.210187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|██████████████████████████████████████████████████████████████████▋            | 81/96 [2:59:27<30:03, 120.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 21609.8845 - mean_absolut_error: 100.8718 - val_loss: 21588.6782 - val_mean_absolut_error: 100.368308.7170 - mean_absolu\n",
      "Time taken: 0:01:57.391654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|███████████████████████████████████████████████████████████████████▍           | 82/96 [3:01:29<28:10, 120.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 21714.3015 - mean_absolut_error: 102.0292 - val_loss: 21571.0556 - val_mean_absolut_error: 100.2073\n",
      "Time taken: 0:01:56.364324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████▎          | 83/96 [3:03:30<26:10, 120.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 233ms/step - loss: 22664.8266 - mean_absolut_error: 112.6150 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.910272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████▏         | 84/96 [3:05:33<24:15, 121.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 227ms/step - loss: 21635.5983 - mean_absolut_error: 101.2727 - val_loss: 21571.8467 - val_mean_absolut_error: 100.2016\n",
      "Time taken: 0:01:54.916435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████▉         | 85/96 [3:07:32<22:07, 120.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 227ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631 loss: 22799.5473 - mean_abs - ETA: 11s - lo\n",
      "Time taken: 0:01:55.188436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████▊        | 86/96 [3:09:32<20:04, 120.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 118s 236ms/step - loss: 22688.0797 - mean_absolut_error: 112.6937 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:59.401365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|███████████████████████████████████████████████████████████████████████▌       | 87/96 [3:11:36<18:13, 121.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 233ms/step - loss: 21608.1020 - mean_absolut_error: 100.8957 - val_loss: 21580.4257 - val_mean_absolut_error: 100.2892\n",
      "Time taken: 0:01:57.989267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|████████████████████████████████████████████████████████████████████████▍      | 88/96 [3:13:38<16:14, 121.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 113s 226ms/step - loss: 22667.5250 - mean_absolut_error: 112.6288 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:54.303621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████████████████████████████████████████████████████████████████████▏     | 89/96 [3:15:37<14:06, 120.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 119s 237ms/step - loss: 22671.7188 - mean_absolut_error: 112.6425 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:02:00.068325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|██████████████████████████████████████████████████████████████████████████     | 90/96 [3:17:42<12:12, 122.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 118s 235ms/step - loss: 21697.2517 - mean_absolut_error: 101.8732 - val_loss: 21571.2060 - val_mean_absolut_error: 100.2044\n",
      "Time taken: 0:01:59.247990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████▉    | 91/96 [3:19:45<10:12, 122.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 231ms/step - loss: 22682.1844 - mean_absolut_error: 112.6575 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.102519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████▋   | 92/96 [3:21:47<08:09, 122.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 22680.7344 - mean_absolut_error: 112.6488 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631 22558.8657 - me\n",
      "Time taken: 0:01:55.598734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|████████████████████████████████████████████████████████████████████████████▌  | 93/96 [3:23:47<06:05, 121.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 117s 233ms/step - loss: 21672.1334 - mean_absolut_error: 101.5717 - val_loss: 21577.4315 - val_mean_absolut_error: 100.2518\n",
      "Time taken: 0:01:58.172297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████████████████████████████████████████████████████████████████████████▎ | 94/96 [3:25:50<04:03, 121.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 21643.0621 - mean_absolut_error: 101.1580 - val_loss: 21575.0124 - val_mean_absolut_error: 100.2146\n",
      "Time taken: 0:01:56.783114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|██████████████████████████████████████████████████████████████████████████████▏| 95/96 [3:27:52<02:01, 121.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 116s 233ms/step - loss: 22680.7344 - mean_absolut_error: 112.6488 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:01:57.917240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 96/96 [3:29:54<00:00, 131.19s/it]\n"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, Global.num_image_channels, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "            t = ta.Scan(\n",
    "                x = dummy_x,\n",
    "                y = dummy_y,\n",
    "                model = gen_net,\n",
    "                params = hyper_parameter,\n",
    "                experiment_name = '{}'.format(Global.dataset),\n",
    "                #shuffle=False,\n",
    "                reduction_metric = Global.reduction_metric,\n",
    "                disable_progress_bar = False,\n",
    "                print_params = True,\n",
    "                clear_session = True,\n",
    "                save_weights = False\n",
    "            )\n",
    "        \n",
    "\n",
    "t.data.to_csv(Global.target_dir + Global.results, index = True)\n",
    "#t.data.to_csv(Global.target_dir + Global.results, index = True, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data.to_csv(Global.target_dir + Global.results, index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
