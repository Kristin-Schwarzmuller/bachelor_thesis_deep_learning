{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kristins Alex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  1 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1682735129922965593\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 20264236482\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17476102872684193506\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\"\n",
      "]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import talos as ta\n",
    "from talos.model import lr_normalizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)\n",
    "\n",
    "#gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "#session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(activation, leaky_alpha, dropout):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation=activation_layer, input_shape=(224,224,Global.num_image_channels)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(units = 2, activation=activation_layer)\n",
    "        #Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(activation, leaky_alpha):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape=(224,224,Global.num_image_channels), filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    #model.add(Dense(units = 2, activation = \"softmax\"))\n",
    "    model.add(Dense(units = 2, activation=activation_layer))\n",
    "\n",
    "    #opt = Adam(lr = 0.001)\n",
    "    #model.compile(optimizer = opt, loss= keras.losses.categorical_crossentropy, metrics = ['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_net(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data_pipline(params['batch_size'], params['samples'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    " \n",
    "    if(Global.net_architecture == 'ALEX'):\n",
    "        model = alexnet(params['activation'], params['leaky_alpha'], params['dropout'])\n",
    "    elif(Global.net_architecture =='VGG16'):\n",
    "        model = vgg16(params['activation'], params['leaky_alpha'])\n",
    "    else:\n",
    "        print('Wrong net architecture!')\n",
    "            \n",
    "    model.compile(\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer'])), \n",
    "        loss = Global.loss_function, \n",
    "        metrics = get_reduction_metric(Global.reduction_metric)\n",
    "    )\n",
    "    #print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        epochs = params['epochs'],\n",
    "        validation_data = valid_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        workers = 8\n",
    "    )\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolut_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduction_metric(metric):\n",
    "    \n",
    "    if metric == 'mean_absolut_error':\n",
    "        return [mean_absolut_error]\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct for global parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class global_parameter:\n",
    "    loss_function: str = 'mean_squared_error'\n",
    "    reduction_metric: str = 'mean_absolut_error'\n",
    "    monitor_value: str = 'val_mean_absolut_error'\n",
    "        \n",
    "    net_architecture = 'VGG16' # 'ALEX' vs 'VGG16'\n",
    "    \n",
    "    dataset: str = '201129_2031'\n",
    "    device: str = 'RTX_2080_Ti'\n",
    "    data_augmentation: bool = True\n",
    "    image_channels: str = 'rgb' # just change this, everything else will automaticlly adjusted\n",
    "    num_image_channels: int = 3\n",
    "    image_dir: str = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(dataset)\n",
    "    \n",
    "    csv_file_name: str = 'labels_ks_RGB.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(net_architecture, dataset, image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(net_architecture, dataset)\n",
    "\n",
    "        \n",
    "Global = global_parameter\n",
    "\n",
    "if(Global.image_channels == 'rgba'):\n",
    "    Global.num_image_channels = 4\n",
    "    csv_file_name: str = 'lables_ks_RGBD.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(net_architecture, dataset, image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(net_architecture, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_pipline(batch_size, num_samples):\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(Global.csv_file)\n",
    "    df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "    df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "    df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "    \n",
    "    if Global.data_augmentation:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.5, 1.0), \n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "        \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\output\\VGG16_201129_2031_rgb\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if(not os.path.exists(Global.target_dir)):\n",
    "    os.makedirs(Global.target_dir)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(Global.target_dir))\n",
    "\n",
    "device_file = open(Global.target_dir + '{}.txt'.format(Global.device), \"a+\")\n",
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "#\n",
    "#hyper_parameter = {\n",
    "#    'samples': [20000],\n",
    "#    'epochs': [1],\n",
    "#    'batch_size': [32, 64],\n",
    "#    'optimizer': [Adam],\n",
    "#    'lr': [1, 2, 5],\n",
    "#    'first_neuron': [1024, 2048, 4096],\n",
    "#    'dropout': [0.25, 0.50],\n",
    "#    'activation': ['leakyrelu', 'relu'],\n",
    "#    'hidden_layers': [0, 1, 2, 3, 4],\n",
    "#    'leaky_alpha': [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "#}\n",
    "\n",
    "hyper_parameter = {\n",
    "    'samples': [20000],\n",
    "    'epochs': [1],\n",
    "    'batch_size': [16, 32],# 64],\n",
    "    'optimizer': [Adam],\n",
    "    'lr': [1, 2, 5],\n",
    "    'first_neuron':  [1024, 2048, 4096],\n",
    "    'dropout': [0.25, 0.50],\n",
    "    'activation': ['leakyrelu', 'relu'],\n",
    "    'hidden_layers': [4], #[0, 1, 2, 3, 4],\n",
    "    'leaky_alpha': [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 1145s 1s/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█                                                                             | 1/72 [19:07<22:37:55, 1147.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:19:05.544419\n",
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▏                                                                            | 2/72 [22:26<16:46:56, 863.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:03:18.314988\n",
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:18.352719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▎                                                                           | 3/72 [25:46<12:43:33, 663.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:17.941511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▍                                                                           | 4/72 [29:05<9:54:23, 524.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▌                                                                          | 5/72 [32:25<7:57:04, 427.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:03:19.389093\n",
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▋                                                                         | 6/72 [35:45<6:34:48, 358.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:03:18.487683\n",
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:19.005596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████▊                                                                        | 7/72 [39:05<5:37:10, 311.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:18.892523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▉                                                                       | 8/72 [42:24<4:56:21, 277.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:19.724106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████                                                                      | 9/72 [45:45<4:27:26, 254.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████████▉                                                                    | 10/72 [49:06<4:06:25, 238.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:03:19.586393\n",
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████                                                                   | 11/72 [52:26<3:50:47, 227.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:03:19.248609\n",
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▏                                                                 | 12/72 [55:46<3:38:51, 218.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:03:18.816115\n",
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▎                                                                | 13/72 [59:06<3:29:49, 213.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:03:19.537606\n",
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:19.810915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|██████████████▉                                                              | 14/72 [1:02:27<3:22:37, 209.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:18.042225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████                                                             | 15/72 [1:05:46<3:16:07, 206.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:19.160904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████                                                            | 16/72 [1:09:07<3:10:55, 204.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:19.740754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████████████▏                                                          | 17/72 [1:12:27<3:06:28, 203.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:19.898307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████████████▎                                                         | 18/72 [1:15:48<3:02:24, 202.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:14.411308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|████████████████████▎                                                        | 19/72 [1:19:04<2:57:07, 200.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:13.872420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|█████████████████████▍                                                       | 20/72 [1:22:19<2:52:19, 198.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 385ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:12.994248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██████████████████████▍                                                      | 21/72 [1:25:33<2:47:47, 197.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 387ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:14.103166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████▌                                                     | 22/72 [1:28:48<2:43:56, 196.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:13.500963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████████████████▌                                                    | 23/72 [1:32:02<2:40:07, 196.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:13.734679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████████████████▋                                                   | 24/72 [1:35:17<2:36:32, 195.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:14.291387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████████████████▋                                                  | 25/72 [1:38:32<2:33:11, 195.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 24147.1185 - mean_absolut_error: 88.9624 - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:14.245562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████████████████▊                                                 | 26/72 [1:41:48<2:29:52, 195.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:14.802062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████▉                                                | 27/72 [1:45:04<2:26:41, 195.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 195s 389ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:15.002586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|█████████████████████████████▉                                               | 28/72 [1:48:20<2:23:33, 195.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:14.816667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████                                              | 29/72 [1:51:36<2:20:19, 195.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 387ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:14.075999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████                                             | 30/72 [1:54:51<2:16:55, 195.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 387ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:14.001550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|█████████████████████████████████▏                                           | 31/72 [1:58:06<2:13:33, 195.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:14.491636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|██████████████████████████████████▏                                          | 32/72 [2:01:21<2:10:19, 195.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 192s 385ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:12.772658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████████████████████▎                                         | 33/72 [2:04:35<2:06:44, 194.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 387ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:13.970460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████████████████████████████████████▎                                        | 34/72 [2:07:50<2:03:29, 195.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:13.821587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████████████████████▍                                       | 35/72 [2:11:05<2:00:13, 194.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: nan - mean_absolut_error: nan - val_loss: nan - val_mean_absolut_error: nan\n",
      "Time taken: 0:03:13.705154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████▌                                      | 36/72 [2:14:20<1:56:56, 194.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:18.170503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████▌                                     | 37/72 [2:17:39<1:54:27, 196.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6380.9697 - mean_absolut_error: 58.8335 - val_loss: 22750675.6673 - val_mean_absolut_error: 2403.7585\n",
      "Time taken: 0:03:18.232860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|████████████████████████████████████████▋                                    | 38/72 [2:20:58<1:51:42, 197.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 7482.6112 - mean_absolut_error: 70.2636 - val_loss: 8326.7837 - val_mean_absolut_error: 72.7942\n",
      "Time taken: 0:03:19.047868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████████████████████████▋                                   | 39/72 [2:24:19<1:48:54, 198.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6224.7992 - mean_absolut_error: 58.1412 - val_loss: 17139.7666 - val_mean_absolut_error: 94.0572\n",
      "Time taken: 0:03:18.687988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████████████████████▊                                  | 40/72 [2:27:38<1:45:53, 198.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6334.6981 - mean_absolut_error: 58.4622 - val_loss: 14320.1228 - val_mean_absolut_error: 85.5982\n",
      "Time taken: 0:03:19.054962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|███████████████████████████████████████████▊                                 | 41/72 [2:30:58<1:42:49, 199.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6355.8540 - mean_absolut_error: 58.4829 - val_loss: 7320.1317 - val_mean_absolut_error: 61.2846\n",
      "Time taken: 0:03:17.408273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|████████████████████████████████████████████▉                                | 42/72 [2:34:17<1:39:25, 198.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6155.2988 - mean_absolut_error: 57.8426 - val_loss: 18596.9648 - val_mean_absolut_error: 99.1993\n",
      "Time taken: 0:03:17.165219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████▉                               | 43/72 [2:37:35<1:36:01, 198.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6252.5059 - mean_absolut_error: 58.2717 - val_loss: 10448163.9028 - val_mean_absolut_error: 1726.2429\n",
      "Time taken: 0:03:18.415763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|███████████████████████████████████████████████                              | 44/72 [2:40:55<1:32:49, 198.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6616.9021 - mean_absolut_error: 61.4096 - val_loss: 8022.0726 - val_mean_absolut_error: 64.4501\n",
      "Time taken: 0:03:18.597311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████▏                            | 45/72 [2:44:14<1:29:36, 199.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6253.9399 - mean_absolut_error: 58.4961 - val_loss: 21093.5790 - val_mean_absolut_error: 109.6563\n",
      "Time taken: 0:03:17.463416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|█████████████████████████████████████████████████▏                           | 46/72 [2:47:33<1:26:13, 198.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 7400.0663 - mean_absolut_error: 70.1224 - val_loss: 15254.7396 - val_mean_absolut_error: 91.8131\n",
      "Time taken: 0:03:17.943588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████████████████████████████████████████████████▎                          | 47/72 [2:50:52<1:22:54, 198.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 7380.0933 - mean_absolut_error: 70.0498 - val_loss: 8178.7867 - val_mean_absolut_error: 72.2664\n",
      "Time taken: 0:03:18.427608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|███████████████████████████████████████████████████▎                         | 48/72 [2:54:11<1:19:39, 199.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6156.3643 - mean_absolut_error: 57.7921 - val_loss: 15292183.2774 - val_mean_absolut_error: 1932.4186\n",
      "Time taken: 0:03:17.181472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████████████████████████▍                        | 49/72 [2:57:30<1:16:14, 198.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 7416.4085 - mean_absolut_error: 70.3044 - val_loss: 13674.7109 - val_mean_absolut_error: 87.3703\n",
      "Time taken: 0:03:17.965170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|█████████████████████████████████████████████████████▍                       | 50/72 [3:00:49<1:12:56, 198.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 196s 196ms/step - loss: 7168.9707 - mean_absolut_error: 66.4496 - val_loss: 43421446.9834 - val_mean_absolut_error: 2807.6282\n",
      "Time taken: 0:03:16.052775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████▌                      | 51/72 [3:04:06<1:09:26, 198.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 7248.1538 - mean_absolut_error: 69.8056 - val_loss: 4854751.9686 - val_mean_absolut_error: 964.7303\n",
      "Time taken: 0:03:17.109181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████████████████▌                     | 52/72 [3:07:24<1:06:06, 198.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6505.3434 - mean_absolut_error: 59.1851 - val_loss: 2804265.8409 - val_mean_absolut_error: 1177.0636\n",
      "Time taken: 0:03:17.390611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████▋                    | 53/72 [3:10:43<1:02:49, 198.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6539.5866 - mean_absolut_error: 59.1053 - val_loss: 7671513.0716 - val_mean_absolut_error: 1278.9927\n",
      "Time taken: 0:03:16.789586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████▎                   | 54/72 [3:14:00<59:27, 198.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 383ms/step - loss: 6380.3903 - mean_absolut_error: 58.7863 - val_loss: 6158.9318 - val_mean_absolut_error: 57.8304\n",
      "Time taken: 0:03:11.831761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████████████████████████████████▎                  | 55/72 [3:17:13<55:42, 196.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 192s 383ms/step - loss: 7411.2430 - mean_absolut_error: 70.1000 - val_loss: 8415.3431 - val_mean_absolut_error: 73.1465\n",
      "Time taken: 0:03:12.065399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████▍                 | 56/72 [3:20:26<52:09, 195.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 382ms/step - loss: 7424.5937 - mean_absolut_error: 70.2614 - val_loss: 6995.1196 - val_mean_absolut_error: 69.1810\n",
      "Time taken: 0:03:11.405757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|██████████████████████████████████████████████████████████████▌                | 57/72 [3:23:39<48:39, 194.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 382ms/step - loss: 7012.4635 - mean_absolut_error: 64.3960 - val_loss: 14544.9418 - val_mean_absolut_error: 63.5634\n",
      "Time taken: 0:03:11.313772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|███████████████████████████████████████████████████████████████▋               | 58/72 [3:26:51<45:15, 193.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 192s 384ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:12.649902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████████████████████████████████████████████████████████████▋              | 59/72 [3:30:05<42:00, 193.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 190s 381ms/step - loss: 8378.9927 - mean_absolut_error: 61.8426 - val_loss: 19120.1352 - val_mean_absolut_error: 108.4846\n",
      "Time taken: 0:03:10.912789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|█████████████████████████████████████████████████████████████████▊             | 60/72 [3:33:17<38:40, 193.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 382ms/step - loss: 6327.0403 - mean_absolut_error: 58.4749 - val_loss: 6040.3821 - val_mean_absolut_error: 57.9638\n",
      "Time taken: 0:03:11.324256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████▉            | 61/72 [3:36:30<35:23, 193.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 192s 384ms/step - loss: 8166.8109 - mean_absolut_error: 70.8801 - val_loss: 7499.0457 - val_mean_absolut_error: 70.6529\n",
      "Time taken: 0:03:12.455107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████           | 62/72 [3:39:43<32:12, 193.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 381ms/step - loss: 8705.4664 - mean_absolut_error: 68.2609 - val_loss: 5922.9169 - val_mean_absolut_error: 57.0991\n",
      "Time taken: 0:03:11.139334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████▏         | 63/72 [3:42:55<28:56, 192.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 383ms/step - loss: 7641.0535 - mean_absolut_error: 70.7658 - val_loss: 6973.8590 - val_mean_absolut_error: 69.1610\n",
      "Time taken: 0:03:11.937493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|██████████████████████████████████████████████████████████████████████▏        | 64/72 [3:46:08<25:43, 192.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 382ms/step - loss: 6467.8924 - mean_absolut_error: 58.6606 - val_loss: 10393.7109 - val_mean_absolut_error: 72.2236\n",
      "Time taken: 0:03:11.534173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████████████████████████████████████▎       | 65/72 [3:49:21<22:30, 192.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 192s 383ms/step - loss: 23402.8815 - mean_absolut_error: 112.5089 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:12.059094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|████████████████████████████████████████████████████████████████████████▍      | 66/72 [3:52:34<19:17, 192.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 192s 383ms/step - loss: 6593.9290 - mean_absolut_error: 59.2100 - val_loss: 6630.8990 - val_mean_absolut_error: 61.0261\n",
      "Time taken: 0:03:12.137310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████████████████████████████████████████████████████████████████████▌     | 67/72 [3:55:47<16:05, 193.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 383ms/step - loss: 7333.4545 - mean_absolut_error: 69.9343 - val_loss: 6982.8482 - val_mean_absolut_error: 69.1773\n",
      "Time taken: 0:03:11.762290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|██████████████████████████████████████████████████████████████████████████▌    | 68/72 [3:59:00<12:51, 192.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 192s 383ms/step - loss: 193264914.7218 - mean_absolut_error: 165.3570 - val_loss: 29481.5068 - val_mean_absolut_error: 76.7182\n",
      "Time taken: 0:03:11.977328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████▋   | 69/72 [4:02:13<09:39, 193.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 192s 384ms/step - loss: 6512.9250 - mean_absolut_error: 58.7970 - val_loss: 6644.6850 - val_mean_absolut_error: 60.3690\n",
      "Time taken: 0:03:12.225361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|████████████████████████████████████████████████████████████████████████████▊  | 70/72 [4:05:27<06:26, 193.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 382ms/step - loss: 6606.6004 - mean_absolut_error: 58.9816 - val_loss: 5816.9313 - val_mean_absolut_error: 56.5364\n",
      "Time taken: 0:03:11.681677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████████████████████████████████████████████████████████████████████████▉ | 71/72 [4:08:40<03:13, 193.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 5, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 192s 384ms/step - loss: 7916.1551 - mean_absolut_error: 65.4649 - val_loss: 5969.2841 - val_mean_absolut_error: 58.5732\n",
      "Time taken: 0:03:12.276798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 72/72 [4:11:53<00:00, 209.91s/it]\n"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    \n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = gen_net,\n",
    "            params = hyper_parameter,\n",
    "            experiment_name = '{}'.format(Global.dataset),\n",
    "            #shuffle=False,\n",
    "            reduction_metric = Global.reduction_metric,\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = True,\n",
    "            save_weights = False\n",
    "        )\n",
    "        \n",
    "\n",
    "#t.data.to_csv(Global.target_dir + Global.results, index = True)\n",
    "t.data.to_csv(Global.target_dir + Global.results, index = True, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
