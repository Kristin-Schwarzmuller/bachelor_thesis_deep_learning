{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Training on GPU 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links <a name = \"Top\"></a>\n",
    "\n",
    "<ol>\n",
    "<li><a href = #setup>Begin Training</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Conda Environment: tf_ks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "print('Current Conda Environment: {}'.format(os.environ['CONDA_DEFAULT_ENV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  1 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10117361832465605259\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 20264236482\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5435260313151150117\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum für Training-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TrainingSet(Enum):\n",
    "    SYNTHETIC = 1\n",
    "    REAL = 2\n",
    "    MIXED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "\n",
    "* <i>SSD</i>, falls genug Speicher auf SSD im SymLink <i>fast_output</i> verfügbar ist\n",
    "* <i>HDD</i>, falls möglicherweise zu wenig SSD-Speicher verfügbar ist $\\rightarrow$ <i>output</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class OutputDirectory(IntEnum):\n",
    "    HDD = 0\n",
    "    SSD = 1\n",
    "    \n",
    "output_path = ['output', 'fast_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Label_Type into suitable label names.\n",
    "$\\Rightarrow$ Angular / Normalized $\\rightarrow$ ['Elevation', 'Azimuth']\n",
    "\n",
    "$\\Rightarrow$ Stereographic $\\rightarrow$ ['S_x', 'S_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Label_Names(label_type):\n",
    "    if label_type == 'Angular' or label_type == 'Normalized':\n",
    "        return ['Elevation', 'Azimuth']\n",
    "    elif label_type == 'Stereographic':\n",
    "        return ['S_x', 'S_y']\n",
    "    else:\n",
    "        assert(True, 'LabelType Invalid')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reduction_Metric(metric):\n",
    "    \n",
    "    if metric == 'custom_mae':\n",
    "        return [custom_mae]\n",
    "    elif metric == 'tf.keras.metrics.MeanAbsoluteError()':\n",
    "        return [tf.keras.metrics.MeanAbsoluteError()]\n",
    "    elif metric == 'circular_mae':\n",
    "        return [circular_mae]\n",
    "    elif metric == 'mean_squared_error':\n",
    "        return ['mean_squared_error']\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'keras.optimizers.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Nadam'>\":\n",
    "        return Nadam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Adagard'>\":\n",
    "        return Adagard\n",
    "    elif optimizer == \"<class 'keras.optimizers.RMSprop'>\":\n",
    "        return RMSprop\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsset-Typ nach String Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_to_string(ts):\n",
    "    if ts == TrainingSet.SYNTHETIC:\n",
    "        return 'Synth'\n",
    "    elif ts == TrainingSet.REAL:\n",
    "        return 'Real'\n",
    "    elif ts == TrainingSet.MIXED:\n",
    "        return 'Mixed'\n",
    "    else:\n",
    "        print('Unknown TrainingSet')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(batch_size, num_samples, label_type):\n",
    "    # if Block für synthetische Daten, um nur auf realen Daten zu trainieren _USE_SYNTHETIC_TRAIN_DATA\n",
    "    # 1. lege df_train und df_valid als leere Liste an\n",
    "    # 2. If-block um Zeile df = ... bis df_valid\n",
    "    \n",
    "    if trainingset == TrainingSet.SYNTHETIC:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "    elif trainingset == TrainingSet.MIXED:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train_real = df_shuffled_real[0: int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid_real = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train_real.shape[0]])\n",
    "        df_train = df_train.drop(df_train.index[df_train.shape[0] - df_train_real.shape[0] : df_train.shape[0]])\n",
    "        df_valid = df_valid.drop(df_valid.index[df_valid.shape[0] - df_valid_real.shape[0] : df_valid.shape[0]])\n",
    "        df_train = df_train.append(df_train_real)\n",
    "        df_valid= df_valid.append(df_valid_real)\n",
    "    \n",
    "    elif trainingset == TrainingSet.REAL: # Add check for num_samples, once the real dataset increases\n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train = df_shuffled_real[0 : int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train.shape[0]])\n",
    "        \n",
    "    else:\n",
    "        print('Create_Data :: should not have reached here')\n",
    "        \n",
    "\n",
    "        \n",
    "    if _USE_DATA_AUGMENTATION:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.25, 0.75),\n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    print('Y-Col: {}'.format(get_Label_Names(label_type)))\n",
    "    print('Train Data Generator: ', end = '')\n",
    "    \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    print('Validation Data Generator: ', end = '')\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Modell (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_model_fine(x, y, x_val, y_val, params):\n",
    "    print('==========================Params:')\n",
    "    print(params)\n",
    "    print('==========================')\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data(params['batch_size'], params['samples'], params['label_type'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    "    dropout_rate = params['dropout']\n",
    "    first_neuron = params['first_neuron']\n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = params['leaky_alpha'])\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    cnn = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "    \n",
    "    for layer in cnn.layers[:15]:\n",
    "        layer.trainable = False\n",
    "        #print(layer.name, layer.trainable)\n",
    "        \n",
    "    print('_________________________________________________________________')\n",
    "    print('{:>16} {:>16}'.format('Network Layer', 'Trainable'))\n",
    "    print('=================================================================')\n",
    "    for layer in cnn.layers:\n",
    "        print('{:>16} {:>16}'.format(layer.name, layer.trainable))\n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    model.add(cnn)\n",
    "    \n",
    "    fc = Sequential()\n",
    "    fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512)\n",
    "    \n",
    "    fc.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    print('Number Hidden Layers {}'.format(params['hidden_layers']))\n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(params['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.load_weights(_MODEL_DIR + _MODEL_TO_LOAD)\n",
    "    model.add(fc)\n",
    "    print('Fully Connected Layers added to Base Network')\n",
    "    \n",
    "    print('Using Loss: {} \\nand Reduction Metric: {}'.format(\n",
    "        params['loss_function'], \n",
    "        get_Reduction_Metric(params['reduction_metric'])))\n",
    "    \n",
    "    model.compile(\n",
    "        #optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])*1e-2),\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer']) * 1e-3),\n",
    "        loss = params['loss_function'],\n",
    "        metrics = get_Reduction_Metric(params['reduction_metric'])\n",
    "    )\n",
    "    print('Model was compiled')\n",
    "    print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = _LOG_DIR + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        monitor =  params['monitor_value'],\n",
    "        verbose = 1,\n",
    "        save_weights_only = False,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    print('Checkpointer was created')\n",
    "    \n",
    "    csv_logger = callbacks.CSVLogger(\n",
    "        filename = _LOG_DIR + 'CNN_Base_{}_Logger_{}.csv'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        separator = ',',\n",
    "        append = False\n",
    "    )\n",
    "    print('CSV Logger was created')\n",
    "\n",
    "    lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 13,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        min_delta = 0.0001\n",
    "    )\n",
    "    print('Learning Rate Reducer was created')\n",
    "    \n",
    "    early_stopper = callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0,\n",
    "        #patience = 15,\n",
    "        patience = 20,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    print('Early Stopper was created')\n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_data = valid_generator,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        callbacks = [checkpointer, csv_logger, lr_reducer, early_stopper],\n",
    "        epochs = params['epochs'],\n",
    "        workers = 8\n",
    "    )\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feinoptimierung <a name = \"setup\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "global_hyper_parameter = {\n",
    "    'samples': None,\n",
    "    'epochs': None,\n",
    "    'batch_size': None,\n",
    "    'optimizer': None,\n",
    "    'lr': None,\n",
    "    'first_neuron': None,\n",
    "    'dropout': None,\n",
    "    'activation': None,\n",
    "    'leaky_alpha': None,\n",
    "    'hidden_layers': None,\n",
    "    # beginning from here, Values should only contain one single entry:\n",
    "    # ===============================================================\n",
    "    'label_type': ['Angular'], # Stereographic, Angular, Normalized\n",
    "    'loss_function': None,\n",
    "    'reduction_metric': None,\n",
    "    'monitor_value': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RUN = 'SYNTH'\n",
    "_LOSS = 'MSE'\n",
    "_DATASET_NAME = '201129_2031'\n",
    "_DEVICE = 'GeForce_RTX_2080_Ti'#'TITAN_GPU1'\n",
    "\n",
    "storage = OutputDirectory.SSD # 'fast_output' if ssd storage may suffice, 'output' otherwise\n",
    "\n",
    "if global_hyper_parameter['label_type'][0] == 'Stereographic':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_stereographic.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_stereographic.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Angular':\n",
    "    _CSV_SYNTH_FILE_NAME = 'labels_ks_RGB.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Normalized':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_normalized.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_normalized.csv'\n",
    "    \n",
    "else:\n",
    "    assert(True, 'Label Type Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = TrainingSet.SYNTHETIC\n",
    "_USE_DATA_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n"
     ]
    }
   ],
   "source": [
    "_IMAGE_DIR = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(_DATASET_NAME)\n",
    "_CSV_FILE = _IMAGE_DIR + _CSV_SYNTH_FILE_NAME\n",
    "_CSV_FILE_REAL = _IMAGE_DIR + _CSV_REAL_FILE_NAME\n",
    "\n",
    "_note = '_Custom-MAE'\n",
    "\n",
    "_MODEL_DIR = '..\\\\output\\\\{}_Regression_{}\\\\{}_{}_Base{}\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "_NET_DIR = '{}_Regression_{}\\\\{}_{}_Top_1{}\\\\{}_TD\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note, trainingset_to_string(trainingset))\n",
    "_LOG_DIR = '..\\\\{}\\\\{}'.format(output_path[storage], _NET_DIR)\n",
    "\n",
    "if(not os.path.exists(_LOG_DIR)):\n",
    "    os.makedirs(_LOG_DIR)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(_LOG_DIR))\n",
    "\n",
    "device_file = open(_LOG_DIR + '{}.txt'.format(_DEVICE), \"a+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 FC-Gewichte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: ..\\output\\SYNTH_Regression_MSE\\201129_2031_Angular_Base_Custom-MAE\\..\\201129_2031_Angular_Base_Custom-MAE_Results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>loss</th>\n",
       "      <th>custom_mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_custom_mae</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>label_type</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>lr</th>\n",
       "      <th>monitor_value</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>reduction_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>12/01/20-164330</td>\n",
       "      <td>12/01/20-164458</td>\n",
       "      <td>87.977822</td>\n",
       "      <td>2945.725098</td>\n",
       "      <td>32.865025</td>\n",
       "      <td>1920.490479</td>\n",
       "      <td>22.284609</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>210</td>\n",
       "      <td>12/01/20-170136</td>\n",
       "      <td>12/01/20-170411</td>\n",
       "      <td>155.303501</td>\n",
       "      <td>2803.241211</td>\n",
       "      <td>31.082224</td>\n",
       "      <td>1987.502930</td>\n",
       "      <td>22.709600</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>12/01/20-164043</td>\n",
       "      <td>12/01/20-164206</td>\n",
       "      <td>82.658976</td>\n",
       "      <td>2817.534180</td>\n",
       "      <td>31.355553</td>\n",
       "      <td>1974.598267</td>\n",
       "      <td>22.803047</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>12/01/20-124846</td>\n",
       "      <td>12/01/20-125119</td>\n",
       "      <td>152.586228</td>\n",
       "      <td>2783.287842</td>\n",
       "      <td>31.109287</td>\n",
       "      <td>1990.522949</td>\n",
       "      <td>22.830732</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>12/01/20-125119</td>\n",
       "      <td>12/01/20-125351</td>\n",
       "      <td>152.103672</td>\n",
       "      <td>2820.208496</td>\n",
       "      <td>31.173552</td>\n",
       "      <td>2007.385620</td>\n",
       "      <td>22.876354</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>213</td>\n",
       "      <td>12/01/20-170922</td>\n",
       "      <td>12/01/20-171216</td>\n",
       "      <td>174.551065</td>\n",
       "      <td>3188.432129</td>\n",
       "      <td>34.184753</td>\n",
       "      <td>2007.128540</td>\n",
       "      <td>22.888334</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>184</td>\n",
       "      <td>12/01/20-163026</td>\n",
       "      <td>12/01/20-163114</td>\n",
       "      <td>48.098176</td>\n",
       "      <td>3074.985596</td>\n",
       "      <td>34.478340</td>\n",
       "      <td>1958.542358</td>\n",
       "      <td>22.909779</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>211</td>\n",
       "      <td>12/01/20-170411</td>\n",
       "      <td>12/01/20-170646</td>\n",
       "      <td>154.531607</td>\n",
       "      <td>2810.544189</td>\n",
       "      <td>31.549955</td>\n",
       "      <td>1969.225220</td>\n",
       "      <td>23.093344</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>181</td>\n",
       "      <td>12/01/20-162803</td>\n",
       "      <td>12/01/20-162850</td>\n",
       "      <td>46.563921</td>\n",
       "      <td>2840.809082</td>\n",
       "      <td>31.588232</td>\n",
       "      <td>2005.707031</td>\n",
       "      <td>23.123720</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12/01/20-121822</td>\n",
       "      <td>12/01/20-121903</td>\n",
       "      <td>41.068142</td>\n",
       "      <td>2797.072754</td>\n",
       "      <td>31.254971</td>\n",
       "      <td>2005.048462</td>\n",
       "      <td>23.160395</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0            start              end    duration         loss  \\\n",
       "198         198  12/01/20-164330  12/01/20-164458   87.977822  2945.725098   \n",
       "210         210  12/01/20-170136  12/01/20-170411  155.303501  2803.241211   \n",
       "196         196  12/01/20-164043  12/01/20-164206   82.658976  2817.534180   \n",
       "30           30  12/01/20-124846  12/01/20-125119  152.586228  2783.287842   \n",
       "31           31  12/01/20-125119  12/01/20-125351  152.103672  2820.208496   \n",
       "213         213  12/01/20-170922  12/01/20-171216  174.551065  3188.432129   \n",
       "184         184  12/01/20-163026  12/01/20-163114   48.098176  3074.985596   \n",
       "211         211  12/01/20-170411  12/01/20-170646  154.531607  2810.544189   \n",
       "181         181  12/01/20-162803  12/01/20-162850   46.563921  2840.809082   \n",
       "1             1  12/01/20-121822  12/01/20-121903   41.068142  2797.072754   \n",
       "\n",
       "     custom_mae     val_loss  val_custom_mae activation  batch_size  dropout  \\\n",
       "198   32.865025  1920.490479       22.284609       relu          32     0.25   \n",
       "210   31.082224  1987.502930       22.709600       relu          32     0.25   \n",
       "196   31.355553  1974.598267       22.803047       relu          32     0.25   \n",
       "30    31.109287  1990.522949       22.830732  leakyrelu          32     0.25   \n",
       "31    31.173552  2007.385620       22.876354  leakyrelu          32     0.25   \n",
       "213   34.184753  2007.128540       22.888334       relu          32     0.25   \n",
       "184   34.478340  1958.542358       22.909779       relu          32     0.25   \n",
       "211   31.549955  1969.225220       23.093344       relu          32     0.25   \n",
       "181   31.588232  2005.707031       23.123720       relu          32     0.25   \n",
       "1     31.254971  2005.048462       23.160395  leakyrelu          32     0.25   \n",
       "\n",
       "     first_neuron  hidden_layers label_type       loss_function  lr  \\\n",
       "198          2048              1    Angular  mean_squared_error   1   \n",
       "210          4096              0    Angular  mean_squared_error   1   \n",
       "196          2048              0    Angular  mean_squared_error   2   \n",
       "30           4096              0    Angular  mean_squared_error   1   \n",
       "31           4096              0    Angular  mean_squared_error   2   \n",
       "213          4096              1    Angular  mean_squared_error   1   \n",
       "184          1024              1    Angular  mean_squared_error   2   \n",
       "211          4096              0    Angular  mean_squared_error   2   \n",
       "181          1024              0    Angular  mean_squared_error   2   \n",
       "1            1024              0    Angular  mean_squared_error   2   \n",
       "\n",
       "      monitor_value                                          optimizer  \\\n",
       "198  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "210  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "196  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "30   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "31   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "213  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "184  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "211  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "181  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "1    val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "\n",
       "    reduction_metric  \n",
       "198       custom_mae  \n",
       "210       custom_mae  \n",
       "196       custom_mae  \n",
       "30        custom_mae  \n",
       "31        custom_mae  \n",
       "213       custom_mae  \n",
       "184       custom_mae  \n",
       "211       custom_mae  \n",
       "181       custom_mae  \n",
       "1         custom_mae  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_results = _MODEL_DIR + '..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "df = pd.read_csv(base_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "sort_value = df['monitor_value'][0]\n",
    "df = df.sort_values(sort_value, axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(base_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(top_results_index):\n",
    "    \n",
    "    #     Adam = RMSprop + Momentum (lr=0.001)\n",
    "    #     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "    #     RMSprop = (lr=0.001)\n",
    "    #     SGD = (lr=0.01)\n",
    "    #     Adagrad\n",
    "\n",
    "    hyper_parameter = global_hyper_parameter\n",
    "\n",
    "    hyper_parameter['samples'] = [100000] \n",
    "    hyper_parameter['epochs'] = [400]\n",
    "    hyper_parameter['batch_size'] = [df.iloc[top_results_index]['batch_size']]\n",
    "    hyper_parameter['optimizer'] = [make_optimizer(df.loc[top_results_index]['optimizer'])]\n",
    "    hyper_parameter['lr'] = [df.iloc[top_results_index]['lr']]\n",
    "    hyper_parameter['first_neuron'] = [df.iloc[top_results_index]['first_neuron']]\n",
    "    hyper_parameter['dropout'] = [df.iloc[top_results_index]['dropout']]\n",
    "    hyper_parameter['activation'] = [df.iloc[top_results_index]['activation']]\n",
    "    hyper_parameter['leaky_alpha'] = [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "    hyper_parameter['hidden_layers'] = [df.iloc[top_results_index]['hidden_layers']]\n",
    "    \n",
    "    hyper_parameter['loss_function'] = [df.iloc[top_results_index]['loss_function']]\n",
    "    hyper_parameter['reduction_metric'] = [df.iloc[top_results_index]['reduction_metric']]\n",
    "    hyper_parameter['monitor_value'] = [df.iloc[top_results_index]['monitor_value']]\n",
    "\n",
    "    return hyper_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#print(_CSV_FILE)\n",
    "#df = pd.read_csv(_CSV_FILE)\n",
    "#df.head()\n",
    "#print(df.iloc[0]['Filename RGB'])\n",
    "##Image(df.iloc[0]['Filename RGB'])\n",
    "##Image('../buddha00000001-0-5-0-10.png')\n",
    "##Image('../../data_generation/dataset/201019_2253_final/buddha/rgb/buddha00000000-0-5-0-5.png')\n",
    "#Image('..\\\\..\\\\data_generation\\\\dataset\\\\201019_2253_final\\\\buddha\\\\rgb\\\\buddha00000000-0-5-0-5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 2048, 'hidden_layers': 1, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================Params:\n",
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 2048, 'hidden_layers': 1, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================\n",
      "Y-Col: ['Elevation', 'Azimuth']\n",
      "Train Data Generator: Found 80000 validated image filenames.\n",
      "Validation Data Generator: Found 20000 validated image filenames.\n",
      "Steps per Epoch: 2500, Validation Steps: 625\n",
      "_________________________________________________________________\n",
      "   Network Layer        Trainable\n",
      "=================================================================\n",
      "         input_1                0\n",
      "    block1_conv1                0\n",
      "    block1_conv2                0\n",
      "     block1_pool                0\n",
      "    block2_conv1                0\n",
      "    block2_conv2                0\n",
      "     block2_pool                0\n",
      "    block3_conv1                0\n",
      "    block3_conv2                0\n",
      "    block3_conv3                0\n",
      "     block3_pool                0\n",
      "    block4_conv1                0\n",
      "    block4_conv2                0\n",
      "    block4_conv3                0\n",
      "     block4_pool                0\n",
      "    block5_conv1                1\n",
      "    block5_conv2                1\n",
      "    block5_conv3                1\n",
      "     block5_pool                1\n",
      "_________________________________________________________________\n",
      "\n",
      "Number Hidden Layers 1\n",
      "Fully Connected Layers added to Base Network\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x000001BBA0CAB3A8>]\n",
      "Model was compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 2)                 53482498  \n",
      "=================================================================\n",
      "Total params: 68,197,186\n",
      "Trainable params: 60,561,922\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Checkpointer was created\n",
      "CSV Logger was created\n",
      "Learning Rate Reducer was created\n",
      "Early Stopper was created\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2500 steps, validate for 625 steps\n",
      "Epoch 1/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 247450.5646 - custom_mae: 247.2290\n",
      "Epoch 00001: val_custom_mae improved from inf to 2502.67505, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_198_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 1341s 537ms/step - loss: 247413.1560 - custom_mae: 247.2213 - val_loss: 9473392.0776 - val_custom_mae: 2502.6750\n",
      "Epoch 2/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 95937.3479 - custom_mae: 175.5869\n",
      "Epoch 00002: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 95884.5973 - custom_mae: 175.5446 - val_loss: 29625685.8048 - val_custom_mae: 3152.8601\n",
      "Epoch 3/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 20198.9861 - custom_mae: 96.0703\n",
      "Epoch 00003: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 20197.1464 - custom_mae: 96.0681 - val_loss: 491806377.1648 - val_custom_mae: 16800.8184\n",
      "Epoch 4/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 13094.9850 - custom_mae: 80.1401\n",
      "Epoch 00004: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 13095.3503 - custom_mae: 80.1423 - val_loss: 2780111808.1024 - val_custom_mae: 41986.5938\n",
      "Epoch 5/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 11177.9623 - custom_mae: 75.7776\n",
      "Epoch 00005: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 249s 100ms/step - loss: 11177.4532 - custom_mae: 75.7765 - val_loss: 10959994622.7712 - val_custom_mae: 83646.7109\n",
      "Epoch 6/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 9275.4124 - custom_mae: 70.6709\n",
      "Epoch 00006: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 9276.0240 - custom_mae: 70.6745 - val_loss: 31598537129.9840 - val_custom_mae: 138964.0781\n",
      "Epoch 7/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 7773.0099 - custom_mae: 65.7020\n",
      "Epoch 00007: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 249s 100ms/step - loss: 7772.1111 - custom_mae: 65.6987 - val_loss: 70636553509.2736 - val_custom_mae: 202132.3594\n",
      "Epoch 8/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 6885.6833 - custom_mae: 61.9581\n",
      "Epoch 00008: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 254s 101ms/step - loss: 6886.4595 - custom_mae: 61.9614 - val_loss: 127424034530.9184 - val_custom_mae: 261016.0938\n",
      "Epoch 9/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 6453.0383 - custom_mae: 59.6993\n",
      "Epoch 00009: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 6453.2853 - custom_mae: 59.7005 - val_loss: 195414139653.3248 - val_custom_mae: 312753.6250\n",
      "Epoch 10/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 6363.0520 - custom_mae: 59.0515\n",
      "Epoch 00010: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 6363.0965 - custom_mae: 59.0523 - val_loss: 257534395233.0752 - val_custom_mae: 352306.9062\n",
      "Epoch 11/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 6359.0219 - custom_mae: 58.9628\n",
      "Epoch 00011: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 249s 100ms/step - loss: 6359.8482 - custom_mae: 58.9660 - val_loss: 313638287939.9936 - val_custom_mae: 386383.7812\n",
      "Epoch 12/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 6452.0516 - custom_mae: 59.22 - ETA: 0s - loss: 6453.0162 - custom_mae: 59.2230\n",
      "Epoch 00012: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 6452.5084 - custom_mae: 59.2209 - val_loss: 365211662365.4912 - val_custom_mae: 415563.4375\n",
      "Epoch 13/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 6495.9713 - custom_mae: 59.3688\n",
      "Epoch 00013: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 251s 101ms/step - loss: 6495.8447 - custom_mae: 59.3693 - val_loss: 414723882562.3552 - val_custom_mae: 441460.3750\n",
      "Epoch 14/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 6555.3251 - custom_mae: 59.5751\n",
      "Epoch 00014: val_custom_mae did not improve from 2502.67505\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 6555.9367 - custom_mae: 59.5777 - val_loss: 487773058315.0592 - val_custom_mae: 479519.1250\n",
      "Epoch 15/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 6545.4527 - custom_mae: 59.4616\n",
      "Epoch 00015: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 6545.3110 - custom_mae: 59.4610 - val_loss: 555611567947.7760 - val_custom_mae: 511855.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 6509.5842 - custom_mae: 59.3562\n",
      "Epoch 00016: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 6509.8017 - custom_mae: 59.3579 - val_loss: 622029663685.8368 - val_custom_mae: 541510.2500\n",
      "Epoch 17/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 6568.2196 - custom_mae: 59.5594\n",
      "Epoch 00017: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 6568.8688 - custom_mae: 59.5604 - val_loss: 698945463176.3969 - val_custom_mae: 574315.8750\n",
      "Epoch 18/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 6553.3906 - custom_mae: 59.4705\n",
      "Epoch 00018: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 6553.0989 - custom_mae: 59.4688 - val_loss: 767569203141.0176 - val_custom_mae: 601650.1250\n",
      "Epoch 19/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 6496.4776 - custom_mae: 59.3322\n",
      "Epoch 00019: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 251s 100ms/step - loss: 6496.6277 - custom_mae: 59.3340 - val_loss: 859443805709.9264 - val_custom_mae: 636958.6875\n",
      "Epoch 20/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 6526.7362 - custom_mae: 59.4339\n",
      "Epoch 00020: val_custom_mae did not improve from 2502.67505\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 6526.6060 - custom_mae: 59.4338 - val_loss: 944873984858.5216 - val_custom_mae: 667983.6250\n",
      "Epoch 21/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 6444.0467 - custom_mae: 59.0976\n",
      "Epoch 00021: val_custom_mae did not improve from 2502.67505\n",
      "Restoring model weights from the end of the best epoch.\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 6444.6031 - custom_mae: 59.0988 - val_loss: 1032306320578.9696 - val_custom_mae: 698338.6250\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [1:46:33<00:00, 6393.95s/it]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1:46:33.989437\n",
      "Writing Device File\n",
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 4096, 'hidden_layers': 0, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================Params:\n",
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 4096, 'hidden_layers': 0, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================\n",
      "Y-Col: ['Elevation', 'Azimuth']\n",
      "Train Data Generator: Found 80000 validated image filenames.\n",
      "Validation Data Generator: Found 20000 validated image filenames.\n",
      "Steps per Epoch: 2500, Validation Steps: 625\n",
      "_________________________________________________________________\n",
      "   Network Layer        Trainable\n",
      "=================================================================\n",
      "         input_1                0\n",
      "    block1_conv1                0\n",
      "    block1_conv2                0\n",
      "     block1_pool                0\n",
      "    block2_conv1                0\n",
      "    block2_conv2                0\n",
      "     block2_pool                0\n",
      "    block3_conv1                0\n",
      "    block3_conv2                0\n",
      "    block3_conv3                0\n",
      "     block3_pool                0\n",
      "    block4_conv1                0\n",
      "    block4_conv2                0\n",
      "    block4_conv3                0\n",
      "     block4_pool                0\n",
      "    block5_conv1                1\n",
      "    block5_conv2                1\n",
      "    block5_conv3                1\n",
      "     block5_pool                1\n",
      "_________________________________________________________________\n",
      "\n",
      "Number Hidden Layers 0\n",
      "Fully Connected Layers added to Base Network\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x000001BBA0CAB3A8>]\n",
      "Model was compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 2)                 102772738 \n",
      "=================================================================\n",
      "Total params: 117,487,426\n",
      "Trainable params: 109,852,162\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Checkpointer was created\n",
      "CSV Logger was created\n",
      "Learning Rate Reducer was created\n",
      "Early Stopper was created\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2500 steps, validate for 625 steps\n",
      "Epoch 1/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 2976440.3621 - custom_mae: 693.8805\n",
      "Epoch 00001: val_custom_mae improved from inf to 1569.27295, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 2977112.7411 - custom_mae: 693.9261 - val_loss: 14320138.7488 - val_custom_mae: 1569.2729\n",
      "Epoch 2/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 6063862.0421 - custom_mae: 1011.5131- ETA: 6s - loss: 6027442.3635 - custom - ETA: 5s\n",
      "Epoch 00002: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 6063405.8322 - custom_mae: 1011.4879 - val_loss: 68873552.2688 - val_custom_mae: 3398.8733\n",
      "Epoch 3/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 9935600.5127 - custom_mae: 1269.2258\n",
      "Epoch 00003: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 9936394.5673 - custom_mae: 1269.2653 - val_loss: 146355077.0176 - val_custom_mae: 4981.4019\n",
      "Epoch 4/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 10531615.4832 - custom_mae: 1281.8706\n",
      "Epoch 00004: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 253s 101ms/step - loss: 10530739.9064 - custom_mae: 1281.7993 - val_loss: 267041148.4928 - val_custom_mae: 6725.8862\n",
      "Epoch 5/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 8333140.9637 - custom_mae: 1118.5353\n",
      "Epoch 00005: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 251s 101ms/step - loss: 8331336.2508 - custom_mae: 1118.4425 - val_loss: 353223087.7184 - val_custom_mae: 7896.1719\n",
      "Epoch 6/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 4990111.4143 - custom_mae: 855.7950\n",
      "Epoch 00006: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 4989733.0738 - custom_mae: 855.7618 - val_loss: 371165758.6688 - val_custom_mae: 8194.0771\n",
      "Epoch 7/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 2309211.4038 - custom_mae: 564.3027\n",
      "Epoch 00007: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 2308948.3369 - custom_mae: 564.2578 - val_loss: 289427072.8192 - val_custom_mae: 7326.2793\n",
      "Epoch 8/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 822250.8974 - custom_mae: 334.5582\n",
      "Epoch 00008: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 822053.3896 - custom_mae: 334.5397 - val_loss: 187458102.8352 - val_custom_mae: 6158.8423\n",
      "Epoch 9/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 234042.6449 - custom_mae: 192.7879\n",
      "Epoch 00009: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 251s 100ms/step - loss: 233988.1237 - custom_mae: 192.7745 - val_loss: 192652845.9264 - val_custom_mae: 6701.3525\n",
      "Epoch 10/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 46638.2884 - custom_mae: 116.4433\n",
      "Epoch 00010: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 46620.7646 - custom_mae: 116.4331 - val_loss: 306066230.2208 - val_custom_mae: 8676.2363\n",
      "Epoch 11/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 17434.4290 - custom_mae: 89.4371\n",
      "Epoch 00011: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 17431.7909 - custom_mae: 89.4327 - val_loss: 503742502.2976 - val_custom_mae: 10886.2627\n",
      "Epoch 12/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 14848.9520 - custom_mae: 83.2926\n",
      "Epoch 00012: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 14849.5257 - custom_mae: 83.2924 - val_loss: 841521821.9008 - val_custom_mae: 13581.4316\n",
      "Epoch 13/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 14277.9927 - custom_mae: 81.4555\n",
      "Epoch 00013: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 14279.6628 - custom_mae: 81.4587 - val_loss: 1433837496.9344 - val_custom_mae: 17579.1309\n",
      "Epoch 14/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 13627.6212 - custom_mae: 79.7317\n",
      "Epoch 00014: val_custom_mae did not improve from 1569.27295\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 13627.9874 - custom_mae: 79.7309 - val_loss: 2456667190.0672 - val_custom_mae: 23316.0938\n",
      "Epoch 15/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 13047.6952 - custom_mae: 77.9621\n",
      "Epoch 00015: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 250s 100ms/step - loss: 13047.7903 - custom_mae: 77.9603 - val_loss: 3052941247.6928 - val_custom_mae: 26101.9277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 12875.6023 - custom_mae: 77.3515\n",
      "Epoch 00016: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 12875.0524 - custom_mae: 77.3499 - val_loss: 3933041922.8672 - val_custom_mae: 29864.7930\n",
      "Epoch 17/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 12700.6651 - custom_mae: 76.7784\n",
      "Epoch 00017: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 251s 100ms/step - loss: 12702.9370 - custom_mae: 76.7847 - val_loss: 4982828391.6288 - val_custom_mae: 33825.8125\n",
      "Epoch 18/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 12476.2681 - custom_mae: 75.9626- ETA: 0s - loss: 12478.4633 - custom_mae: 7\n",
      "Epoch 00018: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 12474.5872 - custom_mae: 75.9573 - val_loss: 6082512809.1648 - val_custom_mae: 37483.4531\n",
      "Epoch 19/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 12320.4816 - custom_mae: 75.5010\n",
      "Epoch 00019: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 251s 100ms/step - loss: 12321.2442 - custom_mae: 75.5048 - val_loss: 7246440579.0720 - val_custom_mae: 40965.7266\n",
      "Epoch 20/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 12180.9886 - custom_mae: 75.0136\n",
      "Epoch 00020: val_custom_mae did not improve from 1569.27295\n",
      "2500/2500 [==============================] - 252s 101ms/step - loss: 12179.8444 - custom_mae: 75.0090 - val_loss: 8497012459.1104 - val_custom_mae: 44370.1797\n",
      "Epoch 21/400\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 12019.3598 - custom_mae: 74.4064\n",
      "Epoch 00021: val_custom_mae did not improve from 1569.27295\n",
      "Restoring model weights from the end of the best epoch.\n",
      "2500/2500 [==============================] - 251s 100ms/step - loss: 12019.0450 - custom_mae: 74.4054 - val_loss: 9854383654.5024 - val_custom_mae: 47843.0117\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [1:28:08<00:00, 5288.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1:28:08.552128\n",
      "Writing Device File\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    #for top_results_index in range(3):\n",
    "    #for top_results_index in [0, 1]:\n",
    "    top_results_index = 0\n",
    "    _MODEL_TO_LOAD_INDEX = df.iloc[top_results_index].name\n",
    "    _MODEL_TO_LOAD = 'Best_Weights_FC_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "    _TMP_DIR = '..\\\\TMP_TALOS_{}'.format(_DEVICE)\n",
    "    _CSV_RESULTS = _LOG_DIR + 'Talos_Results_Fine_Idx{}.csv'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "    startTime = datetime.now()\n",
    "\n",
    "    parameters = get_params(top_results_index)\n",
    "\n",
    "    t = ta.Scan(\n",
    "        x = dummy_x,\n",
    "        y = dummy_y,\n",
    "        model = grid_model_fine,\n",
    "        params = parameters,\n",
    "        experiment_name = _TMP_DIR,\n",
    "        #shuffle=False,\n",
    "        reduction_metric = parameters['reduction_metric'][0],\n",
    "        disable_progress_bar = False,\n",
    "        print_params = True,\n",
    "        clear_session = 'tf'\n",
    "    )\n",
    "\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "\n",
    "    print('Writing Device File')\n",
    "    device_file.write('Trained Model: {}'.format(_MODEL_TO_LOAD))\n",
    "\n",
    "    df_experiment_results = pd.read_csv(_TMP_DIR + '\\\\' + os.listdir(_TMP_DIR)[0])\n",
    "    df_experiment_results['Base'] = None\n",
    "    for i in range(df_experiment_results.shape[0]):\n",
    "        df_experiment_results['Base'][i] = _MODEL_TO_LOAD_INDEX\n",
    "\n",
    "    if os.path.isfile(_CSV_RESULTS):\n",
    "        df_experiment_results.to_csv(_CSV_RESULTS, mode = 'a', index = False, header = False)\n",
    "    else:\n",
    "        df_experiment_results.to_csv(_CSV_RESULTS, index = False)\n",
    "\n",
    "    shutil.rmtree(_TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Results to NAS if SSD Directory was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_directory(src, dst, symlinks = False, ignore = None):\n",
    "    maxLen = 0\n",
    "    message = ''        \n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        \n",
    "        message = 'Creating Path: {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        os.makedirs(dst)\n",
    "        \n",
    "    for item in os.listdir(src):\n",
    "        \n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        \n",
    "        if os.path.isdir(s):\n",
    "            \n",
    "            message = 'Copying Directory: {}'.format(s)\n",
    "            maxLen = max(maxLen, len(message))\n",
    "            print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "            \n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if not os.path.exists(d): #or os.stat(s).st_mtime - os.stat(d).st_mtime > 1:\n",
    "                \n",
    "                message = 'Copying File: {}'.format(s)\n",
    "                maxLen = max(maxLen, len(message))\n",
    "                print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "                \n",
    "                shutil.copy2(s, d)\n",
    "        \n",
    "        time.sleep(.5)\n",
    "     \n",
    "    message = 'Coyping... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "\n",
    "def delete_directory(src, terminator = '\\n'):\n",
    "    message = ''\n",
    "    maxLen = 0\n",
    "    \n",
    "    try:\n",
    "        message = 'Deleting {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        shutil.rmtree(src)\n",
    "        \n",
    "    except OSError as e:\n",
    "        message = 'Error: {} : {}'.format(src, e.strerror)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "        return\n",
    "    \n",
    "    message = 'Deleting... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = terminator)\n",
    "\n",
    "    \n",
    "def copy_fine_training(src, dst):\n",
    "    copy_directory(src, dst)\n",
    "    delete_directory(src, terminator = '\\r')\n",
    "    delete_directory(src + '..\\\\', terminator = '\\r')\n",
    "    if not os.listdir(src + '..\\\\..\\\\'):\n",
    "        delete_directory(src + '..\\\\..\\\\', terminator = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coyping... Done                                                                                                                          \n",
      "Deleting... Done                                                                              \r"
     ]
    }
   ],
   "source": [
    "if(storage == OutputDirectory.SSD):\n",
    "    _COPY_DIR = '..\\\\output\\\\{}'.format(_NET_DIR)\n",
    "    copy_fine_training(_LOG_DIR, _COPY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name = \"CMSE.Mixed\"></a><a href = #Top>Up</a></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
