{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation <a name = \"Top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links\n",
    "\n",
    "<ol>\n",
    "    <li><a href = #setup>Setup</a></li>\n",
    "    <li><a href = #plots>Plots</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "import ntpath\n",
    "\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum für Trainingsset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSet(Enum):\n",
    "    SYNTHETIC = 1\n",
    "    REAL = 2\n",
    "    MIXED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum für Label-Typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelType(Enum):\n",
    "    ANGULAR = 1\n",
    "    STEREOGRAPHIC = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsset-Typ nach String Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_to_string(ts):\n",
    "    if ts == TrainingSet.SYNTHETIC:\n",
    "        return 'Synth'\n",
    "    elif ts == TrainingSet.REAL:\n",
    "        return 'Real'\n",
    "    elif ts == TrainingSet.MIXED:\n",
    "        return 'Mixed'\n",
    "    else:\n",
    "        print('Unknown TrainingSet')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelType nach String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeltype_to_string(lt):\n",
    "    if lt == LabelType.ANGULAR:\n",
    "        return 'Angular'\n",
    "    if lt == LabelType.STEREOGRAPHIC:\n",
    "        return 'Stereographic'\n",
    "    else:\n",
    "        print('Unknown LabelType')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required format of parameters parameter for _model_predict_ (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'dataset_to_use':[],\n",
    "    'model_to_load':[],\n",
    "    'dataset_name':'combined_dataset',\n",
    "    'activation_function':[],\n",
    "    'leaky_ReLU_alpha':[],\n",
    "    'first_neuron':[],\n",
    "    'dropout_rate':[],\n",
    "    'hidden_layers':[],\n",
    "    'optimizer':[],\n",
    "    'learning_rate':[],\n",
    "    'loss_function':[],\n",
    "    'label_type':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertierung ($S_x$, $S_y$) $\\rightarrow$ ($\\phi$, $\\theta$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_stereographic(sx, sy, r = 1):\n",
    "    \n",
    "    l = np.sqrt(sx * sx + sy * sy)\n",
    "    theta = 90 - 2 * np.degrees(np.arctan(l / 2))\n",
    "    \n",
    "    if sx < 0:\n",
    "        phi = 180 - np.degrees(np.arcsin(sy / l))\n",
    "        \n",
    "    elif sx >= 0:\n",
    "        if sy > 0:\n",
    "            phi = np.degrees(np.arcsin(sy / l))\n",
    "        elif sy < 0:\n",
    "            phi = 360 + np.degrees(np.arcsin(sy / l))\n",
    "        else:\n",
    "            #phi1 = np.NaN\n",
    "            phi = 0\n",
    "    else:\n",
    "        print('sx and sy undefined. should not have reached here')\n",
    "\n",
    "    return phi, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertierung ($\\phi$, $\\theta$) $\\rightarrow$ ($S_x$, $S_y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_spheric(phi, theta, r = 1):\n",
    "    m = 2 * r * np.tan(np.radians((90 - theta) / 2))\n",
    "    sy = m * np.sin(np.radians(phi))\n",
    "    sx = m * np.cos(np.radians(phi))\n",
    "    return sx, sy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radians $\\rightarrow$ Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_degree(angle_in_rad):\n",
    "    return angle_in_rad * 180 / np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree $\\rightarrow$ Radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_radians(angle_in_deg):\n",
    "    return angle_in_deg * np.pi / 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sphärische $\\rightarrow$ Karthesische Koordinaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spheric_cartesian_polar(phi_d, theta_d):\n",
    "    x = np.sin(np.radians(90.0 - theta_d)) * np.cos(np.radians(phi_d))\n",
    "    y = np.sin(np.radians(90.0 - theta_d)) * np.sin(np.radians(phi_d))\n",
    "    z = np.cos(np.radians(90.0 - theta_d))\n",
    "    return array([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spheric_cartesian_elevation(phi_d, theta_d):\n",
    "    x = np.cos(np.radians(theta_d)) * np.cos(np.radians(phi_d))\n",
    "    y = np.cos(np.radians(theta_d)) * np.sin(np.radians(phi_d))\n",
    "    z = np.sin(np.radians(theta_d))\n",
    "    return array([x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorlength(vector):\n",
    "    return np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculated Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angular_error(deg_e_phi, deg_e_theta):\n",
    "    return np.degrees(np.arccos(np.cos(np.radians(deg_e_phi)) * np.cos(np.radians(deg_e_theta))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalarprodukt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDot(a, b):\n",
    "    dot = 0;\n",
    "    it = np.nditer(a, flags=['f_index'])\n",
    "    for x in it:\n",
    "        dot = dot + (x * b[it.index])\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_angular_error_elevation(predicted_deg_vector, true_deg_vector):    \n",
    "    c_predicted = spheric_cartesian_elevation(predicted_deg_vector[0], predicted_deg_vector[1])\n",
    "    c_true = spheric_cartesian_elevation(true_deg_vector[0], true_deg_vector[1])\n",
    "    \n",
    "    len_prediction = vectorlength(c_predicted)\n",
    "    len_true = vectorlength(c_true)\n",
    "    \n",
    "    cos_angle = np.dot(c_true, c_predicted) / len_prediction / len_true\n",
    "    \n",
    "    return abs(np.degrees(np.arccos(cos_angle)))\n",
    "\n",
    "def dot_angular_error_polar(predicted_deg_vector, true_deg_vector):\n",
    "    c_predicted = spheric_cartesian_polar(predicted_deg_vector[0], predicted_deg_vector[1])\n",
    "    c_true = spheric_cartesian_polar(true_deg_vector[0], true_deg_vector[1])\n",
    "    \n",
    "    len_prediction = vectorlength(c_predicted)\n",
    "    len_true = vectorlength(c_true)\n",
    "    \n",
    "    cos_angle = np.dot(c_true, c_predicted) / len_prediction / len_true\n",
    "    \n",
    "    return abs(np.degrees(np.arccos(cos_angle)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normierte sphärische Koordinaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normierte $\\rightarrow$ Sphärische"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_to_spheric(n_phi, n_theta):\n",
    "    phi = n_phi * 180 + 180\n",
    "    theta = n_theta * 45 + 45\n",
    "    \n",
    "    return phi, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation <a name = \"setup\"></a>\n",
    "<p><a href = #Top>Up</a>\n",
    "<p><a href = #plots>Plots</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'SYNTH'\n",
    "loss = 'MSE'\n",
    "dataset_name = '2020-05-28'\n",
    "net_index = [99, 204, 195]\n",
    "APPENDIX = ['Angular', 'Normalized', 'Stereographic']\n",
    "_note = ['', '', '_Custom-MAE']\n",
    "\n",
    "eval_dir = '..\\\\output\\\\{}_Regression_{}\\\\Graphical_Evaluation\\\\'.format(run, loss)\n",
    "\n",
    "if(not os.path.exists(eval_dir)):\n",
    "    os.makedirs(eval_dir)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(eval_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "net_results = [None, None, None]\n",
    "\n",
    "for i in range(3):\n",
    "    with open(eval_dir + '{}_Net{}_{}{}_Results.pickle'.format(dataset_name, net_index[i], APPENDIX[i], _note[i]), \"rb\") as fp:   # Unpickling\n",
    "        net_results[i] = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(net_results[i])\n",
    "    print()\n",
    "    print()\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as text\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "import tikzplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "Enable_Plotting = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots <a name = \"plots\">\n",
    "<p></a><a href = #Top>Up</a>\n",
    "<p><a href = #setup>Setup</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if Enable_Plotting:\n",
    "    #interesting_cases = [0, 3, 4, 5]\n",
    "    interesting_cases = [0, 5]\n",
    "    interesting_cases = [0, 5]\n",
    "    \n",
    "    net_len = len(net_results)\n",
    "    net_len = 3\n",
    "    \n",
    "    net_df = [[], [], []]\n",
    "    \n",
    "    for net_idx in range(net_len):\n",
    "        net = net_results[net_idx]\n",
    "        for run_idx in interesting_cases:\n",
    "            df = net[run_idx][1]\n",
    "            net_df[net_idx].append(df)\n",
    "            #print(df)\n",
    "    print(net_df[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net_df[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Elevation-Dependent Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_elev_dependent_angular_synthetic(dataframe, eval_path, eval_file, elevation_normalized = False, store_to_file = False):\n",
    "    elev_depenent_angular_error =  pd.DataFrame(data=None, index=None, columns=['elevation_angle', 'angular_error'])\n",
    "    \n",
    "    operation_df = dataframe\n",
    "    \n",
    "    dictionary = {}\n",
    "    \n",
    "    for index, row in operation_df.iterrows():\n",
    "        true_elevation = row['Elevation_true'] * 45.0 + 45.0 if elevation_normalized else row['Elevation_true']\n",
    "        key = round(true_elevation, 1)\n",
    "        if key not in dictionary.keys():\n",
    "            e = row['dot_angular_err_elevation']\n",
    "            a = 1\n",
    "            dictionary[key] = [e, a]\n",
    "            \n",
    "        else:\n",
    "            e = dictionary[key][0]\n",
    "            a = dictionary[key][1]\n",
    "            e = e + row['dot_angular_err_elevation']\n",
    "            a = a + 1\n",
    "            dictionary[key] = [e, a]\n",
    "            \n",
    "    for key, value in dictionary.items():\n",
    "        avg_error = value[0] / value[1]\n",
    "        elev_depenent_angular_error = elev_depenent_angular_error.append(pd.DataFrame(\n",
    "            data = [[key, avg_error, value[1]]], \n",
    "            index = None, \n",
    "            columns = ['elevation_angle', 'angular_error', 'No Images']))\n",
    "\n",
    "    elev_depenent_angular_error = elev_depenent_angular_error.sort_values(by=['elevation_angle'])\n",
    "\n",
    "    #cmse_real_df_error_azimuth.to_csv(evaluation_path + 'Net_568_Real_TD_Azimut_Fehler_in_Abhängigkeit_der_Elevation.csv', index = False)\n",
    "    if(store_to_file):\n",
    "        elev_depenent_angular_error.to_csv(eval_path + eval_file, index = False)\n",
    "    return elev_depenent_angular_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fast_elev_dependent_azimuth_synthetic(dataframe, eval_path, eval_file, elevation_normalized = False, store_to_file = False):\n",
    "    elev_depenent_angular_error =  pd.DataFrame(data=None, index=None, columns=['elevation_angle', 'azimuth_error'])\n",
    "    \n",
    "    operation_df = dataframe\n",
    "    \n",
    "    dictionary = {}\n",
    "    \n",
    "    for index, row in operation_df.iterrows():\n",
    "        true_elevation = row['Elevation_true'] * 45.0 + 45.0 if elevation_normalized else row['Elevation_true']\n",
    "        key = round(true_elevation, 1)\n",
    "        if key not in dictionary.keys():\n",
    "            e = row['Azimuth_err']\n",
    "            a = 1\n",
    "            dictionary[key] = [e, a]\n",
    "            \n",
    "        else:\n",
    "            e = dictionary[key][0]\n",
    "            a = dictionary[key][1]\n",
    "            e = e + row['Azimuth_err']\n",
    "            a = a + 1\n",
    "            dictionary[key] = [e, a]\n",
    "            \n",
    "    for key, value in dictionary.items():\n",
    "        avg_error = value[0] / value[1]\n",
    "        elev_depenent_angular_error = elev_depenent_angular_error.append(pd.DataFrame(\n",
    "            data = [[key, avg_error, value[1]]], \n",
    "            index = None, \n",
    "            columns = ['elevation_angle', 'azimuth_error', 'No Images']))\n",
    "\n",
    "    elev_depenent_angular_error = elev_depenent_angular_error.sort_values(by=['elevation_angle'])\n",
    "\n",
    "    #cmse_real_df_error_azimuth.to_csv(evaluation_path + 'Net_568_Real_TD_Azimut_Fehler_in_Abhängigkeit_der_Elevation.csv', index = False)\n",
    "    if(store_to_file):\n",
    "        elev_depenent_angular_error.to_csv(eval_path + eval_file, index = False)\n",
    "    return elev_depenent_angular_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fast_elev_dependent_stereographic_synthetic(dataframe, eval_path, eval_file, elevation_normalized = False, store_to_file = False):\n",
    "    elev_depenent_stereographic_error =  pd.DataFrame(data=None, index=None, columns=['elevation_angle', 's_x_error', 's_y_error'])\n",
    "    \n",
    "    operation_df = dataframe\n",
    "    \n",
    "    dictionary = {}\n",
    "    \n",
    "    for index, row in operation_df.iterrows():\n",
    "        true_elevation = row['Elevation_true'] * 45.0 + 45.0 if elevation_normalized else row['Elevation_true']\n",
    "        key = round(true_elevation, 0)\n",
    "        if key not in dictionary.keys():\n",
    "            e_x = row['S_x_err']\n",
    "            e_y = row['S_y_err']\n",
    "            a = 1\n",
    "            dictionary[key] = [e_x, e_y, a]\n",
    "            \n",
    "        else:\n",
    "            e_x = dictionary[key][0]\n",
    "            e_y = dictionary[key][1]\n",
    "            a = dictionary[key][2]\n",
    "            e_x = e_x + row['S_x_err']\n",
    "            e_y = e_x + row['S_y_err']\n",
    "            a = a + 1\n",
    "            dictionary[key] = [e_x, e_y, a]\n",
    "            \n",
    "    for key, value in dictionary.items():\n",
    "        avg_error_x = value[0] / value[2]\n",
    "        avg_error_y = value[1] / value[2]\n",
    "        elev_depenent_stereographic_error = elev_depenent_stereographic_error.append(pd.DataFrame(\n",
    "            data = [[key, avg_error_x, avg_error_y, value[2]]], \n",
    "            index = None, \n",
    "            columns = ['elevation_angle', 's_x_error', 's_y_error', 'No Images']))\n",
    "\n",
    "    elev_depenent_stereographic_error = elev_depenent_stereographic_error.sort_values(by=['elevation_angle'])\n",
    "\n",
    "    #cmse_real_df_error_azimuth.to_csv(evaluation_path + 'Net_568_Real_TD_Azimut_Fehler_in_Abhängigkeit_der_Elevation.csv', index = False)\n",
    "    if(store_to_file):\n",
    "        elev_depenent_stereographic_error.to_csv(eval_path + eval_file, index = False)\n",
    "    return elev_depenent_stereographic_error\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "def elev_dependent_angular_real(dataframe, eval_path, eval_file, elevation_normalized = False, store_to_file = False):\n",
    "    cmse_real_df_error_azimuth =  pd.DataFrame(data=None, index=None, columns=['elevation_angle', 'angular_error'])\n",
    "\n",
    "    operation_df = dataframe\n",
    "\n",
    "    dictionary = {}\n",
    "\n",
    "    # Laufe über alle Einträge\n",
    "    # Hashe Elevation_true\n",
    "    # Falls Hash nicht im dict -> eintrag im dict mit {fehler, anzahl = 1}\n",
    "    # Falls im dict -> fehler um aktuell erhöhen, anzahl++\n",
    "\n",
    "    # für fertigen df\n",
    "    # laufe über alle dict einträge\n",
    "    # df der df-liste mit key (elevation) und fehler (fehler/anzahl) eintragen\n",
    "\n",
    "    for index, row in operation_df.iterrows():\n",
    "        true_elevation = row['Elevation_true'] * 45.0 + 45.0 if elevation_normalized else row['Elevation_true']\n",
    "        key = round(true_elevation, 0)\n",
    "        if key not in dictionary.keys():\n",
    "            e = row['dot_angular_err_elevation']\n",
    "            a = 1\n",
    "            dictionary[key] = [e, a]\n",
    "\n",
    "        else:\n",
    "            e = dictionary[key][0]\n",
    "            a = dictionary[key][1]\n",
    "            e = e + row['dot_angular_err_elevation']\n",
    "            a = a + 1\n",
    "            dictionary[key] = [e, a]\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "        avg_error = value[0] / value[1]\n",
    "        cmse_real_df_error_azimuth = cmse_real_df_error_azimuth.append(pd.DataFrame(\n",
    "            data = [[key, avg_error, value[1]]], \n",
    "            index = None, \n",
    "            columns = ['elevation_angle', 'angular_error', 'No Images']))\n",
    "\n",
    "    cmse_real_df_error_azimuth = cmse_real_df_error_azimuth.sort_values(by=['elevation_angle'])\n",
    "\n",
    "    #cmse_real_df_error_azimuth.to_csv(evaluation_path + 'Net_568_Real_TD_Azimut_Fehler_in_Abhängigkeit_der_Elevation.csv', index = False)\n",
    "    if(store_to_file):\n",
    "        cmse_real_df_error_azimuth.to_csv(eval_path + eval_file, index = False)\n",
    "    return cmse_real_df_error_azimuth\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def elev_dependent_azimuth_real(dataframe, eval_path, eval_file, elevation_normalized = False, store_to_file = False):\n",
    "    cmse_real_df_error_azimuth =  pd.DataFrame(data=None, index=None, columns=['elevation_angle', 'azimuth_error'])\n",
    "\n",
    "    operation_df = dataframe\n",
    "\n",
    "    dictionary = {}\n",
    "\n",
    "    # Laufe über alle Einträge\n",
    "    # Hashe Elevation_true\n",
    "    # Falls Hash nicht im dict -> eintrag im dict mit {fehler, anzahl = 1}\n",
    "    # Falls im dict -> fehler um aktuell erhöhen, anzahl++\n",
    "\n",
    "    # für fertigen df\n",
    "    # laufe über alle dict einträge\n",
    "    # df der df-liste mit key (elevation) und fehler (fehler/anzahl) eintragen\n",
    "\n",
    "    for index, row in operation_df.iterrows():\n",
    "        true_elevation = row['Elevation_true'] * 45.0 + 45.0 if elevation_normalized else row['Elevation_true']\n",
    "        key = round(true_elevation, 0)\n",
    "        if key not in dictionary.keys():\n",
    "            e = row['Azimuth_err']\n",
    "            a = 1\n",
    "            dictionary[key] = [e, a]\n",
    "\n",
    "        else:\n",
    "            e = dictionary[key][0]\n",
    "            a = dictionary[key][1]\n",
    "            e = e + row['Azimuth_err']\n",
    "            a = a + 1\n",
    "            dictionary[key] = [e, a]\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "        avg_error = value[0] / value[1]\n",
    "        cmse_real_df_error_azimuth = cmse_real_df_error_azimuth.append(pd.DataFrame(\n",
    "            data = [[key, avg_error, value[1]]], \n",
    "            index = None, \n",
    "            columns = ['elevation_angle', 'azimuth_error', 'No Images']))\n",
    "\n",
    "    cmse_real_df_error_azimuth = cmse_real_df_error_azimuth.sort_values(by=['elevation_angle'])\n",
    "\n",
    "    #cmse_real_df_error_azimuth.to_csv(evaluation_path + 'Net_568_Real_TD_Azimut_Fehler_in_Abhängigkeit_der_Elevation.csv', index = False)\n",
    "    if(store_to_file):\n",
    "        cmse_real_df_error_azimuth.to_csv(eval_path + eval_file, index = False)\n",
    "    return cmse_real_df_error_azimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_elev_dependent_angular_synthetic(net_df[0][0], eval_dir, '99_train-synth_test-synth.csv', store_to_file = True)\n",
    "fast_elev_dependent_angular_synthetic(net_df[1][0], eval_dir, '204_train-synth_test-synth.csv', elevation_normalized = True, store_to_file = True)\n",
    "fast_elev_dependent_angular_synthetic(net_df[2][0], eval_dir, '195_train-synth_test-synth.csv', store_to_file = True)\n",
    "\n",
    "fast_elev_dependent_azimuth_synthetic(net_df[0][0], eval_dir, 'azimuth_99_train-synth_test-synth.csv', store_to_file = True)\n",
    "fast_elev_dependent_azimuth_synthetic(net_df[1][0], eval_dir, 'azimuth_204_train-synth_test-synth.csv', elevation_normalized = True, store_to_file = True)\n",
    "fast_elev_dependent_azimuth_synthetic(net_df[2][0], eval_dir, 'azimuth_195_train-synth_test-synth.csv', store_to_file = True)\n",
    "\n",
    "elev_dependent_angular_real(net_df[0][1], eval_dir, '99_train-mixed_test-real.csv', store_to_file = True)\n",
    "elev_dependent_angular_real(net_df[1][1], eval_dir, '204_train-mixed_test-real.csv', elevation_normalized = True, store_to_file = True)\n",
    "elev_dependent_angular_real(net_df[2][1], eval_dir, '195_train-mixed_test-real.csv', store_to_file = True)\n",
    "\n",
    "elev_dependent_azimuth_real(net_df[0][1], eval_dir, 'azimuth_99_train-mixed_test-real.csv', store_to_file = True)\n",
    "elev_dependent_azimuth_real(net_df[1][1], eval_dir, 'azimuth_204_train-mixed_test-real.csv', elevation_normalized = True, store_to_file = True)\n",
    "elev_dependent_azimuth_real(net_df[2][1], eval_dir, 'azimuth_195_train-mixed_test-real.csv', store_to_file = True)\n",
    "\n",
    "fast_elev_dependent_stereographic_synthetic(net_df[2][0], eval_dir, 'stereographic_195_train-synth_test-synth.csv', store_to_file = True)\n",
    "fast_elev_dependent_stereographic_synthetic(net_df[2][1], eval_dir, 'stereographic_195_train-mixed_test-real.csv', store_to_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_Plot(title, eval_file, save = True):\n",
    "    #fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (30,10))\n",
    "    fig, ax = plt.subplots(figsize = (10,5))\n",
    "    \n",
    "    syn1 = fast_elev_dependent_angular_synthetic(net_df[0][0], None, None) # Net 99 Tr syn Te syn\n",
    "    syn1.set_index('elevation_angle')\n",
    "    syn2 = fast_elev_dependent_angular_synthetic(net_df[1][0], None, None, elevation_normalized = True) # Net 204 Tr syn Te syn\n",
    "    syn2.set_index('elevation_angle')\n",
    "    syn3 = fast_elev_dependent_angular_synthetic(net_df[2][0], None, None) # Net 195 Tr syn Te syn\n",
    "    syn3.set_index('elevation_angle')\n",
    "    \n",
    "    syn_azi_1 = fast_elev_dependent_azimuth_synthetic(net_df[0][0], None, None) # Net 99 Tr syn Te syn\n",
    "    syn_azi_1.set_index('elevation_angle')\n",
    "    syn_azi_2 = fast_elev_dependent_azimuth_synthetic(net_df[1][0], None, None, elevation_normalized = True) # Net 204 Tr syn Te syn\n",
    "    syn_azi_2.set_index('elevation_angle')\n",
    "    syn_azi_3 = fast_elev_dependent_azimuth_synthetic(net_df[2][0], None, None) # Net 195 Tr syn Te syn\n",
    "    syn_azi_3.set_index('elevation_angle')\n",
    "\n",
    "    re_azi_1 = elev_dependent_azimuth_real(net_df[0][1], None, None) # Net 99 Tr mix Te real\n",
    "    re_azi_1.set_index('elevation_angle')\n",
    "    re_azi_2 = elev_dependent_azimuth_real(net_df[1][1], None, None, elevation_normalized = True) # Net 204 Tr mix Te real\n",
    "    re_azi_2.set_index('elevation_angle')\n",
    "    re_azi_3 = elev_dependent_azimuth_real(net_df[2][1], None, None) # Net 195 Tr mix Te real\n",
    "    re_azi_3.set_index('elevation_angle')\n",
    "    \n",
    "    stereo_data = fast_elev_dependent_stereographic_synthetic(net_df[2][0], None, None)\n",
    "\n",
    "    \n",
    "    syn_dfs = [syn1[['elevation_angle', 'angular_error']], syn2['angular_error'], syn3['angular_error']]\n",
    "    syn_azi_dfs = [syn_azi_1[['elevation_angle', 'azimuth_error']], syn_azi_2['azimuth_error']]\n",
    "    re_azi_dfs = [re_azi_1[['elevation_angle', 'azimuth_error']], re_azi_2['azimuth_error']]\n",
    "    stereo_dfs = [stereo_data[['elevation_angle', 's_x_error', 's_y_error']]]\n",
    "    \n",
    "    synth_plot = pd.concat(syn_dfs, join = 'outer', axis = 1)\n",
    "    synth_plot.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    syn_azi_plot = pd.concat(syn_azi_dfs, join = 'outer', axis = 1)\n",
    "    syn_azi_plot.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    re_azi_plot = pd.concat(re_azi_dfs, join = 'outer', axis = 1)\n",
    "    re_azi_plot.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    stereo_plot = pd.concat(stereo_dfs, join = 'outer', axis = 1)\n",
    "    stereo_plot.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    cmap = cm.get_cmap('tab20c')\n",
    "    #colours = [cmap(0.), cmap(0.05), cmap(0.1), cmap(0.15), cmap(0.2), cmap(0.25), cmap(0.3), cmap(0.35)]\n",
    "    colours = [cmap(0.4), cmap(0.45)]\n",
    "        \n",
    "    #synth_plot.plot(x = 'elevation_angle', kind = 'bar', ax = ax)\n",
    "    #syn_azi_plot.plot(x = 'elevation_angle', kind = 'bar', ax = ax)\n",
    "    stereo_plot.plot(x = 'elevation_angle', kind = 'bar', ax = ax, color = ['green', 'lightgreen'])\n",
    "    #ax[1][1].axis('off')\n",
    "    \n",
    "    #fig.subplots_adjust(hspace = 0.3)\n",
    "    \n",
    "    ax.grid(True, ls = '--', axis = 'y')\n",
    "    #ax[0][1].grid(True, ls = '--', axis = 'y')\n",
    "    #ax[1][0].grid(True, ls = '--', axis = 'y')\n",
    "    #ax[1][1].grid(True, ls = '--', axis = 'y')\n",
    "    \n",
    "    ax.set_ylabel('Mean Angular Error')\n",
    "    ax.set_title('Elevation-dependent angular error on synth. test data using synth. training data')\n",
    "    #ax[0][1].set_ylabel('Mean Azimuth Error')\n",
    "    #ax[0][1].set_title('Elevation-dependent azimuth error on synth. test data using synth. training data')\n",
    "    #ax[1][0].set_ylabel('Mean Stereographic Error')\n",
    "    #ax[1][0].set_title('Elevation-dependent stereographic error sx, sy on synth. test data using synth. training data')\n",
    "    #ax[1][1].set_ylabel('Mean Azimuth Error')\n",
    "    #ax[1][1].set_title('Elevation-dependent azimuth error on real test data using mixed training data')\n",
    "    \n",
    "    ax.set_xlabel('Elevation Angle')\n",
    "    #ax[0][1].set_xlabel('Elevation Angle')\n",
    "    #ax[1][0].set_xlabel('Elevation Angle')\n",
    "    #ax[1][1].set_xlabel('Elevation Angle')\n",
    "    \n",
    "    #ax.legend(['Net\\_p,t', 'Net\\_|p,t|', 'Net\\_sx,sy'])\n",
    "    ax.legend(['mean s\\_x error', 'mean s\\_y error'])\n",
    "    \n",
    "    for o in fig.findobj(text.Text):\n",
    "        o.set_fontstyle('italic')\n",
    "    \n",
    "    if(save):\n",
    "        tikzplotlib.save('{}.tex'.format(eval_file))\n",
    "        plt.savefig('{}.png'.format(eval_file), format = 'png', bbox_inches = \"tight\", dpi = 300)\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if Enable_Plotting:\n",
    "    tsts_plot = do_Plot('Elevation-Dependent Mean Angular Error', eval_dir + 'Ele-Dep-Angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_test]",
   "language": "python",
   "name": "conda-env-tf_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
