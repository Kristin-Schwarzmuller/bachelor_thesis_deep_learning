{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Training on GPU 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links <a name = \"Top\"></a>\n",
    "\n",
    "<ol>\n",
    "<li><a href = #setup>Begin Training</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Conda Environment: tf_ks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "print('Current Conda Environment: {}'.format(os.environ['CONDA_DEFAULT_ENV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  2 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7606523854522899611\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9105744200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1914957022813141045\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9104897474\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10480317982496326279\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum für Training-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TrainingSet(Enum):\n",
    "    SYNTHETIC = 1\n",
    "    REAL = 2\n",
    "    MIXED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "\n",
    "* <i>SSD</i>, falls genug Speicher auf SSD im SymLink <i>fast_output</i> verfügbar ist\n",
    "* <i>HDD</i>, falls möglicherweise zu wenig SSD-Speicher verfügbar ist $\\rightarrow$ <i>output</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class OutputDirectory(IntEnum):\n",
    "    HDD = 0\n",
    "    SSD = 1\n",
    "    \n",
    "output_path = ['output', 'fast_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Label_Type into suitable label names.\n",
    "$\\Rightarrow$ Angular / Normalized $\\rightarrow$ ['Elevation', 'Azimuth']\n",
    "\n",
    "$\\Rightarrow$ Stereographic $\\rightarrow$ ['S_x', 'S_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Label_Names(label_type):\n",
    "    if label_type == 'Angular' or label_type == 'Normalized':\n",
    "        return ['Elevation', 'Azimuth']\n",
    "    elif label_type == 'Stereographic':\n",
    "        return ['S_x', 'S_y']\n",
    "    else:\n",
    "        assert(True, 'LabelType Invalid')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reduction_Metric(metric):\n",
    "    \n",
    "    if metric == 'custom_mae':\n",
    "        return [custom_mae]\n",
    "    elif metric == 'tf.keras.metrics.MeanAbsoluteError()':\n",
    "        return [tf.keras.metrics.MeanAbsoluteError()]\n",
    "    elif metric == 'circular_mae':\n",
    "        return [circular_mae]\n",
    "    elif metric == 'mean_squared_error':\n",
    "        return ['mean_squared_error']\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'keras.optimizers.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Nadam'>\":\n",
    "        return Nadam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Adagard'>\":\n",
    "        return Adagard\n",
    "    elif optimizer == \"<class 'keras.optimizers.RMSprop'>\":\n",
    "        return RMSprop\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsset-Typ nach String Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_to_string(ts):\n",
    "    if ts == TrainingSet.SYNTHETIC:\n",
    "        return 'Synth'\n",
    "    elif ts == TrainingSet.REAL:\n",
    "        return 'Real'\n",
    "    elif ts == TrainingSet.MIXED:\n",
    "        return 'Mixed'\n",
    "    else:\n",
    "        print('Unknown TrainingSet')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(batch_size, num_samples, label_type):\n",
    "    # if Block für synthetische Daten, um nur auf realen Daten zu trainieren _USE_SYNTHETIC_TRAIN_DATA\n",
    "    # 1. lege df_train und df_valid als leere Liste an\n",
    "    # 2. If-block um Zeile df = ... bis df_valid\n",
    "    \n",
    "    if trainingset == TrainingSet.SYNTHETIC:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "    elif trainingset == TrainingSet.MIXED:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train_real = df_shuffled_real[0: int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid_real = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train_real.shape[0]])\n",
    "        df_train = df_train.drop(df_train.index[df_train.shape[0] - df_train_real.shape[0] : df_train.shape[0]])\n",
    "        df_valid = df_valid.drop(df_valid.index[df_valid.shape[0] - df_valid_real.shape[0] : df_valid.shape[0]])\n",
    "        df_train = df_train.append(df_train_real)\n",
    "        df_valid= df_valid.append(df_valid_real)\n",
    "    \n",
    "    elif trainingset == TrainingSet.REAL: # Add check for num_samples, once the real dataset increases\n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train = df_shuffled_real[0 : int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train.shape[0]])\n",
    "        \n",
    "    else:\n",
    "        print('Create_Data :: should not have reached here')\n",
    "        \n",
    "\n",
    "        \n",
    "    if _USE_DATA_AUGMENTATION:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.25, 0.75),\n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    print('Y-Col: {}'.format(get_Label_Names(label_type)))\n",
    "    print('Train Data Generator: ', end = '')\n",
    "    \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    print('Validation Data Generator: ', end = '')\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Modell (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_model_fine(x, y, x_val, y_val, params):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data(params['batch_size'], params['samples'], params['label_type'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    "    dropout_rate = params['dropout']\n",
    "    first_neuron = params['first_neuron']\n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = params['leaky_alpha'])\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    cnn = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "    \n",
    "    for layer in cnn.layers[:15]:\n",
    "        layer.trainable = False\n",
    "        #print(layer.name, layer.trainable)\n",
    "        \n",
    "    print('_________________________________________________________________')\n",
    "    print('{:>16} {:>16}'.format('Network Layer', 'Trainable'))\n",
    "    print('=================================================================')\n",
    "    for layer in cnn.layers:\n",
    "        print('{:>16} {:>16}'.format(layer.name, layer.trainable))\n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    model.add(cnn)\n",
    "    \n",
    "    fc = Sequential()\n",
    "    fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512)\n",
    "    \n",
    "    fc.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    print('Number Hidden Layers {}'.format(params['hidden_layers']))\n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(params['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.load_weights(_MODEL_DIR + _MODEL_TO_LOAD)\n",
    "    model.add(fc)\n",
    "    print('Fully Connected Layers added to Base Network')\n",
    "    \n",
    "    print('Using Loss: {} \\nand Reduction Metric: {}'.format(\n",
    "        params['loss_function'], \n",
    "        get_Reduction_Metric(params['reduction_metric'])))\n",
    "    \n",
    "    model.compile(\n",
    "        #optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])*1e-2),\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer']) * 1e-3),\n",
    "        loss = params['loss_function'],\n",
    "        metrics = get_Reduction_Metric(params['reduction_metric'])\n",
    "    )\n",
    "    print('Model was compiled')\n",
    "    print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = _LOG_DIR + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        monitor =  params['monitor_value'],\n",
    "        verbose = 1,\n",
    "        save_weights_only = False,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    print('Checkpointer was created')\n",
    "    \n",
    "    csv_logger = callbacks.CSVLogger(\n",
    "        filename = _LOG_DIR + 'CNN_Base_{}_Logger_{}.csv'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        separator = ',',\n",
    "        append = False\n",
    "    )\n",
    "    print('CSV Logger was created')\n",
    "\n",
    "    lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 13,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        min_delta = 0.0001\n",
    "    )\n",
    "    print('Learning Rate Reducer was created')\n",
    "    \n",
    "    early_stopper = callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0,\n",
    "        #patience = 15,\n",
    "        patience = 20,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    print('Early Stopper was created')\n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_data = valid_generator,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        callbacks = [checkpointer, csv_logger, lr_reducer, early_stopper],\n",
    "        epochs = params['epochs'],\n",
    "        workers = 8\n",
    "    )\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feinoptimierung <a name = \"setup\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "global_hyper_parameter = {\n",
    "    'samples': None,\n",
    "    'epochs': None,\n",
    "    'batch_size': None,\n",
    "    'optimizer': None,\n",
    "    'lr': None,\n",
    "    'first_neuron': None,\n",
    "    'dropout': None,\n",
    "    'activation': None,\n",
    "    'leaky_alpha': None,\n",
    "    'hidden_layers': None,\n",
    "    # beginning from here, Values should only contain one single entry:\n",
    "    # ===============================================================\n",
    "    'label_type': ['Normalized'], # Stereographic, Angular, Normalized\n",
    "    'loss_function': None,\n",
    "    'reduction_metric': None,\n",
    "    'monitor_value': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RUN = 'SYNTH'\n",
    "_LOSS = 'MSE'\n",
    "_DATASET_NAME = '2020-05-28'\n",
    "_DEVICE = 'TITAN_GPU0'\n",
    "\n",
    "storage = OutputDirectory.SSD # 'fast_output' if ssd storage may suffice, 'output' otherwise\n",
    "\n",
    "if global_hyper_parameter['label_type'][0] == 'Stereographic':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_stereographic.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_stereographic.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Angular':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Normalized':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_normalized.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_normalized.csv'\n",
    "    \n",
    "else:\n",
    "    assert(True, 'Label Type Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = TrainingSet.SYNTHETIC\n",
    "_USE_DATA_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n"
     ]
    }
   ],
   "source": [
    "_IMAGE_DIR = '..\\\\dataset_mm\\\\{}\\\\'.format(_DATASET_NAME)\n",
    "_CSV_FILE = _IMAGE_DIR + _CSV_SYNTH_FILE_NAME\n",
    "_CSV_FILE_REAL = _IMAGE_DIR + _CSV_REAL_FILE_NAME\n",
    "\n",
    "_note = '_Custom-MAE'\n",
    "\n",
    "_MODEL_DIR = '..\\\\output\\\\{}_Regression_{}\\\\{}_{}_Base{}\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "_NET_DIR = '{}_Regression_{}\\\\{}_{}_Top_1{}\\\\{}_TD\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note, trainingset_to_string(trainingset))\n",
    "_LOG_DIR = '..\\\\{}\\\\{}'.format(output_path[storage], _NET_DIR)\n",
    "\n",
    "if(not os.path.exists(_LOG_DIR)):\n",
    "    os.makedirs(_LOG_DIR)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(_LOG_DIR))\n",
    "\n",
    "device_file = open(_LOG_DIR + '{}.txt'.format(_DEVICE), \"a+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 FC-Gewichte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: ..\\output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Base_Custom-MAE\\..\\2020-05-28_Normalized_Base_Custom-MAE_Results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>loss</th>\n",
       "      <th>custom_mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_custom_mae</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>label_type</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>lr</th>\n",
       "      <th>monitor_value</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>reduction_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>210</td>\n",
       "      <td>10/29/20-193146</td>\n",
       "      <td>10/29/20-193201</td>\n",
       "      <td>14.936548</td>\n",
       "      <td>10.795315</td>\n",
       "      <td>0.595088</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.214072</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>10/29/20-192905</td>\n",
       "      <td>10/29/20-192915</td>\n",
       "      <td>10.025699</td>\n",
       "      <td>5.247484</td>\n",
       "      <td>0.605640</td>\n",
       "      <td>0.100757</td>\n",
       "      <td>0.215078</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>180</td>\n",
       "      <td>10/29/20-192658</td>\n",
       "      <td>10/29/20-192709</td>\n",
       "      <td>11.400928</td>\n",
       "      <td>2.732533</td>\n",
       "      <td>0.477908</td>\n",
       "      <td>0.104949</td>\n",
       "      <td>0.220771</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>300</td>\n",
       "      <td>10/29/20-194850</td>\n",
       "      <td>10/29/20-194902</td>\n",
       "      <td>12.204615</td>\n",
       "      <td>22.274675</td>\n",
       "      <td>0.953535</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.238330</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>211</td>\n",
       "      <td>10/29/20-193201</td>\n",
       "      <td>10/29/20-193216</td>\n",
       "      <td>14.956688</td>\n",
       "      <td>42.028975</td>\n",
       "      <td>0.893910</td>\n",
       "      <td>0.116852</td>\n",
       "      <td>0.238365</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>240</td>\n",
       "      <td>10/29/20-193755</td>\n",
       "      <td>10/29/20-193805</td>\n",
       "      <td>10.143713</td>\n",
       "      <td>4.793068</td>\n",
       "      <td>0.598642</td>\n",
       "      <td>0.116115</td>\n",
       "      <td>0.239593</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>183</td>\n",
       "      <td>10/29/20-192725</td>\n",
       "      <td>10/29/20-192733</td>\n",
       "      <td>7.764266</td>\n",
       "      <td>1.332591</td>\n",
       "      <td>0.489436</td>\n",
       "      <td>0.118337</td>\n",
       "      <td>0.241562</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>285</td>\n",
       "      <td>10/29/20-194634</td>\n",
       "      <td>10/29/20-194643</td>\n",
       "      <td>8.401402</td>\n",
       "      <td>10.567411</td>\n",
       "      <td>0.992507</td>\n",
       "      <td>0.117695</td>\n",
       "      <td>0.242329</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>10/29/20-192915</td>\n",
       "      <td>10/29/20-192925</td>\n",
       "      <td>9.898231</td>\n",
       "      <td>18.965731</td>\n",
       "      <td>0.943573</td>\n",
       "      <td>0.118273</td>\n",
       "      <td>0.242819</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>345</td>\n",
       "      <td>10/29/20-195613</td>\n",
       "      <td>10/29/20-195625</td>\n",
       "      <td>12.194129</td>\n",
       "      <td>21.463338</td>\n",
       "      <td>0.990857</td>\n",
       "      <td>0.118296</td>\n",
       "      <td>0.245393</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0            start              end   duration       loss  \\\n",
       "210         210  10/29/20-193146  10/29/20-193201  14.936548  10.795315   \n",
       "195         195  10/29/20-192905  10/29/20-192915  10.025699   5.247484   \n",
       "180         180  10/29/20-192658  10/29/20-192709  11.400928   2.732533   \n",
       "300         300  10/29/20-194850  10/29/20-194902  12.204615  22.274675   \n",
       "211         211  10/29/20-193201  10/29/20-193216  14.956688  42.028975   \n",
       "240         240  10/29/20-193755  10/29/20-193805  10.143713   4.793068   \n",
       "183         183  10/29/20-192725  10/29/20-192733   7.764266   1.332591   \n",
       "285         285  10/29/20-194634  10/29/20-194643   8.401402  10.567411   \n",
       "196         196  10/29/20-192915  10/29/20-192925   9.898231  18.965731   \n",
       "345         345  10/29/20-195613  10/29/20-195625  12.194129  21.463338   \n",
       "\n",
       "     custom_mae  val_loss  val_custom_mae activation  batch_size  dropout  \\\n",
       "210    0.595088  0.101124        0.214072       relu          32     0.25   \n",
       "195    0.605640  0.100757        0.215078       relu          32     0.25   \n",
       "180    0.477908  0.104949        0.220771       relu          32     0.25   \n",
       "300    0.953535  0.112903        0.238330       relu          64     0.25   \n",
       "211    0.893910  0.116852        0.238365       relu          32     0.25   \n",
       "240    0.598642  0.116115        0.239593       relu          32     0.50   \n",
       "183    0.489436  0.118337        0.241562       relu          32     0.25   \n",
       "285    0.992507  0.117695        0.242329       relu          64     0.25   \n",
       "196    0.943573  0.118273        0.242819       relu          32     0.25   \n",
       "345    0.990857  0.118296        0.245393       relu          64     0.50   \n",
       "\n",
       "     first_neuron  hidden_layers  label_type       loss_function  lr  \\\n",
       "210          4096              0  Normalized  mean_squared_error   1   \n",
       "195          2048              0  Normalized  mean_squared_error   1   \n",
       "180          1024              0  Normalized  mean_squared_error   1   \n",
       "300          4096              0  Normalized  mean_squared_error   1   \n",
       "211          4096              0  Normalized  mean_squared_error   2   \n",
       "240          2048              0  Normalized  mean_squared_error   1   \n",
       "183          1024              1  Normalized  mean_squared_error   1   \n",
       "285          2048              0  Normalized  mean_squared_error   1   \n",
       "196          2048              0  Normalized  mean_squared_error   2   \n",
       "345          4096              0  Normalized  mean_squared_error   1   \n",
       "\n",
       "      monitor_value                                          optimizer  \\\n",
       "210  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "195  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "180  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "300  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "211  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "240  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "183  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "285  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "196  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "345  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "\n",
       "    reduction_metric  \n",
       "210       custom_mae  \n",
       "195       custom_mae  \n",
       "180       custom_mae  \n",
       "300       custom_mae  \n",
       "211       custom_mae  \n",
       "240       custom_mae  \n",
       "183       custom_mae  \n",
       "285       custom_mae  \n",
       "196       custom_mae  \n",
       "345       custom_mae  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_results = _MODEL_DIR + '..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "df = pd.read_csv(base_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "sort_value = df['monitor_value'][0]\n",
    "df = df.sort_values(sort_value, axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(base_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(top_results_index):\n",
    "    \n",
    "    #     Adam = RMSprop + Momentum (lr=0.001)\n",
    "    #     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "    #     RMSprop = (lr=0.001)\n",
    "    #     SGD = (lr=0.01)\n",
    "    #     Adagrad\n",
    "\n",
    "    hyper_parameter = global_hyper_parameter\n",
    "\n",
    "    hyper_parameter['samples'] = [100000]\n",
    "    hyper_parameter['epochs'] = [400]\n",
    "    hyper_parameter['batch_size'] = [df.iloc[top_results_index]['batch_size']]\n",
    "    hyper_parameter['optimizer'] = [make_optimizer(df.loc[top_results_index]['optimizer'])]\n",
    "    hyper_parameter['lr'] = [df.iloc[top_results_index]['lr']]\n",
    "    hyper_parameter['first_neuron'] = [df.iloc[top_results_index]['first_neuron']]\n",
    "    hyper_parameter['dropout'] = [df.iloc[top_results_index]['dropout']]\n",
    "    hyper_parameter['activation'] = [df.iloc[top_results_index]['activation']]\n",
    "    hyper_parameter['leaky_alpha'] = [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "    hyper_parameter['hidden_layers'] = [df.iloc[top_results_index]['hidden_layers']]\n",
    "    \n",
    "    hyper_parameter['loss_function'] = [df.iloc[top_results_index]['loss_function']]\n",
    "    hyper_parameter['reduction_metric'] = [df.iloc[top_results_index]['reduction_metric']]\n",
    "    hyper_parameter['monitor_value'] = [df.iloc[top_results_index]['monitor_value']]\n",
    "\n",
    "    return hyper_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 4096, 'hidden_layers': 0, 'label_type': 'Normalized', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "Y-Col: ['Elevation', 'Azimuth']\n",
      "Train Data Generator: Found 80000 validated image filenames.\n",
      "Validation Data Generator: Found 20000 validated image filenames.\n",
      "Steps per Epoch: 2500, Validation Steps: 625\n",
      "_________________________________________________________________\n",
      "   Network Layer        Trainable\n",
      "=================================================================\n",
      "         input_1                0\n",
      "    block1_conv1                0\n",
      "    block1_conv2                0\n",
      "     block1_pool                0\n",
      "    block2_conv1                0\n",
      "    block2_conv2                0\n",
      "     block2_pool                0\n",
      "    block3_conv1                0\n",
      "    block3_conv2                0\n",
      "    block3_conv3                0\n",
      "     block3_pool                0\n",
      "    block4_conv1                0\n",
      "    block4_conv2                0\n",
      "    block4_conv3                0\n",
      "     block4_pool                0\n",
      "    block5_conv1                1\n",
      "    block5_conv2                1\n",
      "    block5_conv3                1\n",
      "     block5_pool                1\n",
      "_________________________________________________________________\n",
      "\n",
      "Number Hidden Layers 0\n",
      "Fully Connected Layers added to Base Network\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x000002C0B32EEAF8>]\n",
      "Model was compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 2)                 102772738 \n",
      "=================================================================\n",
      "Total params: 117,487,426\n",
      "Trainable params: 109,852,162\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Checkpointer was created\n",
      "CSV Logger was created\n",
      "Learning Rate Reducer was created\n",
      "Early Stopper was created\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2500 steps, validate for 625 steps\n",
      "Epoch 1/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.2455 - custom_mae: 0.4006\n",
      "Epoch 00001: val_custom_mae improved from inf to 0.29500, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 369s 148ms/step - loss: 0.2455 - custom_mae: 0.4006 - val_loss: 0.1592 - val_custom_mae: 0.2950\n",
      "Epoch 2/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.1380 - custom_mae: 0.2731\n",
      "Epoch 00002: val_custom_mae improved from 0.29500 to 0.22660, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.1380 - custom_mae: 0.2731 - val_loss: 0.1042 - val_custom_mae: 0.2266\n",
      "Epoch 3/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.1058 - custom_mae: 0.2293\n",
      "Epoch 00003: val_custom_mae improved from 0.22660 to 0.19716, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.1058 - custom_mae: 0.2293 - val_loss: 0.0849 - val_custom_mae: 0.1972\n",
      "Epoch 4/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0907 - custom_mae: 0.2069\n",
      "Epoch 00004: val_custom_mae improved from 0.19716 to 0.17795, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0907 - custom_mae: 0.2069 - val_loss: 0.0734 - val_custom_mae: 0.1780\n",
      "Epoch 5/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0814 - custom_mae: 0.1924\n",
      "Epoch 00005: val_custom_mae improved from 0.17795 to 0.16873, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0814 - custom_mae: 0.1924 - val_loss: 0.0679 - val_custom_mae: 0.1687\n",
      "Epoch 6/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0763 - custom_mae: 0.1832\n",
      "Epoch 00006: val_custom_mae improved from 0.16873 to 0.15941, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0763 - custom_mae: 0.1832 - val_loss: 0.0631 - val_custom_mae: 0.1594\n",
      "Epoch 7/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0715 - custom_mae: 0.1755\n",
      "Epoch 00007: val_custom_mae improved from 0.15941 to 0.15383, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0715 - custom_mae: 0.1755 - val_loss: 0.0604 - val_custom_mae: 0.1538\n",
      "Epoch 8/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0686 - custom_mae: 0.1696\n",
      "Epoch 00008: val_custom_mae improved from 0.15383 to 0.14902, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0686 - custom_mae: 0.1696 - val_loss: 0.0581 - val_custom_mae: 0.1490\n",
      "Epoch 9/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0654 - custom_mae: 0.1643\n",
      "Epoch 00009: val_custom_mae improved from 0.14902 to 0.14247, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0654 - custom_mae: 0.1643 - val_loss: 0.0547 - val_custom_mae: 0.1425\n",
      "Epoch 10/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0629 - custom_mae: 0.1594\n",
      "Epoch 00010: val_custom_mae improved from 0.14247 to 0.14078, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0629 - custom_mae: 0.1594 - val_loss: 0.0535 - val_custom_mae: 0.1408\n",
      "Epoch 11/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0610 - custom_mae: 0.1559\n",
      "Epoch 00011: val_custom_mae improved from 0.14078 to 0.13671, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 291s 116ms/step - loss: 0.0610 - custom_mae: 0.1559 - val_loss: 0.0516 - val_custom_mae: 0.1367\n",
      "Epoch 12/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0590 - custom_mae: 0.1522\n",
      "Epoch 00012: val_custom_mae improved from 0.13671 to 0.13299, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0590 - custom_mae: 0.1522 - val_loss: 0.0499 - val_custom_mae: 0.1330\n",
      "Epoch 13/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0570 - custom_mae: 0.1488\n",
      "Epoch 00013: val_custom_mae improved from 0.13299 to 0.13137, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0570 - custom_mae: 0.1488 - val_loss: 0.0492 - val_custom_mae: 0.1314\n",
      "Epoch 14/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0554 - custom_mae: 0.1458\n",
      "Epoch 00014: val_custom_mae improved from 0.13137 to 0.12808, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0554 - custom_mae: 0.1458 - val_loss: 0.0477 - val_custom_mae: 0.1281\n",
      "Epoch 15/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0542 - custom_mae: 0.1434\n",
      "Epoch 00015: val_custom_mae improved from 0.12808 to 0.12795, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0542 - custom_mae: 0.1434 - val_loss: 0.0476 - val_custom_mae: 0.1279\n",
      "Epoch 16/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0529 - custom_mae: 0.1408\n",
      "Epoch 00016: val_custom_mae improved from 0.12795 to 0.12464, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0529 - custom_mae: 0.1409 - val_loss: 0.0463 - val_custom_mae: 0.1246\n",
      "Epoch 17/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0514 - custom_mae: 0.1385\n",
      "Epoch 00017: val_custom_mae improved from 0.12464 to 0.12169, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0514 - custom_mae: 0.1385 - val_loss: 0.0446 - val_custom_mae: 0.1217\n",
      "Epoch 18/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0506 - custom_mae: 0.1368\n",
      "Epoch 00018: val_custom_mae improved from 0.12169 to 0.12064, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0506 - custom_mae: 0.1368 - val_loss: 0.0437 - val_custom_mae: 0.1206\n",
      "Epoch 19/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0497 - custom_mae: 0.1349\n",
      "Epoch 00019: val_custom_mae did not improve from 0.12064\n",
      "2500/2500 [==============================] - 285s 114ms/step - loss: 0.0497 - custom_mae: 0.1349 - val_loss: 0.0440 - val_custom_mae: 0.1213\n",
      "Epoch 20/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0487 - custom_mae: 0.1330\n",
      "Epoch 00020: val_custom_mae improved from 0.12064 to 0.11853, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 291s 116ms/step - loss: 0.0487 - custom_mae: 0.1330 - val_loss: 0.0432 - val_custom_mae: 0.1185\n",
      "Epoch 21/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0480 - custom_mae: 0.1317 - E - ETA: 0s - loss: 0.0480 - \n",
      "Epoch 00021: val_custom_mae improved from 0.11853 to 0.11738, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 291s 116ms/step - loss: 0.0480 - custom_mae: 0.1317 - val_loss: 0.0424 - val_custom_mae: 0.1174\n",
      "Epoch 22/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0468 - custom_mae: 0.1295\n",
      "Epoch 00022: val_custom_mae improved from 0.11738 to 0.11525, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0468 - custom_mae: 0.1294 - val_loss: 0.0412 - val_custom_mae: 0.1153\n",
      "Epoch 23/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0461 - custom_mae: 0.1285\n",
      "Epoch 00023: val_custom_mae did not improve from 0.11525\n",
      "2500/2500 [==============================] - 286s 114ms/step - loss: 0.0461 - custom_mae: 0.1285 - val_loss: 0.0410 - val_custom_mae: 0.1155\n",
      "Epoch 24/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0452 - custom_mae: 0.1264\n",
      "Epoch 00024: val_custom_mae improved from 0.11525 to 0.11460, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 291s 116ms/step - loss: 0.0452 - custom_mae: 0.1264 - val_loss: 0.0409 - val_custom_mae: 0.1146\n",
      "Epoch 25/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0447 - custom_mae: 0.1254\n",
      "Epoch 00025: val_custom_mae improved from 0.11460 to 0.11208, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0447 - custom_mae: 0.1254 - val_loss: 0.0397 - val_custom_mae: 0.1121\n",
      "Epoch 26/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0442 - custom_mae: 0.1246\n",
      "Epoch 00026: val_custom_mae improved from 0.11208 to 0.11117, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 291s 116ms/step - loss: 0.0442 - custom_mae: 0.1246 - val_loss: 0.0395 - val_custom_mae: 0.1112\n",
      "Epoch 27/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0434 - custom_mae: 0.1230\n",
      "Epoch 00027: val_custom_mae improved from 0.11117 to 0.11100, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 291s 116ms/step - loss: 0.0434 - custom_mae: 0.1230 - val_loss: 0.0391 - val_custom_mae: 0.1110\n",
      "Epoch 28/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0425 - custom_mae: 0.1213\n",
      "Epoch 00028: val_custom_mae improved from 0.11100 to 0.11013, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0425 - custom_mae: 0.1213 - val_loss: 0.0386 - val_custom_mae: 0.1101\n",
      "Epoch 29/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0422 - custom_mae: 0.1207\n",
      "Epoch 00029: val_custom_mae did not improve from 0.11013\n",
      "2500/2500 [==============================] - 285s 114ms/step - loss: 0.0422 - custom_mae: 0.1207 - val_loss: 0.0396 - val_custom_mae: 0.1104\n",
      "Epoch 30/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0419 - custom_mae: 0.1197\n",
      "Epoch 00030: val_custom_mae improved from 0.11013 to 0.10805, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0419 - custom_mae: 0.1197 - val_loss: 0.0377 - val_custom_mae: 0.1080\n",
      "Epoch 31/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0414 - custom_mae: 0.1188\n",
      "Epoch 00031: val_custom_mae improved from 0.10805 to 0.10677, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0414 - custom_mae: 0.1188 - val_loss: 0.0371 - val_custom_mae: 0.1068\n",
      "Epoch 32/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0405 - custom_mae: 0.1176\n",
      "Epoch 00032: val_custom_mae did not improve from 0.10677\n",
      "2500/2500 [==============================] - 286s 114ms/step - loss: 0.0405 - custom_mae: 0.1176 - val_loss: 0.0377 - val_custom_mae: 0.1075\n",
      "Epoch 33/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0402 - custom_mae: 0.1166\n",
      "Epoch 00033: val_custom_mae improved from 0.10677 to 0.10613, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 357s 143ms/step - loss: 0.0402 - custom_mae: 0.1165 - val_loss: 0.0369 - val_custom_mae: 0.1061\n",
      "Epoch 34/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0395 - custom_mae: 0.1155\n",
      "Epoch 00034: val_custom_mae improved from 0.10613 to 0.10610, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0395 - custom_mae: 0.1155 - val_loss: 0.0367 - val_custom_mae: 0.1061\n",
      "Epoch 35/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0391 - custom_mae: 0.1148\n",
      "Epoch 00035: val_custom_mae improved from 0.10610 to 0.10373, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0391 - custom_mae: 0.1148 - val_loss: 0.0359 - val_custom_mae: 0.1037\n",
      "Epoch 36/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0389 - custom_mae: 0.1143 ETA: 1s - loss: 0\n",
      "Epoch 00036: val_custom_mae did not improve from 0.10373\n",
      "2500/2500 [==============================] - 287s 115ms/step - loss: 0.0389 - custom_mae: 0.1143 - val_loss: 0.0360 - val_custom_mae: 0.1039\n",
      "Epoch 37/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0383 - custom_mae: 0.1132\n",
      "Epoch 00037: val_custom_mae did not improve from 0.10373\n",
      "2500/2500 [==============================] - 286s 114ms/step - loss: 0.0383 - custom_mae: 0.1132 - val_loss: 0.0365 - val_custom_mae: 0.1038\n",
      "Epoch 38/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0379 - custom_mae: 0.1123 ETA: \n",
      "Epoch 00038: val_custom_mae improved from 0.10373 to 0.10138, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 293s 117ms/step - loss: 0.0379 - custom_mae: 0.1123 - val_loss: 0.0349 - val_custom_mae: 0.1014\n",
      "Epoch 39/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0375 - custom_mae: 0.1117\n",
      "Epoch 00039: val_custom_mae improved from 0.10138 to 0.10095, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0375 - custom_mae: 0.1117 - val_loss: 0.0350 - val_custom_mae: 0.1009\n",
      "Epoch 40/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0370 - custom_mae: 0.1104\n",
      "Epoch 00040: val_custom_mae did not improve from 0.10095\n",
      "2500/2500 [==============================] - 287s 115ms/step - loss: 0.0370 - custom_mae: 0.1104 - val_loss: 0.0350 - val_custom_mae: 0.1013\n",
      "Epoch 41/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0369 - custom_mae: 0.1102 ETA: 3s - loss: 0.036\n",
      "Epoch 00041: val_custom_mae did not improve from 0.10095\n",
      "2500/2500 [==============================] - 286s 115ms/step - loss: 0.0369 - custom_mae: 0.1102 - val_loss: 0.0363 - val_custom_mae: 0.1019\n",
      "Epoch 42/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0363 - custom_mae: 0.1095\n",
      "Epoch 00042: val_custom_mae improved from 0.10095 to 0.10068, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 293s 117ms/step - loss: 0.0363 - custom_mae: 0.1096 - val_loss: 0.0349 - val_custom_mae: 0.1007\n",
      "Epoch 43/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0360 - custom_mae: 0.1086\n",
      "Epoch 00043: val_custom_mae did not improve from 0.10068\n",
      "2500/2500 [==============================] - 287s 115ms/step - loss: 0.0359 - custom_mae: 0.1086 - val_loss: 0.0348 - val_custom_mae: 0.1008\n",
      "Epoch 44/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0356 - custom_mae: 0.1081\n",
      "Epoch 00044: val_custom_mae improved from 0.10068 to 0.09967, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 293s 117ms/step - loss: 0.0356 - custom_mae: 0.1081 - val_loss: 0.0343 - val_custom_mae: 0.0997\n",
      "Epoch 45/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0353 - custom_mae: 0.1075 ETA\n",
      "Epoch 00045: val_custom_mae improved from 0.09967 to 0.09669, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 293s 117ms/step - loss: 0.0353 - custom_mae: 0.1076 - val_loss: 0.0331 - val_custom_mae: 0.0967\n",
      "Epoch 46/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0351 - custom_mae: 0.1069\n",
      "Epoch 00046: val_custom_mae did not improve from 0.09669\n",
      "2500/2500 [==============================] - 287s 115ms/step - loss: 0.0351 - custom_mae: 0.1069 - val_loss: 0.0341 - val_custom_mae: 0.0981\n",
      "Epoch 47/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0346 - custom_mae: 0.1060\n",
      "Epoch 00047: val_custom_mae improved from 0.09669 to 0.09550, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0346 - custom_mae: 0.1060 - val_loss: 0.0323 - val_custom_mae: 0.0955\n",
      "Epoch 48/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0346 - custom_mae: 0.1058\n",
      "Epoch 00048: val_custom_mae did not improve from 0.09550\n",
      "2500/2500 [==============================] - 287s 115ms/step - loss: 0.0346 - custom_mae: 0.1058 - val_loss: 0.0334 - val_custom_mae: 0.0977\n",
      "Epoch 49/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0343 - custom_mae: 0.1053\n",
      "Epoch 00049: val_custom_mae improved from 0.09550 to 0.09520, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0343 - custom_mae: 0.1052 - val_loss: 0.0324 - val_custom_mae: 0.0952\n",
      "Epoch 50/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0340 - custom_mae: 0.1048\n",
      "Epoch 00050: val_custom_mae improved from 0.09520 to 0.09505, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 293s 117ms/step - loss: 0.0340 - custom_mae: 0.1048 - val_loss: 0.0325 - val_custom_mae: 0.0951\n",
      "Epoch 51/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0333 - custom_mae: 0.1036\n",
      "Epoch 00051: val_custom_mae improved from 0.09505 to 0.09406, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0333 - custom_mae: 0.1036 - val_loss: 0.0319 - val_custom_mae: 0.0941\n",
      "Epoch 52/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0335 - custom_mae: 0.1035 ETA: 1s -\n",
      "Epoch 00052: val_custom_mae improved from 0.09406 to 0.09389, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 293s 117ms/step - loss: 0.0335 - custom_mae: 0.1035 - val_loss: 0.0322 - val_custom_mae: 0.0939\n",
      "Epoch 53/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0332 - custom_mae: 0.1030 ETA: 0s - loss: 0.0332 - cust\n",
      "Epoch 00053: val_custom_mae did not improve from 0.09389\n",
      "2500/2500 [==============================] - 287s 115ms/step - loss: 0.0332 - custom_mae: 0.1030 - val_loss: 0.0333 - val_custom_mae: 0.0962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0328 - custom_mae: 0.1025\n",
      "Epoch 00054: val_custom_mae improved from 0.09389 to 0.09353, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0328 - custom_mae: 0.1025 - val_loss: 0.0320 - val_custom_mae: 0.0935\n",
      "Epoch 55/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0325 - custom_mae: 0.1018\n",
      "Epoch 00055: val_custom_mae did not improve from 0.09353\n",
      "2500/2500 [==============================] - 287s 115ms/step - loss: 0.0324 - custom_mae: 0.1017 - val_loss: 0.0337 - val_custom_mae: 0.0962\n",
      "Epoch 56/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0320 - custom_mae: 0.1009\n",
      "Epoch 00056: val_custom_mae did not improve from 0.09353\n",
      "2500/2500 [==============================] - 289s 115ms/step - loss: 0.0320 - custom_mae: 0.1009 - val_loss: 0.0324 - val_custom_mae: 0.0937\n",
      "Epoch 57/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0320 - custom_mae: 0.1008\n",
      "Epoch 00057: val_custom_mae did not improve from 0.09353\n",
      "2500/2500 [==============================] - 288s 115ms/step - loss: 0.0320 - custom_mae: 0.1008 - val_loss: 0.0347 - val_custom_mae: 0.0975\n",
      "Epoch 58/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0317 - custom_mae: 0.1002\n",
      "Epoch 00058: val_custom_mae improved from 0.09353 to 0.09342, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 294s 117ms/step - loss: 0.0317 - custom_mae: 0.1002 - val_loss: 0.0320 - val_custom_mae: 0.0934\n",
      "Epoch 59/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0312 - custom_mae: 0.0996\n",
      "Epoch 00059: val_custom_mae improved from 0.09342 to 0.09290, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0312 - custom_mae: 0.0995 - val_loss: 0.0313 - val_custom_mae: 0.0929\n",
      "Epoch 60/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0312 - custom_mae: 0.0993\n",
      "Epoch 00060: val_custom_mae improved from 0.09290 to 0.09177, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0312 - custom_mae: 0.0993 - val_loss: 0.0305 - val_custom_mae: 0.0918\n",
      "Epoch 61/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0311 - custom_mae: 0.0991 ETA: 1s - los\n",
      "Epoch 00061: val_custom_mae did not improve from 0.09177\n",
      "2500/2500 [==============================] - 288s 115ms/step - loss: 0.0311 - custom_mae: 0.0991 - val_loss: 0.0318 - val_custom_mae: 0.0922\n",
      "Epoch 62/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0307 - custom_mae: 0.0985\n",
      "Epoch 00062: val_custom_mae improved from 0.09177 to 0.09110, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0308 - custom_mae: 0.0985 - val_loss: 0.0317 - val_custom_mae: 0.0911\n",
      "Epoch 63/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0306 - custom_mae: 0.0982\n",
      "Epoch 00063: val_custom_mae improved from 0.09110 to 0.09079, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0306 - custom_mae: 0.0982 - val_loss: 0.0302 - val_custom_mae: 0.0908\n",
      "Epoch 64/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0305 - custom_mae: 0.0977\n",
      "Epoch 00064: val_custom_mae improved from 0.09079 to 0.08963, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0305 - custom_mae: 0.0977 - val_loss: 0.0299 - val_custom_mae: 0.0896\n",
      "Epoch 65/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0302 - custom_mae: 0.0971\n",
      "Epoch 00065: val_custom_mae improved from 0.08963 to 0.08851, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0302 - custom_mae: 0.0971 - val_loss: 0.0295 - val_custom_mae: 0.0885\n",
      "Epoch 66/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0299 - custom_mae: 0.0968\n",
      "Epoch 00066: val_custom_mae did not improve from 0.08851\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0299 - custom_mae: 0.0968 - val_loss: 0.0305 - val_custom_mae: 0.0906\n",
      "Epoch 67/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0299 - custom_mae: 0.0964\n",
      "Epoch 00067: val_custom_mae did not improve from 0.08851\n",
      "2500/2500 [==============================] - 289s 115ms/step - loss: 0.0299 - custom_mae: 0.0964 - val_loss: 0.0298 - val_custom_mae: 0.0891\n",
      "Epoch 68/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0293 - custom_mae: 0.0960\n",
      "Epoch 00068: val_custom_mae did not improve from 0.08851\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0293 - custom_mae: 0.0959 - val_loss: 0.0316 - val_custom_mae: 0.0908\n",
      "Epoch 69/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0294 - custom_mae: 0.0957 ETA: 0s - loss: 0.0294 - custom_m\n",
      "Epoch 00069: val_custom_mae did not improve from 0.08851\n",
      "2500/2500 [==============================] - 288s 115ms/step - loss: 0.0294 - custom_mae: 0.0957 - val_loss: 0.0309 - val_custom_mae: 0.0893\n",
      "Epoch 70/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0290 - custom_mae: 0.0950\n",
      "Epoch 00070: val_custom_mae did not improve from 0.08851\n",
      "2500/2500 [==============================] - 289s 115ms/step - loss: 0.0290 - custom_mae: 0.0950 - val_loss: 0.0320 - val_custom_mae: 0.0904\n",
      "Epoch 71/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0286 - custom_mae: 0.0944\n",
      "Epoch 00071: val_custom_mae did not improve from 0.08851\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0286 - custom_mae: 0.0944 - val_loss: 0.0318 - val_custom_mae: 0.0908\n",
      "Epoch 72/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0290 - custom_mae: 0.0950\n",
      "Epoch 00072: val_custom_mae did not improve from 0.08851\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0290 - custom_mae: 0.0950 - val_loss: 0.0289 - val_custom_mae: 0.0894\n",
      "Epoch 73/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0289 - custom_mae: 0.0945\n",
      "Epoch 00073: val_custom_mae did not improve from 0.08851\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0289 - custom_mae: 0.0945 - val_loss: 0.0307 - val_custom_mae: 0.0911\n",
      "Epoch 74/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0284 - custom_mae: 0.0936\n",
      "Epoch 00074: val_custom_mae improved from 0.08851 to 0.08810, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0284 - custom_mae: 0.0936 - val_loss: 0.0296 - val_custom_mae: 0.0881\n",
      "Epoch 75/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0281 - custom_mae: 0.0934\n",
      "Epoch 00075: val_custom_mae improved from 0.08810 to 0.08696, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 296s 118ms/step - loss: 0.0281 - custom_mae: 0.0935 - val_loss: 0.0284 - val_custom_mae: 0.0870\n",
      "Epoch 76/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0277 - custom_mae: 0.0930\n",
      "Epoch 00076: val_custom_mae improved from 0.08696 to 0.08624, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 296s 118ms/step - loss: 0.0277 - custom_mae: 0.0930 - val_loss: 0.0287 - val_custom_mae: 0.0862\n",
      "Epoch 77/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0279 - custom_mae: 0.0928\n",
      "Epoch 00077: val_custom_mae improved from 0.08624 to 0.08473, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 299s 120ms/step - loss: 0.0279 - custom_mae: 0.0928 - val_loss: 0.0276 - val_custom_mae: 0.0847\n",
      "Epoch 78/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0274 - custom_mae: 0.0922\n",
      "Epoch 00078: val_custom_mae did not improve from 0.08473\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0274 - custom_mae: 0.0922 - val_loss: 0.0302 - val_custom_mae: 0.0892\n",
      "Epoch 79/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0274 - custom_mae: 0.0920\n",
      "Epoch 00079: val_custom_mae did not improve from 0.08473\n",
      "2500/2500 [==============================] - 289s 116ms/step - loss: 0.0274 - custom_mae: 0.0920 - val_loss: 0.0288 - val_custom_mae: 0.0871\n",
      "Epoch 80/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0274 - custom_mae: 0.0918 ETA: 1s - l\n",
      "Epoch 00080: val_custom_mae did not improve from 0.08473\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0274 - custom_mae: 0.0918 - val_loss: 0.0285 - val_custom_mae: 0.0863\n",
      "Epoch 81/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0273 - custom_mae: 0.0916\n",
      "Epoch 00081: val_custom_mae did not improve from 0.08473\n",
      "2500/2500 [==============================] - 290s 116ms/step - loss: 0.0273 - custom_mae: 0.0916 - val_loss: 0.0280 - val_custom_mae: 0.0857\n",
      "Epoch 82/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0270 - custom_mae: 0.0913 ETA: 1s - loss: 0.0270 \n",
      "Epoch 00082: val_custom_mae did not improve from 0.08473\n",
      "2500/2500 [==============================] - 293s 117ms/step - loss: 0.0270 - custom_mae: 0.0913 - val_loss: 0.0291 - val_custom_mae: 0.0853\n",
      "Epoch 83/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0270 - custom_mae: 0.0909\n",
      "Epoch 00083: val_custom_mae did not improve from 0.08473\n",
      "2500/2500 [==============================] - 292s 117ms/step - loss: 0.0270 - custom_mae: 0.0909 - val_loss: 0.0276 - val_custom_mae: 0.0852\n",
      "Epoch 84/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0269 - custom_mae: 0.0907 ETA: 1s - loss: 0.026\n",
      "Epoch 00084: val_custom_mae did not improve from 0.08473\n",
      "2500/2500 [==============================] - 294s 117ms/step - loss: 0.0269 - custom_mae: 0.0907 - val_loss: 0.0288 - val_custom_mae: 0.0860\n",
      "Epoch 85/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0266 - custom_mae: 0.0904\n",
      "Epoch 00085: val_custom_mae did not improve from 0.08473\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0266 - custom_mae: 0.0904 - val_loss: 0.0301 - val_custom_mae: 0.0873\n",
      "Epoch 86/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0269 - custom_mae: 0.0904\n",
      "Epoch 00086: val_custom_mae did not improve from 0.08473\n",
      "2500/2500 [==============================] - 301s 121ms/step - loss: 0.0269 - custom_mae: 0.0904 - val_loss: 0.0290 - val_custom_mae: 0.0870\n",
      "Epoch 87/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0263 - custom_mae: 0.0897\n",
      "Epoch 00087: val_custom_mae improved from 0.08473 to 0.08404, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 335s 134ms/step - loss: 0.0263 - custom_mae: 0.0897 - val_loss: 0.0280 - val_custom_mae: 0.0840\n",
      "Epoch 88/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0264 - custom_mae: 0.0896\n",
      "Epoch 00088: val_custom_mae improved from 0.08404 to 0.08355, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 312s 125ms/step - loss: 0.0264 - custom_mae: 0.0896 - val_loss: 0.0273 - val_custom_mae: 0.0835\n",
      "Epoch 89/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0261 - custom_mae: 0.0892\n",
      "Epoch 00089: val_custom_mae improved from 0.08355 to 0.08244, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 334s 134ms/step - loss: 0.0261 - custom_mae: 0.0892 - val_loss: 0.0268 - val_custom_mae: 0.0824\n",
      "Epoch 90/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0256 - custom_mae: 0.0887\n",
      "Epoch 00090: val_custom_mae improved from 0.08244 to 0.08211, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 328s 131ms/step - loss: 0.0257 - custom_mae: 0.0887 - val_loss: 0.0270 - val_custom_mae: 0.0821\n",
      "Epoch 91/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0258 - custom_mae: 0.0888\n",
      "Epoch 00091: val_custom_mae did not improve from 0.08211\n",
      "2500/2500 [==============================] - 338s 135ms/step - loss: 0.0258 - custom_mae: 0.0888 - val_loss: 0.0270 - val_custom_mae: 0.0827\n",
      "Epoch 92/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0256 - custom_mae: 0.0884\n",
      "Epoch 00092: val_custom_mae improved from 0.08211 to 0.08198, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 328s 131ms/step - loss: 0.0256 - custom_mae: 0.0884 - val_loss: 0.0262 - val_custom_mae: 0.0820\n",
      "Epoch 93/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0256 - custom_mae: 0.0880\n",
      "Epoch 00093: val_custom_mae did not improve from 0.08198\n",
      "2500/2500 [==============================] - 351s 140ms/step - loss: 0.0256 - custom_mae: 0.0880 - val_loss: 0.0268 - val_custom_mae: 0.0825\n",
      "Epoch 94/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0254 - custom_mae: 0.0879\n",
      "Epoch 00094: val_custom_mae did not improve from 0.08198\n",
      "2500/2500 [==============================] - 344s 137ms/step - loss: 0.0254 - custom_mae: 0.0879 - val_loss: 0.0269 - val_custom_mae: 0.0823\n",
      "Epoch 95/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0253 - custom_mae: 0.0876\n",
      "Epoch 00095: val_custom_mae did not improve from 0.08198\n",
      "2500/2500 [==============================] - 364s 146ms/step - loss: 0.0253 - custom_mae: 0.0876 - val_loss: 0.0272 - val_custom_mae: 0.0827\n",
      "Epoch 96/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0255 - custom_mae: 0.0878\n",
      "Epoch 00096: val_custom_mae did not improve from 0.08198\n",
      "2500/2500 [==============================] - 369s 148ms/step - loss: 0.0255 - custom_mae: 0.0878 - val_loss: 0.0278 - val_custom_mae: 0.0844\n",
      "Epoch 97/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0252 - custom_mae: 0.0871\n",
      "Epoch 00097: val_custom_mae improved from 0.08198 to 0.08129, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 376s 150ms/step - loss: 0.0252 - custom_mae: 0.0871 - val_loss: 0.0262 - val_custom_mae: 0.0813\n",
      "Epoch 98/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0251 - custom_mae: 0.0871\n",
      "Epoch 00098: val_custom_mae did not improve from 0.08129\n",
      "2500/2500 [==============================] - 378s 151ms/step - loss: 0.0251 - custom_mae: 0.0871 - val_loss: 0.0273 - val_custom_mae: 0.0849\n",
      "Epoch 99/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0248 - custom_mae: 0.0866 ETA: 1s - l\n",
      "Epoch 00099: val_custom_mae did not improve from 0.08129\n",
      "2500/2500 [==============================] - 373s 149ms/step - loss: 0.0248 - custom_mae: 0.0866 - val_loss: 0.0269 - val_custom_mae: 0.0822\n",
      "Epoch 100/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0248 - custom_mae: 0.0865\n",
      "Epoch 00100: val_custom_mae improved from 0.08129 to 0.08087, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 389s 156ms/step - loss: 0.0248 - custom_mae: 0.0865 - val_loss: 0.0257 - val_custom_mae: 0.0809\n",
      "Epoch 101/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0246 - custom_mae: 0.0862\n",
      "Epoch 00101: val_custom_mae did not improve from 0.08087\n",
      "2500/2500 [==============================] - 377s 151ms/step - loss: 0.0246 - custom_mae: 0.0862 - val_loss: 0.0267 - val_custom_mae: 0.0822\n",
      "Epoch 102/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0244 - custom_mae: 0.0860\n",
      "Epoch 00102: val_custom_mae improved from 0.08087 to 0.07959, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 381s 152ms/step - loss: 0.0244 - custom_mae: 0.0860 - val_loss: 0.0255 - val_custom_mae: 0.0796\n",
      "Epoch 103/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0246 - custom_mae: 0.0858\n",
      "Epoch 00103: val_custom_mae did not improve from 0.07959\n",
      "2500/2500 [==============================] - 377s 151ms/step - loss: 0.0246 - custom_mae: 0.0858 - val_loss: 0.0256 - val_custom_mae: 0.0800\n",
      "Epoch 104/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0244 - custom_mae: 0.0856\n",
      "Epoch 00104: val_custom_mae did not improve from 0.07959\n",
      "2500/2500 [==============================] - 371s 148ms/step - loss: 0.0244 - custom_mae: 0.0856 - val_loss: 0.0257 - val_custom_mae: 0.0816\n",
      "Epoch 105/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0240 - custom_mae: 0.0851\n",
      "Epoch 00105: val_custom_mae did not improve from 0.07959\n",
      "2500/2500 [==============================] - 373s 149ms/step - loss: 0.0240 - custom_mae: 0.0851 - val_loss: 0.0269 - val_custom_mae: 0.0817\n",
      "Epoch 106/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0239 - custom_mae: 0.0847\n",
      "Epoch 00106: val_custom_mae did not improve from 0.07959\n",
      "2500/2500 [==============================] - 374s 150ms/step - loss: 0.0239 - custom_mae: 0.0847 - val_loss: 0.0269 - val_custom_mae: 0.0814\n",
      "Epoch 107/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0241 - custom_mae: 0.0849\n",
      "Epoch 00107: val_custom_mae did not improve from 0.07959\n",
      "2500/2500 [==============================] - 374s 150ms/step - loss: 0.0241 - custom_mae: 0.0849 - val_loss: 0.0257 - val_custom_mae: 0.0802\n",
      "Epoch 108/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0238 - custom_mae: 0.0847\n",
      "Epoch 00108: val_custom_mae did not improve from 0.07959\n",
      "2500/2500 [==============================] - 375s 150ms/step - loss: 0.0238 - custom_mae: 0.0847 - val_loss: 0.0264 - val_custom_mae: 0.0809\n",
      "Epoch 109/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0238 - custom_mae: 0.0846\n",
      "Epoch 00109: val_custom_mae improved from 0.07959 to 0.07930, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 364s 145ms/step - loss: 0.0238 - custom_mae: 0.0846 - val_loss: 0.0259 - val_custom_mae: 0.0793\n",
      "Epoch 110/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0236 - custom_mae: 0.0843 ETA: 0s - loss: 0.0236 - custom_mae: 0.08\n",
      "Epoch 00110: val_custom_mae improved from 0.07930 to 0.07824, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 375s 150ms/step - loss: 0.0236 - custom_mae: 0.0843 - val_loss: 0.0250 - val_custom_mae: 0.0782\n",
      "Epoch 111/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0234 - custom_mae: 0.0836 ETA: 3s - los - ETA\n",
      "Epoch 00111: val_custom_mae did not improve from 0.07824\n",
      "2500/2500 [==============================] - 304s 122ms/step - loss: 0.0234 - custom_mae: 0.0836 - val_loss: 0.0261 - val_custom_mae: 0.0802\n",
      "Epoch 112/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0233 - custom_mae: 0.0837\n",
      "Epoch 00112: val_custom_mae did not improve from 0.07824\n",
      "2500/2500 [==============================] - 294s 117ms/step - loss: 0.0233 - custom_mae: 0.0837 - val_loss: 0.0255 - val_custom_mae: 0.0783\n",
      "Epoch 113/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0233 - custom_mae: 0.0834\n",
      "Epoch 00113: val_custom_mae improved from 0.07824 to 0.07800, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 298s 119ms/step - loss: 0.0233 - custom_mae: 0.0834 - val_loss: 0.0251 - val_custom_mae: 0.0780\n",
      "Epoch 114/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0229 - custom_mae: 0.0830\n",
      "Epoch 00114: val_custom_mae did not improve from 0.07800\n",
      "2500/2500 [==============================] - 301s 120ms/step - loss: 0.0229 - custom_mae: 0.0830 - val_loss: 0.0249 - val_custom_mae: 0.0788\n",
      "Epoch 115/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0233 - custom_mae: 0.0833\n",
      "Epoch 00115: val_custom_mae did not improve from 0.07800\n",
      "2500/2500 [==============================] - 293s 117ms/step - loss: 0.0233 - custom_mae: 0.0833 - val_loss: 0.0266 - val_custom_mae: 0.0798\n",
      "Epoch 116/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0229 - custom_mae: 0.0828\n",
      "Epoch 00116: val_custom_mae improved from 0.07800 to 0.07713, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 304s 121ms/step - loss: 0.0229 - custom_mae: 0.0828 - val_loss: 0.0246 - val_custom_mae: 0.0771\n",
      "Epoch 117/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0229 - custom_mae: 0.0827\n",
      "Epoch 00117: val_custom_mae improved from 0.07713 to 0.07697, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 309s 124ms/step - loss: 0.0229 - custom_mae: 0.0827 - val_loss: 0.0245 - val_custom_mae: 0.0770\n",
      "Epoch 118/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0226 - custom_mae: 0.0824\n",
      "Epoch 00118: val_custom_mae did not improve from 0.07697\n",
      "2500/2500 [==============================] - 328s 131ms/step - loss: 0.0226 - custom_mae: 0.0824 - val_loss: 0.0252 - val_custom_mae: 0.0779\n",
      "Epoch 119/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0228 - custom_mae: 0.0824\n",
      "Epoch 00119: val_custom_mae did not improve from 0.07697\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0228 - custom_mae: 0.0824 - val_loss: 0.0257 - val_custom_mae: 0.0782\n",
      "Epoch 120/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0226 - custom_mae: 0.0822\n",
      "Epoch 00120: val_custom_mae improved from 0.07697 to 0.07674, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 299s 120ms/step - loss: 0.0226 - custom_mae: 0.0822 - val_loss: 0.0246 - val_custom_mae: 0.0767\n",
      "Epoch 121/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0226 - custom_mae: 0.0820\n",
      "Epoch 00121: val_custom_mae did not improve from 0.07674\n",
      "2500/2500 [==============================] - 293s 117ms/step - loss: 0.0226 - custom_mae: 0.0820 - val_loss: 0.0256 - val_custom_mae: 0.0786\n",
      "Epoch 122/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0226 - custom_mae: 0.0818\n",
      "Epoch 00122: val_custom_mae improved from 0.07674 to 0.07548, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0226 - custom_mae: 0.0818 - val_loss: 0.0241 - val_custom_mae: 0.0755\n",
      "Epoch 123/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0224 - custom_mae: 0.0817\n",
      "Epoch 00123: val_custom_mae did not improve from 0.07548\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0224 - custom_mae: 0.0817 - val_loss: 0.0241 - val_custom_mae: 0.0773\n",
      "Epoch 124/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0222 - custom_mae: 0.0813\n",
      "Epoch 00124: val_custom_mae did not improve from 0.07548\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0222 - custom_mae: 0.0813 - val_loss: 0.0242 - val_custom_mae: 0.0762\n",
      "Epoch 125/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0223 - custom_mae: 0.0813\n",
      "Epoch 00125: val_custom_mae improved from 0.07548 to 0.07533, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 299s 120ms/step - loss: 0.0223 - custom_mae: 0.0813 - val_loss: 0.0235 - val_custom_mae: 0.0753\n",
      "Epoch 126/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0221 - custom_mae: 0.0810\n",
      "Epoch 00126: val_custom_mae did not improve from 0.07533\n",
      "2500/2500 [==============================] - 294s 117ms/step - loss: 0.0221 - custom_mae: 0.0810 - val_loss: 0.0239 - val_custom_mae: 0.0760\n",
      "Epoch 127/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0220 - custom_mae: 0.0807\n",
      "Epoch 00127: val_custom_mae did not improve from 0.07533\n",
      "2500/2500 [==============================] - 294s 117ms/step - loss: 0.0220 - custom_mae: 0.0807 - val_loss: 0.0243 - val_custom_mae: 0.0761\n",
      "Epoch 128/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0220 - custom_mae: 0.0807\n",
      "Epoch 00128: val_custom_mae did not improve from 0.07533\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0220 - custom_mae: 0.0807 - val_loss: 0.0244 - val_custom_mae: 0.0767\n",
      "Epoch 129/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0217 - custom_mae: 0.0803\n",
      "Epoch 00129: val_custom_mae improved from 0.07533 to 0.07495, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0217 - custom_mae: 0.0803 - val_loss: 0.0232 - val_custom_mae: 0.0749\n",
      "Epoch 130/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0219 - custom_mae: 0.0804\n",
      "Epoch 00130: val_custom_mae did not improve from 0.07495\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0218 - custom_mae: 0.0804 - val_loss: 0.0235 - val_custom_mae: 0.0750\n",
      "Epoch 131/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0216 - custom_mae: 0.0801\n",
      "Epoch 00131: val_custom_mae improved from 0.07495 to 0.07491, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0216 - custom_mae: 0.0801 - val_loss: 0.0239 - val_custom_mae: 0.0749\n",
      "Epoch 132/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0217 - custom_mae: 0.0800\n",
      "Epoch 00132: val_custom_mae did not improve from 0.07491\n",
      "2500/2500 [==============================] - 296s 118ms/step - loss: 0.0217 - custom_mae: 0.0800 - val_loss: 0.0253 - val_custom_mae: 0.0762\n",
      "Epoch 133/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0215 - custom_mae: 0.0797\n",
      "Epoch 00133: val_custom_mae improved from 0.07491 to 0.07424, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0215 - custom_mae: 0.0798 - val_loss: 0.0231 - val_custom_mae: 0.0742\n",
      "Epoch 134/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0215 - custom_mae: 0.0797\n",
      "Epoch 00134: val_custom_mae did not improve from 0.07424\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0215 - custom_mae: 0.0797 - val_loss: 0.0235 - val_custom_mae: 0.0771\n",
      "Epoch 135/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0214 - custom_mae: 0.0793\n",
      "Epoch 00135: val_custom_mae did not improve from 0.07424\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0214 - custom_mae: 0.0793 - val_loss: 0.0247 - val_custom_mae: 0.0780\n",
      "Epoch 136/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0214 - custom_mae: 0.0792\n",
      "Epoch 00136: val_custom_mae did not improve from 0.07424\n",
      "2500/2500 [==============================] - 296s 118ms/step - loss: 0.0214 - custom_mae: 0.0792 - val_loss: 0.0231 - val_custom_mae: 0.0745\n",
      "Epoch 137/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0210 - custom_mae: 0.0790\n",
      "Epoch 00137: val_custom_mae did not improve from 0.07424\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0210 - custom_mae: 0.0790 - val_loss: 0.0231 - val_custom_mae: 0.0745\n",
      "Epoch 138/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0209 - custom_mae: 0.0787\n",
      "Epoch 00138: val_custom_mae did not improve from 0.07424\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0209 - custom_mae: 0.0787 - val_loss: 0.0242 - val_custom_mae: 0.0765\n",
      "Epoch 139/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0212 - custom_mae: 0.0790\n",
      "Epoch 00139: val_custom_mae did not improve from 0.07424\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0212 - custom_mae: 0.0790 - val_loss: 0.0244 - val_custom_mae: 0.0762\n",
      "Epoch 140/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0209 - custom_mae: 0.0786\n",
      "Epoch 00140: val_custom_mae improved from 0.07424 to 0.07341, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0209 - custom_mae: 0.0786 - val_loss: 0.0232 - val_custom_mae: 0.0734\n",
      "Epoch 141/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0208 - custom_mae: 0.0784\n",
      "Epoch 00141: val_custom_mae did not improve from 0.07341\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0208 - custom_mae: 0.0784 - val_loss: 0.0233 - val_custom_mae: 0.0743\n",
      "Epoch 142/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0208 - custom_mae: 0.0782\n",
      "Epoch 00142: val_custom_mae improved from 0.07341 to 0.07285, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 301s 120ms/step - loss: 0.0208 - custom_mae: 0.0782 - val_loss: 0.0226 - val_custom_mae: 0.0728\n",
      "Epoch 143/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0207 - custom_mae: 0.0780\n",
      "Epoch 00143: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 294s 117ms/step - loss: 0.0207 - custom_mae: 0.0780 - val_loss: 0.0230 - val_custom_mae: 0.0743\n",
      "Epoch 144/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0206 - custom_mae: 0.0779\n",
      "Epoch 00144: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0206 - custom_mae: 0.0779 - val_loss: 0.0232 - val_custom_mae: 0.0739\n",
      "Epoch 145/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0209 - custom_mae: 0.0780\n",
      "Epoch 00145: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0209 - custom_mae: 0.0780 - val_loss: 0.0242 - val_custom_mae: 0.0758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0207 - custom_mae: 0.0778\n",
      "Epoch 00146: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0208 - custom_mae: 0.0778 - val_loss: 0.0238 - val_custom_mae: 0.0737\n",
      "Epoch 147/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0206 - custom_mae: 0.0774\n",
      "Epoch 00147: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0206 - custom_mae: 0.0774 - val_loss: 0.0236 - val_custom_mae: 0.0752\n",
      "Epoch 148/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0207 - custom_mae: 0.0778\n",
      "Epoch 00148: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 298s 119ms/step - loss: 0.0207 - custom_mae: 0.0778 - val_loss: 0.0229 - val_custom_mae: 0.0751\n",
      "Epoch 149/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0204 - custom_mae: 0.0773\n",
      "Epoch 00149: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 297s 119ms/step - loss: 0.0204 - custom_mae: 0.0773 - val_loss: 0.0231 - val_custom_mae: 0.0732\n",
      "Epoch 150/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0202 - custom_mae: 0.0772\n",
      "Epoch 00150: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 296s 119ms/step - loss: 0.0202 - custom_mae: 0.0772 - val_loss: 0.0227 - val_custom_mae: 0.0739\n",
      "Epoch 151/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0203 - custom_mae: 0.0771 ETA: 1s - loss: 0.0\n",
      "Epoch 00151: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 294s 117ms/step - loss: 0.0203 - custom_mae: 0.0771 - val_loss: 0.0238 - val_custom_mae: 0.0744\n",
      "Epoch 152/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0201 - custom_mae: 0.0771\n",
      "Epoch 00152: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 297s 119ms/step - loss: 0.0201 - custom_mae: 0.0771 - val_loss: 0.0228 - val_custom_mae: 0.0730\n",
      "Epoch 153/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0199 - custom_mae: 0.0764\n",
      "Epoch 00153: val_custom_mae did not improve from 0.07285\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0199 - custom_mae: 0.0763 - val_loss: 0.0235 - val_custom_mae: 0.0733\n",
      "Epoch 154/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0201 - custom_mae: 0.0766\n",
      "Epoch 00154: val_custom_mae improved from 0.07285 to 0.07187, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 304s 122ms/step - loss: 0.0201 - custom_mae: 0.0766 - val_loss: 0.0228 - val_custom_mae: 0.0719\n",
      "Epoch 155/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0198 - custom_mae: 0.0763\n",
      "Epoch 00155: val_custom_mae did not improve from 0.07187\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "2500/2500 [==============================] - 294s 118ms/step - loss: 0.0198 - custom_mae: 0.0763 - val_loss: 0.0231 - val_custom_mae: 0.0722\n",
      "Epoch 156/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0190 - custom_mae: 0.0749 - ETA: 12s - l\n",
      "Epoch 00156: val_custom_mae improved from 0.07187 to 0.07122, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 304s 122ms/step - loss: 0.0190 - custom_mae: 0.0749 - val_loss: 0.0220 - val_custom_mae: 0.0712\n",
      "Epoch 157/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0189 - custom_mae: 0.0748\n",
      "Epoch 00157: val_custom_mae improved from 0.07122 to 0.07084, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0189 - custom_mae: 0.0748 - val_loss: 0.0219 - val_custom_mae: 0.0708\n",
      "Epoch 158/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0189 - custom_mae: 0.0746\n",
      "Epoch 00158: val_custom_mae did not improve from 0.07084\n",
      "2500/2500 [==============================] - 299s 120ms/step - loss: 0.0189 - custom_mae: 0.0746 - val_loss: 0.0218 - val_custom_mae: 0.0709\n",
      "Epoch 159/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0186 - custom_mae: 0.0743\n",
      "Epoch 00159: val_custom_mae did not improve from 0.07084\n",
      "2500/2500 [==============================] - 296s 118ms/step - loss: 0.0185 - custom_mae: 0.0743 - val_loss: 0.0218 - val_custom_mae: 0.0709\n",
      "Epoch 160/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0187 - custom_mae: 0.0744\n",
      "Epoch 00160: val_custom_mae did not improve from 0.07084\n",
      "2500/2500 [==============================] - 299s 120ms/step - loss: 0.0187 - custom_mae: 0.0744 - val_loss: 0.0218 - val_custom_mae: 0.0709\n",
      "Epoch 161/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0187 - custom_mae: 0.0744\n",
      "Epoch 00161: val_custom_mae did not improve from 0.07084\n",
      "2500/2500 [==============================] - 295s 118ms/step - loss: 0.0187 - custom_mae: 0.0744 - val_loss: 0.0218 - val_custom_mae: 0.0713\n",
      "Epoch 162/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0188 - custom_mae: 0.0744\n",
      "Epoch 00162: val_custom_mae improved from 0.07084 to 0.07060, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 305s 122ms/step - loss: 0.0188 - custom_mae: 0.0744 - val_loss: 0.0217 - val_custom_mae: 0.0706\n",
      "Epoch 163/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0185 - custom_mae: 0.0741\n",
      "Epoch 00163: val_custom_mae did not improve from 0.07060\n",
      "2500/2500 [==============================] - 298s 119ms/step - loss: 0.0185 - custom_mae: 0.0741 - val_loss: 0.0218 - val_custom_mae: 0.0713\n",
      "Epoch 164/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0187 - custom_mae: 0.0743\n",
      "Epoch 00164: val_custom_mae did not improve from 0.07060\n",
      "2500/2500 [==============================] - 299s 120ms/step - loss: 0.0187 - custom_mae: 0.0743 - val_loss: 0.0216 - val_custom_mae: 0.0708\n",
      "Epoch 165/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0189 - custom_mae: 0.0745 ETA: 1s - los\n",
      "Epoch 00165: val_custom_mae did not improve from 0.07060\n",
      "2500/2500 [==============================] - 299s 120ms/step - loss: 0.0189 - custom_mae: 0.0745 - val_loss: 0.0216 - val_custom_mae: 0.0709\n",
      "Epoch 166/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0186 - custom_mae: 0.0744\n",
      "Epoch 00166: val_custom_mae did not improve from 0.07060\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0186 - custom_mae: 0.0744 - val_loss: 0.0216 - val_custom_mae: 0.0708\n",
      "Epoch 167/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0186 - custom_mae: 0.0741\n",
      "Epoch 00167: val_custom_mae did not improve from 0.07060\n",
      "2500/2500 [==============================] - 298s 119ms/step - loss: 0.0186 - custom_mae: 0.0740 - val_loss: 0.0216 - val_custom_mae: 0.0707\n",
      "Epoch 168/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0186 - custom_mae: 0.0741 ETA: 0s - loss: 0.0185 - cust\n",
      "Epoch 00168: val_custom_mae did not improve from 0.07060\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0186 - custom_mae: 0.0741 - val_loss: 0.0217 - val_custom_mae: 0.0708\n",
      "Epoch 169/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0186 - custom_mae: 0.0741\n",
      "Epoch 00169: val_custom_mae did not improve from 0.07060\n",
      "2500/2500 [==============================] - 296s 118ms/step - loss: 0.0186 - custom_mae: 0.0741 - val_loss: 0.0217 - val_custom_mae: 0.0707\n",
      "Epoch 170/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0185 - custom_mae: 0.0741\n",
      "Epoch 00170: val_custom_mae improved from 0.07060 to 0.07056, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 305s 122ms/step - loss: 0.0185 - custom_mae: 0.0741 - val_loss: 0.0216 - val_custom_mae: 0.0706\n",
      "Epoch 171/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0187 - custom_mae: 0.0743\n",
      "Epoch 00171: val_custom_mae did not improve from 0.07056\n",
      "2500/2500 [==============================] - 297s 119ms/step - loss: 0.0187 - custom_mae: 0.0743 - val_loss: 0.0217 - val_custom_mae: 0.0706\n",
      "Epoch 172/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0185 - custom_mae: 0.0741\n",
      "Epoch 00172: val_custom_mae did not improve from 0.07056\n",
      "2500/2500 [==============================] - 300s 120ms/step - loss: 0.0185 - custom_mae: 0.0741 - val_loss: 0.0217 - val_custom_mae: 0.0707\n",
      "Epoch 173/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0188 - custom_mae: 0.0742\n",
      "Epoch 00173: val_custom_mae did not improve from 0.07056\n",
      "2500/2500 [==============================] - 296s 118ms/step - loss: 0.0188 - custom_mae: 0.0742 - val_loss: 0.0217 - val_custom_mae: 0.0707\n",
      "Epoch 174/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.0185 - custom_mae: 0.0740\n",
      "Epoch 00174: val_custom_mae improved from 0.07056 to 0.07007, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\2020-05-28_Normalized_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_210_Model_and_Weights_80000.hdf5\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 392. MiB for an array with shape (25088, 4096) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-341b004e17f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mdisable_progress_bar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mprint_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mclear_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         )\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;31m# start runtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscan_run\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mscan_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\talos\\scan\\scan_run.py\u001b[0m in \u001b[0;36mscan_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# otherwise proceed with next permutation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscan_round\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\talos\\scan\\scan_round.py\u001b[0m in \u001b[0;36mscan_round\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mingest_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\talos\\model\\ingest_model.py\u001b[0m in \u001b[0;36mingest_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                       self.round_params)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-f275f46d5126>\u001b[0m in \u001b[0;36mgrid_model_fine\u001b[1;34m(x, y, x_val, y_val, params)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_reducer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopper\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \"\"\"\n\u001b[0;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1008\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[1;32m--> 112\u001b[1;33m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m     \u001b[0mweight_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   3268\u001b[0m   \"\"\"\n\u001b[0;32m   3269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3270\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3271\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3272\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3268\u001b[0m   \"\"\"\n\u001b[0;32m   3269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3270\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3271\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3272\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    584\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 392. MiB for an array with shape (25088, 4096) and data type float32"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    #for top_results_index in range(3):\n",
    "    for top_results_index in [0, 1]:\n",
    "        #top_results_index = 1\n",
    "        _MODEL_TO_LOAD_INDEX = df.iloc[top_results_index].name\n",
    "        _MODEL_TO_LOAD = 'Best_Weights_FC_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        _TMP_DIR = '..\\\\TMP_TALOS_{}'.format(_DEVICE)\n",
    "        _CSV_RESULTS = _LOG_DIR + 'Talos_Results_Fine_Idx{}.csv'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        startTime = datetime.now()\n",
    "        \n",
    "        parameters = get_params(top_results_index)\n",
    "\n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = grid_model_fine,\n",
    "            params = parameters,\n",
    "            experiment_name = _TMP_DIR,\n",
    "            #shuffle=False,\n",
    "            reduction_metric = parameters['reduction_metric'][0],\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = True\n",
    "        )\n",
    "\n",
    "        print(\"Time taken:\", datetime.now() - startTime)\n",
    "        \n",
    "        print('Writing Device File')\n",
    "        device_file.write('Trained Model: {}'.format(_MODEL_TO_LOAD))\n",
    "\n",
    "        df_experiment_results = pd.read_csv(_TMP_DIR + '\\\\' + os.listdir(_TMP_DIR)[0])\n",
    "        df_experiment_results['Base'] = None\n",
    "        for i in range(df_experiment_results.shape[0]):\n",
    "            df_experiment_results['Base'][i] = _MODEL_TO_LOAD_INDEX\n",
    "\n",
    "        if os.path.isfile(_CSV_RESULTS):\n",
    "            df_experiment_results.to_csv(_CSV_RESULTS, mode = 'a', index = False, header = False)\n",
    "        else:\n",
    "            df_experiment_results.to_csv(_CSV_RESULTS, index = False)\n",
    "\n",
    "        shutil.rmtree(_TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Results to NAS if SSD Directory was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_directory(src, dst, symlinks = False, ignore = None):\n",
    "    maxLen = 0\n",
    "    message = ''        \n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        \n",
    "        message = 'Creating Path: {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        os.makedirs(dst)\n",
    "        \n",
    "    for item in os.listdir(src):\n",
    "        \n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        \n",
    "        if os.path.isdir(s):\n",
    "            \n",
    "            message = 'Copying Directory: {}'.format(s)\n",
    "            maxLen = max(maxLen, len(message))\n",
    "            print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "            \n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if not os.path.exists(d): #or os.stat(s).st_mtime - os.stat(d).st_mtime > 1:\n",
    "                \n",
    "                message = 'Copying File: {}'.format(s)\n",
    "                maxLen = max(maxLen, len(message))\n",
    "                print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "                \n",
    "                shutil.copy2(s, d)\n",
    "        \n",
    "        time.sleep(.5)\n",
    "     \n",
    "    message = 'Coyping... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "\n",
    "def delete_directory(src, terminator = '\\n'):\n",
    "    message = ''\n",
    "    maxLen = 0\n",
    "    \n",
    "    try:\n",
    "        message = 'Deleting {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        shutil.rmtree(src)\n",
    "        \n",
    "    except OSError as e:\n",
    "        message = 'Error: {} : {}'.format(src, e.strerror)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "        return\n",
    "    \n",
    "    message = 'Deleting... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = terminator)\n",
    "\n",
    "    \n",
    "def copy_fine_training(src, dst):\n",
    "    copy_directory(src, dst)\n",
    "    delete_directory(src, terminator = '\\r')\n",
    "    delete_directory(src + '..\\\\', terminator = '\\r')\n",
    "    if not os.listdir(src + '..\\\\..\\\\'):\n",
    "        delete_directory(src + '..\\\\..\\\\', terminator = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(storage == OutputDirectory.SSD):\n",
    "    _COPY_DIR = '..\\\\output\\\\{}'.format(_NET_DIR)\n",
    "    copy_fine_training(_LOG_DIR, _COPY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name = \"CMSE.Mixed\"></a><a href = #Top>Up</a></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
