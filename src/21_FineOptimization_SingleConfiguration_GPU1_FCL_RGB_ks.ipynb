{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Training on GPU 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links <a name = \"Top\"></a>\n",
    "\n",
    "<ol>\n",
    "<li><a href = #setup>Begin Training</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Conda Environment: tf_ks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "print('Current Conda Environment: {}'.format(os.environ['CONDA_DEFAULT_ENV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  2 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17934970905719419569\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9105744200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6607230501284618302\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9104897474\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12686101363689963674\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum für Training-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TrainingSet(Enum):\n",
    "    SYNTHETIC = 1\n",
    "    REAL = 2\n",
    "    MIXED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "\n",
    "* <i>SSD</i>, falls genug Speicher auf SSD im SymLink <i>fast_output</i> verfügbar ist\n",
    "* <i>HDD</i>, falls möglicherweise zu wenig SSD-Speicher verfügbar ist $\\rightarrow$ <i>output</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class OutputDirectory(IntEnum):\n",
    "    HDD = 0\n",
    "    SSD = 1\n",
    "    \n",
    "output_path = ['output', 'fast_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Label_Type into suitable label names.\n",
    "$\\Rightarrow$ Angular / Normalized $\\rightarrow$ ['Elevation', 'Azimuth']\n",
    "\n",
    "$\\Rightarrow$ Stereographic $\\rightarrow$ ['S_x', 'S_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Label_Names(label_type):\n",
    "    if label_type == 'Angular' or label_type == 'Normalized':\n",
    "        return ['Elevation', 'Azimuth']\n",
    "    elif label_type == 'Stereographic':\n",
    "        return ['S_x', 'S_y']\n",
    "    else:\n",
    "        assert(True, 'LabelType Invalid')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reduction_Metric(metric):\n",
    "    \n",
    "    if metric == 'custom_mae':\n",
    "        return [custom_mae]\n",
    "    elif metric == 'tf.keras.metrics.MeanAbsoluteError()':\n",
    "        return [tf.keras.metrics.MeanAbsoluteError()]\n",
    "    elif metric == 'circular_mae':\n",
    "        return [circular_mae]\n",
    "    elif metric == 'mean_squared_error':\n",
    "        return ['mean_squared_error']\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'keras.optimizers.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Nadam'>\":\n",
    "        return Nadam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Adagard'>\":\n",
    "        return Adagard\n",
    "    elif optimizer == \"<class 'keras.optimizers.RMSprop'>\":\n",
    "        return RMSprop\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsset-Typ nach String Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_to_string(ts):\n",
    "    if ts == TrainingSet.SYNTHETIC:\n",
    "        return 'Synth'\n",
    "    elif ts == TrainingSet.REAL:\n",
    "        return 'Real'\n",
    "    elif ts == TrainingSet.MIXED:\n",
    "        return 'Mixed'\n",
    "    else:\n",
    "        print('Unknown TrainingSet')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(batch_size, num_samples, label_type):\n",
    "    # if Block für synthetische Daten, um nur auf realen Daten zu trainieren _USE_SYNTHETIC_TRAIN_DATA\n",
    "    # 1. lege df_train und df_valid als leere Liste an\n",
    "    # 2. If-block um Zeile df = ... bis df_valid\n",
    "    \n",
    "    if trainingset == TrainingSet.SYNTHETIC:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "    elif trainingset == TrainingSet.MIXED:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train_real = df_shuffled_real[0: int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid_real = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train_real.shape[0]])\n",
    "        df_train = df_train.drop(df_train.index[df_train.shape[0] - df_train_real.shape[0] : df_train.shape[0]])\n",
    "        df_valid = df_valid.drop(df_valid.index[df_valid.shape[0] - df_valid_real.shape[0] : df_valid.shape[0]])\n",
    "        df_train = df_train.append(df_train_real)\n",
    "        df_valid= df_valid.append(df_valid_real)\n",
    "    \n",
    "    elif trainingset == TrainingSet.REAL: # Add check for num_samples, once the real dataset increases\n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train = df_shuffled_real[0 : int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train.shape[0]])\n",
    "        \n",
    "    else:\n",
    "        print('Create_Data :: should not have reached here')\n",
    "        \n",
    "\n",
    "        \n",
    "    if _USE_DATA_AUGMENTATION:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.25, 0.75),\n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    print('Y-Col: {}'.format(get_Label_Names(label_type)))\n",
    "    print('Train Data Generator: ', end = '')\n",
    "    \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename RGB',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    print('Validation Data Generator: ', end = '')\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename RGB',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Modell (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_model_fine(x, y, x_val, y_val, params):\n",
    "    print('==========================Params:')\n",
    "    print(params)\n",
    "    print('==========================')\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data(params['batch_size'], params['samples'], params['label_type'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    "    dropout_rate = params['dropout']\n",
    "    first_neuron = params['first_neuron']\n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = params['leaky_alpha'])\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    cnn = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "    \n",
    "    for layer in cnn.layers[:15]:\n",
    "        layer.trainable = False\n",
    "        #print(layer.name, layer.trainable)\n",
    "        \n",
    "    print('_________________________________________________________________')\n",
    "    print('{:>16} {:>16}'.format('Network Layer', 'Trainable'))\n",
    "    print('=================================================================')\n",
    "    for layer in cnn.layers:\n",
    "        print('{:>16} {:>16}'.format(layer.name, layer.trainable))\n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    model.add(cnn)\n",
    "    \n",
    "    fc = Sequential()\n",
    "    fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512)\n",
    "    \n",
    "    fc.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    print('Number Hidden Layers {}'.format(params['hidden_layers']))\n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(params['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.load_weights(_MODEL_DIR + _MODEL_TO_LOAD)\n",
    "    model.add(fc)\n",
    "    print('Fully Connected Layers added to Base Network')\n",
    "    \n",
    "    print('Using Loss: {} \\nand Reduction Metric: {}'.format(\n",
    "        params['loss_function'], \n",
    "        get_Reduction_Metric(params['reduction_metric'])))\n",
    "    \n",
    "    model.compile(\n",
    "        #optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])*1e-2),\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer']) * 1e-3),\n",
    "        loss = params['loss_function'],\n",
    "        metrics = get_Reduction_Metric(params['reduction_metric'])\n",
    "    )\n",
    "    print('Model was compiled')\n",
    "    print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = _LOG_DIR + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        monitor =  params['monitor_value'],\n",
    "        verbose = 1,\n",
    "        save_weights_only = False,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    print('Checkpointer was created')\n",
    "    \n",
    "    csv_logger = callbacks.CSVLogger(\n",
    "        filename = _LOG_DIR + 'CNN_Base_{}_Logger_{}.csv'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        separator = ',',\n",
    "        append = False\n",
    "    )\n",
    "    print('CSV Logger was created')\n",
    "\n",
    "    lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 13,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        min_delta = 0.0001\n",
    "    )\n",
    "    print('Learning Rate Reducer was created')\n",
    "    \n",
    "    early_stopper = callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0,\n",
    "        #patience = 15,\n",
    "        patience = 20,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    print('Early Stopper was created')\n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_data = valid_generator,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        callbacks = [checkpointer, csv_logger, lr_reducer, early_stopper],\n",
    "        epochs = params['epochs'],\n",
    "        workers = 8\n",
    "    )\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feinoptimierung <a name = \"setup\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "global_hyper_parameter = {\n",
    "    'samples': None,\n",
    "    'epochs': None,\n",
    "    'batch_size': None,\n",
    "    'optimizer': None,\n",
    "    'lr': None,\n",
    "    'first_neuron': None,\n",
    "    'dropout': None,\n",
    "    'activation': None,\n",
    "    'leaky_alpha': None,\n",
    "    'hidden_layers': None,\n",
    "    # beginning from here, Values should only contain one single entry:\n",
    "    # ===============================================================\n",
    "    'label_type': ['Angular'], # Stereographic, Angular, Normalized\n",
    "    'loss_function': None,\n",
    "    'reduction_metric': None,\n",
    "    'monitor_value': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RUN = 'SYNTH'\n",
    "_LOSS = 'MSE'\n",
    "_DATASET_NAME = '201019_2253_final'#'2020-05-28'\n",
    "_DEVICE = 'GeForce_RTX_2080_Ti'#'TITAN_GPU1'\n",
    "\n",
    "storage = OutputDirectory.SSD # 'fast_output' if ssd storage may suffice, 'output' otherwise\n",
    "\n",
    "if global_hyper_parameter['label_type'][0] == 'Stereographic':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_stereographic.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_stereographic.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Angular':\n",
    "    _CSV_SYNTH_FILE_NAME = 'labels_ks.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Normalized':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_normalized.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_normalized.csv'\n",
    "    \n",
    "else:\n",
    "    assert(True, 'Label Type Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = TrainingSet.SYNTHETIC\n",
    "_USE_DATA_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n"
     ]
    }
   ],
   "source": [
    "_IMAGE_DIR = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(_DATASET_NAME)\n",
    "_CSV_FILE = _IMAGE_DIR + _CSV_SYNTH_FILE_NAME\n",
    "_CSV_FILE_REAL = _IMAGE_DIR + _CSV_REAL_FILE_NAME\n",
    "\n",
    "_note = '_Custom-MAE'\n",
    "\n",
    "_MODEL_DIR = '..\\\\output\\\\{}_Regression_{}\\\\{}_{}_Base{}\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "_NET_DIR = '{}_Regression_{}\\\\{}_{}_Top_1{}\\\\{}_TD\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note, trainingset_to_string(trainingset))\n",
    "_LOG_DIR = '..\\\\{}\\\\{}'.format(output_path[storage], _NET_DIR)\n",
    "\n",
    "if(not os.path.exists(_LOG_DIR)):\n",
    "    os.makedirs(_LOG_DIR)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(_LOG_DIR))\n",
    "\n",
    "device_file = open(_LOG_DIR + '{}.txt'.format(_DEVICE), \"a+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 FC-Gewichte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: ..\\output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Base_Custom-MAE\\..\\201019_2253_final_Angular_Base_Custom-MAE_Results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>loss</th>\n",
       "      <th>custom_mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_custom_mae</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>label_type</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>lr</th>\n",
       "      <th>monitor_value</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>reduction_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>10/22/20-210538</td>\n",
       "      <td>10/22/20-210553</td>\n",
       "      <td>14.862882</td>\n",
       "      <td>4383.826411</td>\n",
       "      <td>40.741211</td>\n",
       "      <td>2547.475186</td>\n",
       "      <td>28.446991</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>5</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>10/22/20-212354</td>\n",
       "      <td>10/22/20-212407</td>\n",
       "      <td>12.900292</td>\n",
       "      <td>4370.115269</td>\n",
       "      <td>41.885189</td>\n",
       "      <td>2500.798960</td>\n",
       "      <td>28.509195</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>10/22/20-210249</td>\n",
       "      <td>10/22/20-210259</td>\n",
       "      <td>9.804774</td>\n",
       "      <td>4001.966997</td>\n",
       "      <td>39.212330</td>\n",
       "      <td>2567.742597</td>\n",
       "      <td>28.672417</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>5</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>10/22/20-210657</td>\n",
       "      <td>10/22/20-210713</td>\n",
       "      <td>16.076730</td>\n",
       "      <td>5342.255862</td>\n",
       "      <td>44.598431</td>\n",
       "      <td>2508.385596</td>\n",
       "      <td>29.009453</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>10/22/20-212434</td>\n",
       "      <td>10/22/20-212447</td>\n",
       "      <td>12.947577</td>\n",
       "      <td>4892.983958</td>\n",
       "      <td>44.019562</td>\n",
       "      <td>2545.006084</td>\n",
       "      <td>29.102833</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109</td>\n",
       "      <td>10/22/20-212138</td>\n",
       "      <td>10/22/20-212146</td>\n",
       "      <td>8.363558</td>\n",
       "      <td>4502.218566</td>\n",
       "      <td>42.888409</td>\n",
       "      <td>2549.238323</td>\n",
       "      <td>29.138407</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>10/22/20-210641</td>\n",
       "      <td>10/22/20-210657</td>\n",
       "      <td>15.954672</td>\n",
       "      <td>4115.395587</td>\n",
       "      <td>41.639866</td>\n",
       "      <td>2575.603947</td>\n",
       "      <td>29.298376</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>10/22/20-212129</td>\n",
       "      <td>10/22/20-212137</td>\n",
       "      <td>8.271264</td>\n",
       "      <td>3881.180195</td>\n",
       "      <td>40.146988</td>\n",
       "      <td>2558.554630</td>\n",
       "      <td>29.299927</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>10/22/20-210300</td>\n",
       "      <td>10/22/20-210310</td>\n",
       "      <td>10.109706</td>\n",
       "      <td>3871.796719</td>\n",
       "      <td>39.797153</td>\n",
       "      <td>2617.595140</td>\n",
       "      <td>29.322357</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>10/22/20-210403</td>\n",
       "      <td>10/22/20-210413</td>\n",
       "      <td>10.425704</td>\n",
       "      <td>4304.128398</td>\n",
       "      <td>43.794651</td>\n",
       "      <td>2461.137874</td>\n",
       "      <td>29.322683</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0            start              end   duration         loss  \\\n",
       "32           32  10/22/20-210538  10/22/20-210553  14.862882  4383.826411   \n",
       "123         123  10/22/20-212354  10/22/20-212407  12.900292  4370.115269   \n",
       "17           17  10/22/20-210249  10/22/20-210259   9.804774  4001.966997   \n",
       "37           37  10/22/20-210657  10/22/20-210713  16.076730  5342.255862   \n",
       "126         126  10/22/20-212434  10/22/20-212447  12.947577  4892.983958   \n",
       "109         109  10/22/20-212138  10/22/20-212146   8.363558  4502.218566   \n",
       "36           36  10/22/20-210641  10/22/20-210657  15.954672  4115.395587   \n",
       "108         108  10/22/20-212129  10/22/20-212137   8.271264  3881.180195   \n",
       "18           18  10/22/20-210300  10/22/20-210310  10.109706  3871.796719   \n",
       "24           24  10/22/20-210403  10/22/20-210413  10.425704  4304.128398   \n",
       "\n",
       "     custom_mae     val_loss  val_custom_mae activation  batch_size  dropout  \\\n",
       "32    40.741211  2547.475186       28.446991  leakyrelu          32     0.25   \n",
       "123   41.885189  2500.798960       28.509195  leakyrelu          64     0.25   \n",
       "17    39.212330  2567.742597       28.672417  leakyrelu          32     0.25   \n",
       "37    44.598431  2508.385596       29.009453  leakyrelu          32     0.25   \n",
       "126   44.019562  2545.006084       29.102833  leakyrelu          64     0.25   \n",
       "109   42.888409  2549.238323       29.138407  leakyrelu          64     0.25   \n",
       "36    41.639866  2575.603947       29.298376  leakyrelu          32     0.25   \n",
       "108   40.146988  2558.554630       29.299927  leakyrelu          64     0.25   \n",
       "18    39.797153  2617.595140       29.322357  leakyrelu          32     0.25   \n",
       "24    43.794651  2461.137874       29.322683  leakyrelu          32     0.25   \n",
       "\n",
       "     first_neuron  hidden_layers label_type       loss_function  lr  \\\n",
       "32           4096              0    Angular  mean_squared_error   5   \n",
       "123          4096              1    Angular  mean_squared_error   1   \n",
       "17           2048              0    Angular  mean_squared_error   5   \n",
       "37           4096              2    Angular  mean_squared_error   2   \n",
       "126          4096              2    Angular  mean_squared_error   1   \n",
       "109          2048              1    Angular  mean_squared_error   2   \n",
       "36           4096              2    Angular  mean_squared_error   1   \n",
       "108          2048              1    Angular  mean_squared_error   1   \n",
       "18           2048              1    Angular  mean_squared_error   1   \n",
       "24           2048              3    Angular  mean_squared_error   1   \n",
       "\n",
       "      monitor_value                                          optimizer  \\\n",
       "32   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "123  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "17   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "37   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "126  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "109  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "36   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "108  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "18   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "24   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "\n",
       "    reduction_metric  \n",
       "32        custom_mae  \n",
       "123       custom_mae  \n",
       "17        custom_mae  \n",
       "37        custom_mae  \n",
       "126       custom_mae  \n",
       "109       custom_mae  \n",
       "36        custom_mae  \n",
       "108       custom_mae  \n",
       "18        custom_mae  \n",
       "24        custom_mae  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_results = _MODEL_DIR + '..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "df = pd.read_csv(base_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "sort_value = df['monitor_value'][0]\n",
    "df = df.sort_values(sort_value, axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(base_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(top_results_index):\n",
    "    \n",
    "    #     Adam = RMSprop + Momentum (lr=0.001)\n",
    "    #     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "    #     RMSprop = (lr=0.001)\n",
    "    #     SGD = (lr=0.01)\n",
    "    #     Adagrad\n",
    "\n",
    "    hyper_parameter = global_hyper_parameter\n",
    "\n",
    "    hyper_parameter['samples'] = [100000] \n",
    "    hyper_parameter['epochs'] = [400]\n",
    "    hyper_parameter['batch_size'] = [df.iloc[top_results_index]['batch_size']]\n",
    "    hyper_parameter['optimizer'] = [make_optimizer(df.loc[top_results_index]['optimizer'])]\n",
    "    hyper_parameter['lr'] = [df.iloc[top_results_index]['lr']]\n",
    "    hyper_parameter['first_neuron'] = [df.iloc[top_results_index]['first_neuron']]\n",
    "    hyper_parameter['dropout'] = [df.iloc[top_results_index]['dropout']]\n",
    "    hyper_parameter['activation'] = [df.iloc[top_results_index]['activation']]\n",
    "    hyper_parameter['leaky_alpha'] = [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "    hyper_parameter['hidden_layers'] = [df.iloc[top_results_index]['hidden_layers']]\n",
    "    \n",
    "    hyper_parameter['loss_function'] = [df.iloc[top_results_index]['loss_function']]\n",
    "    hyper_parameter['reduction_metric'] = [df.iloc[top_results_index]['reduction_metric']]\n",
    "    hyper_parameter['monitor_value'] = [df.iloc[top_results_index]['monitor_value']]\n",
    "\n",
    "    return hyper_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 4096, 'hidden_layers': 0, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================Params:\n",
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 4096, 'hidden_layers': 0, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================\n",
      "Y-Col: ['Elevation', 'Azimuth']\n",
      "Train Data Generator: Found 80000 validated image filenames.\n",
      "Validation Data Generator: Found 20000 validated image filenames.\n",
      "Steps per Epoch: 2500, Validation Steps: 625\n",
      "_________________________________________________________________\n",
      "   Network Layer        Trainable\n",
      "=================================================================\n",
      "         input_1                0\n",
      "    block1_conv1                0\n",
      "    block1_conv2                0\n",
      "     block1_pool                0\n",
      "    block2_conv1                0\n",
      "    block2_conv2                0\n",
      "     block2_pool                0\n",
      "    block3_conv1                0\n",
      "    block3_conv2                0\n",
      "    block3_conv3                0\n",
      "     block3_pool                0\n",
      "    block4_conv1                0\n",
      "    block4_conv2                0\n",
      "    block4_conv3                0\n",
      "     block4_pool                0\n",
      "    block5_conv1                1\n",
      "    block5_conv2                1\n",
      "    block5_conv3                1\n",
      "     block5_pool                1\n",
      "_________________________________________________________________\n",
      "\n",
      "Number Hidden Layers 0\n",
      "Fully Connected Layers added to Base Network\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x000002418892E1F8>]\n",
      "Model was compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 2)                 102772738 \n",
      "=================================================================\n",
      "Total params: 117,487,426\n",
      "Trainable params: 109,852,162\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Checkpointer was created\n",
      "CSV Logger was created\n",
      "Learning Rate Reducer was created\n",
      "Early Stopper was created\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2500 steps, validate for 625 steps\n",
      "Epoch 1/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 4491.1428 - custom_mae: 44.9546\n",
      "Epoch 00001: val_custom_mae improved from inf to 33.93222, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 344s 138ms/step - loss: 4490.4625 - custom_mae: 44.9489 - val_loss: 2783.9253 - val_custom_mae: 33.9322\n",
      "Epoch 2/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 2584.0053 - custom_mae: 31.6849\n",
      "Epoch 00002: val_custom_mae improved from 33.93222 to 27.83964, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 265s 106ms/step - loss: 2583.9207 - custom_mae: 31.6831 - val_loss: 2048.9408 - val_custom_mae: 27.8396\n",
      "Epoch 3/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 2092.1009 - custom_mae: 27.4297\n",
      "Epoch 00003: val_custom_mae improved from 27.83964 to 24.74543, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 2091.8259 - custom_mae: 27.4281 - val_loss: 1752.6252 - val_custom_mae: 24.7454\n",
      "Epoch 4/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1807.5790 - custom_mae: 24.8440\n",
      "Epoch 00004: val_custom_mae improved from 24.74543 to 23.24442, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 1807.5735 - custom_mae: 24.8448 - val_loss: 1613.3787 - val_custom_mae: 23.2444\n",
      "Epoch 5/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1633.3249 - custom_mae: 23.1498\n",
      "Epoch 00005: val_custom_mae improved from 23.24442 to 22.13437, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 262s 105ms/step - loss: 1633.2391 - custom_mae: 23.1495 - val_loss: 1519.2738 - val_custom_mae: 22.1344\n",
      "Epoch 6/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1517.6916 - custom_mae: 21.9708\n",
      "Epoch 00006: val_custom_mae improved from 22.13437 to 21.07001, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 261s 104ms/step - loss: 1517.4295 - custom_mae: 21.9696 - val_loss: 1394.8790 - val_custom_mae: 21.0700\n",
      "Epoch 7/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1416.6104 - custom_mae: 20.9579\n",
      "Epoch 00007: val_custom_mae improved from 21.07001 to 20.04381, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 261s 105ms/step - loss: 1416.5026 - custom_mae: 20.9571 - val_loss: 1318.2332 - val_custom_mae: 20.0438\n",
      "Epoch 8/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1339.3253 - custom_mae: 20.1483\n",
      "Epoch 00008: val_custom_mae improved from 20.04381 to 19.85958, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 261s 105ms/step - loss: 1339.4192 - custom_mae: 20.1490 - val_loss: 1318.9889 - val_custom_mae: 19.8596\n",
      "Epoch 9/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1276.4327 - custom_mae: 19.4786 ETA: 0s - loss: 1274.5731 - c\n",
      "Epoch 00009: val_custom_mae improved from 19.85958 to 18.99544, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 105ms/step - loss: 1276.3260 - custom_mae: 19.4779 - val_loss: 1226.3346 - val_custom_mae: 18.9954\n",
      "Epoch 10/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1237.6960 - custom_mae: 18.9240\n",
      "Epoch 00010: val_custom_mae improved from 18.99544 to 18.55334, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 1237.3674 - custom_mae: 18.9217 - val_loss: 1187.6494 - val_custom_mae: 18.5533\n",
      "Epoch 11/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1178.8501 - custom_mae: 18.3069\n",
      "Epoch 00011: val_custom_mae improved from 18.55334 to 18.16761, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 1178.7720 - custom_mae: 18.3068 - val_loss: 1181.3023 - val_custom_mae: 18.1676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1137.4354 - custom_mae: 17.8849\n",
      "Epoch 00012: val_custom_mae improved from 18.16761 to 17.56576, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 1137.4420 - custom_mae: 17.8854 - val_loss: 1135.9251 - val_custom_mae: 17.5658\n",
      "Epoch 13/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1103.2302 - custom_mae: 17.4637\n",
      "Epoch 00013: val_custom_mae did not improve from 17.56576\n",
      "2500/2500 [==============================] - 259s 103ms/step - loss: 1103.4053 - custom_mae: 17.4644 - val_loss: 1154.9486 - val_custom_mae: 17.7272\n",
      "Epoch 14/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1071.7986 - custom_mae: 17.1018\n",
      "Epoch 00014: val_custom_mae improved from 17.56576 to 17.15912, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 1071.6243 - custom_mae: 17.1012 - val_loss: 1095.0198 - val_custom_mae: 17.1591\n",
      "Epoch 15/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1044.4666 - custom_mae: 16.7202\n",
      "Epoch 00015: val_custom_mae did not improve from 17.15912\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 1044.2061 - custom_mae: 16.7188 - val_loss: 1117.2121 - val_custom_mae: 17.4385\n",
      "Epoch 16/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1017.3745 - custom_mae: 16.4117\n",
      "Epoch 00016: val_custom_mae improved from 17.15912 to 16.42795, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 1018.0595 - custom_mae: 16.4163 - val_loss: 1046.9299 - val_custom_mae: 16.4279\n",
      "Epoch 17/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 996.6726 - custom_mae: 16.1323- ETA: 7s - los\n",
      "Epoch 00017: val_custom_mae did not improve from 16.42795\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 996.8619 - custom_mae: 16.1324 - val_loss: 1148.0882 - val_custom_mae: 17.8825\n",
      "Epoch 18/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 971.3091 - custom_mae: 15.8404- ETA: 0s - loss: 971.6668 - custom_mae: 1\n",
      "Epoch 00018: val_custom_mae improved from 16.42795 to 15.79748, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 262s 105ms/step - loss: 971.4012 - custom_mae: 15.8397 - val_loss: 982.1584 - val_custom_mae: 15.7975\n",
      "Epoch 19/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 951.5942 - custom_mae: 15.6081\n",
      "Epoch 00019: val_custom_mae did not improve from 15.79748\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 951.9076 - custom_mae: 15.6108 - val_loss: 1011.1295 - val_custom_mae: 16.2305\n",
      "Epoch 20/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 930.8349 - custom_mae: 15.3383- ETA\n",
      "Epoch 00020: val_custom_mae did not improve from 15.79748\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 930.6268 - custom_mae: 15.3369 - val_loss: 1037.5119 - val_custom_mae: 16.4004\n",
      "Epoch 21/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 912.0362 - custom_mae: 15.1340- ET\n",
      "Epoch 00021: val_custom_mae did not improve from 15.79748\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 911.9131 - custom_mae: 15.1330 - val_loss: 1070.2333 - val_custom_mae: 16.2996\n",
      "Epoch 22/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 896.4042 - custom_mae: 14.8949\n",
      "Epoch 00022: val_custom_mae improved from 15.79748 to 15.10876, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 896.3395 - custom_mae: 14.8942 - val_loss: 953.2078 - val_custom_mae: 15.1088\n",
      "Epoch 23/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 878.0293 - custom_mae: 14.6406- ETA: 3s - loss: 877.1566 - cus - ETA: 2s - loss: 8\n",
      "Epoch 00023: val_custom_mae improved from 15.10876 to 15.03766, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 878.0956 - custom_mae: 14.6413 - val_loss: 946.0226 - val_custom_mae: 15.0377\n",
      "Epoch 24/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 859.6100 - custom_mae: 14.3706\n",
      "Epoch 00024: val_custom_mae did not improve from 15.03766\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 860.0611 - custom_mae: 14.3732 - val_loss: 972.6485 - val_custom_mae: 15.8025\n",
      "Epoch 25/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 845.4330 - custom_mae: 14.2320- ETA: 6s - loss:  - \n",
      "Epoch 00025: val_custom_mae improved from 15.03766 to 14.52485, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 845.3108 - custom_mae: 14.2314 - val_loss: 882.9583 - val_custom_mae: 14.5248\n",
      "Epoch 26/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 838.7857 - custom_mae: 14.1017\n",
      "Epoch 00026: val_custom_mae did not improve from 14.52485\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 838.5728 - custom_mae: 14.1007 - val_loss: 908.2556 - val_custom_mae: 14.6019\n",
      "Epoch 27/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 820.8695 - custom_mae: 13.8789\n",
      "Epoch 00027: val_custom_mae did not improve from 14.52485\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 821.0496 - custom_mae: 13.8800 - val_loss: 886.0125 - val_custom_mae: 14.6898\n",
      "Epoch 28/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 812.0089 - custom_mae: 13.7775\n",
      "Epoch 00028: val_custom_mae improved from 14.52485 to 14.38498, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 262s 105ms/step - loss: 812.3569 - custom_mae: 13.7786 - val_loss: 886.3823 - val_custom_mae: 14.3850\n",
      "Epoch 29/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 797.9267 - custom_mae: 13.5647\n",
      "Epoch 00029: val_custom_mae improved from 14.38498 to 13.57374, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 798.0046 - custom_mae: 13.5647 - val_loss: 827.7013 - val_custom_mae: 13.5737\n",
      "Epoch 30/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 786.1635 - custom_mae: 13.3977\n",
      "Epoch 00030: val_custom_mae did not improve from 13.57374\n",
      "2500/2500 [==============================] - 257s 103ms/step - loss: 786.6214 - custom_mae: 13.4009 - val_loss: 851.7161 - val_custom_mae: 14.3206\n",
      "Epoch 31/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 782.4621 - custom_mae: 13.2799\n",
      "Epoch 00031: val_custom_mae improved from 13.57374 to 13.31459, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 782.4273 - custom_mae: 13.2798 - val_loss: 810.3981 - val_custom_mae: 13.3146\n",
      "Epoch 32/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 763.2402 - custom_mae: 13.1212\n",
      "Epoch 00032: val_custom_mae did not improve from 13.31459\n",
      "2500/2500 [==============================] - 267s 107ms/step - loss: 763.0798 - custom_mae: 13.1209 - val_loss: 905.6849 - val_custom_mae: 14.4185\n",
      "Epoch 33/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2500 [============================>.] - ETA: 0s - loss: 752.7549 - custom_mae: 12.9846\n",
      "Epoch 00033: val_custom_mae did not improve from 13.31459\n",
      "2500/2500 [==============================] - 264s 105ms/step - loss: 752.6269 - custom_mae: 12.9839 - val_loss: 865.8044 - val_custom_mae: 13.9774\n",
      "Epoch 34/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 741.0006 - custom_mae: 12.8299\n",
      "Epoch 00034: val_custom_mae did not improve from 13.31459\n",
      "2500/2500 [==============================] - 260s 104ms/step - loss: 741.0123 - custom_mae: 12.8308 - val_loss: 830.4286 - val_custom_mae: 13.8514\n",
      "Epoch 35/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 735.2980 - custom_mae: 12.7096- ETA: 3s \n",
      "Epoch 00035: val_custom_mae improved from 13.31459 to 12.97901, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 735.1506 - custom_mae: 12.7090 - val_loss: 793.8654 - val_custom_mae: 12.9790\n",
      "Epoch 36/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 728.9868 - custom_mae: 12.6260- ETA: 3s - loss: - ETA: 1s - loss: 728.8126 - custom - ETA: 0s - loss: 728.8610 - custom_mae: 12.6256\n",
      "Epoch 00036: val_custom_mae improved from 12.97901 to 12.87944, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 105ms/step - loss: 728.8044 - custom_mae: 12.6252 - val_loss: 827.4906 - val_custom_mae: 12.8794\n",
      "Epoch 37/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 712.2719 - custom_mae: 12.3964- ETA: 1s - loss: 712.5377 - custom_\n",
      "Epoch 00037: val_custom_mae improved from 12.87944 to 12.84490, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 265s 106ms/step - loss: 712.6354 - custom_mae: 12.3989 - val_loss: 780.0144 - val_custom_mae: 12.8449\n",
      "Epoch 38/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 709.9946 - custom_mae: 12.3530- ETA: 0s - loss: 709.4349 - custom_mae:\n",
      "Epoch 00038: val_custom_mae did not improve from 12.84490\n",
      "2500/2500 [==============================] - 259s 104ms/step - loss: 710.0800 - custom_mae: 12.3543 - val_loss: 776.0210 - val_custom_mae: 12.9875\n",
      "Epoch 39/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 691.9264 - custom_mae: 12.2188- ETA: 4s - loss: 691.5 - ETA: 2s - loss: 690\n",
      "Epoch 00039: val_custom_mae improved from 12.84490 to 12.79591, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 692.2613 - custom_mae: 12.2204 - val_loss: 760.6111 - val_custom_mae: 12.7959\n",
      "Epoch 40/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 691.4916 - custom_mae: 12.0865- ETA: 8s - los - ETA: 5s - loss: - ETA: 2s - loss: 692.8529 - custo - ETA: 1s - loss: 692.9436 - cust - ETA: 0s - loss: 692.0004 - custom_mae: 12.0\n",
      "Epoch 00040: val_custom_mae improved from 12.79591 to 12.49916, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 691.3088 - custom_mae: 12.0852 - val_loss: 761.0820 - val_custom_mae: 12.4992\n",
      "Epoch 41/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 684.3592 - custom_mae: 12.0047- ETA: 2s - loss: 683.9247 - custom_mae: 12 - ETA: 2s - loss: 68\n",
      "Epoch 00041: val_custom_mae did not improve from 12.49916\n",
      "2500/2500 [==============================] - 259s 104ms/step - loss: 684.3069 - custom_mae: 12.0040 - val_loss: 771.8906 - val_custom_mae: 12.5398\n",
      "Epoch 42/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 679.2414 - custom_mae: 11.8750\n",
      "Epoch 00042: val_custom_mae improved from 12.49916 to 12.39582, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 679.1368 - custom_mae: 11.8743 - val_loss: 752.7109 - val_custom_mae: 12.3958\n",
      "Epoch 43/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 673.5709 - custom_mae: 11.8231- ETA: 1s - loss: 673.9677 - custom\n",
      "Epoch 00043: val_custom_mae improved from 12.39582 to 12.03956, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 673.4549 - custom_mae: 11.8220 - val_loss: 743.1318 - val_custom_mae: 12.0396\n",
      "Epoch 44/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 662.9205 - custom_mae: 11.7144 - ETA: 1s - loss: 663.1979 - custom_\n",
      "Epoch 00044: val_custom_mae did not improve from 12.03956\n",
      "2500/2500 [==============================] - 259s 104ms/step - loss: 663.4187 - custom_mae: 11.7167 - val_loss: 732.6219 - val_custom_mae: 12.2936\n",
      "Epoch 45/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 655.8076 - custom_mae: 11.5659\n",
      "Epoch 00045: val_custom_mae improved from 12.03956 to 11.90422, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 656.0413 - custom_mae: 11.5671 - val_loss: 724.7295 - val_custom_mae: 11.9042\n",
      "Epoch 46/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 647.7040 - custom_mae: 11.4753\n",
      "Epoch 00046: val_custom_mae did not improve from 11.90422\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 647.5045 - custom_mae: 11.4741 - val_loss: 729.1344 - val_custom_mae: 11.9549\n",
      "Epoch 47/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 646.0253 - custom_mae: 11.4187\n",
      "Epoch 00047: val_custom_mae improved from 11.90422 to 11.65496, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 645.8843 - custom_mae: 11.4184 - val_loss: 706.7203 - val_custom_mae: 11.6550\n",
      "Epoch 48/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 639.3240 - custom_mae: 11.2819- ETA: 0s - loss: 639.2739 - custom_mae: 11.281\n",
      "Epoch 00048: val_custom_mae did not improve from 11.65496\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 639.2847 - custom_mae: 11.2808 - val_loss: 760.2999 - val_custom_mae: 11.7822\n",
      "Epoch 49/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 642.8859 - custom_mae: 11.2874- ETA:\n",
      "Epoch 00049: val_custom_mae did not improve from 11.65496\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 642.7430 - custom_mae: 11.2868 - val_loss: 750.0358 - val_custom_mae: 11.9160\n",
      "Epoch 50/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 627.3053 - custom_mae: 11.1134\n",
      "Epoch 00050: val_custom_mae improved from 11.65496 to 11.50635, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 627.2158 - custom_mae: 11.1135 - val_loss: 700.2916 - val_custom_mae: 11.5064\n",
      "Epoch 51/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 623.0052 - custom_mae: 11.0510\n",
      "Epoch 00051: val_custom_mae did not improve from 11.50635\n",
      "2500/2500 [==============================] - 259s 103ms/step - loss: 622.8490 - custom_mae: 11.0502 - val_loss: 709.6373 - val_custom_mae: 11.8641\n",
      "Epoch 52/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2500 [============================>.] - ETA: 0s - loss: 621.2222 - custom_mae: 10.9989  ETA: 11s - loss: 621.13 - ETA: 10s - loss: 621 - ETA: 8s - loss: 621.8086 - custom_mae - ETA: 7s - loss: 620.9620 - custom_mae: - ETA: 6s - loss: 621.1 - ETA: 4s - loss: 621.1546 - custo - ETA: 3s - \n",
      "Epoch 00052: val_custom_mae did not improve from 11.50635\n",
      "2500/2500 [==============================] - 259s 104ms/step - loss: 621.2014 - custom_mae: 10.9984 - val_loss: 715.5433 - val_custom_mae: 12.3339\n",
      "Epoch 53/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 611.2411 - custom_mae: 10.9049\n",
      "Epoch 00053: val_custom_mae improved from 11.50635 to 11.37237, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 611.1908 - custom_mae: 10.9053 - val_loss: 703.5482 - val_custom_mae: 11.3724\n",
      "Epoch 54/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 610.7855 - custom_mae: 10.8310\n",
      "Epoch 00054: val_custom_mae did not improve from 11.37237\n",
      "2500/2500 [==============================] - 259s 103ms/step - loss: 611.0014 - custom_mae: 10.8320 - val_loss: 758.3417 - val_custom_mae: 11.4344\n",
      "Epoch 55/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 600.4381 - custom_mae: 10.7431- ETA: 2s - loss: 5\n",
      "Epoch 00055: val_custom_mae improved from 11.37237 to 11.32313, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 265s 106ms/step - loss: 600.6827 - custom_mae: 10.7445 - val_loss: 690.6192 - val_custom_mae: 11.3231\n",
      "Epoch 56/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 599.2440 - custom_mae: 10.6920-\n",
      "Epoch 00056: val_custom_mae improved from 11.32313 to 11.13101, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 265s 106ms/step - loss: 599.0587 - custom_mae: 10.6910 - val_loss: 677.9786 - val_custom_mae: 11.1310\n",
      "Epoch 57/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 597.3163 - custom_mae: 10.6253\n",
      "Epoch 00057: val_custom_mae improved from 11.13101 to 10.89316, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 265s 106ms/step - loss: 597.1785 - custom_mae: 10.6248 - val_loss: 659.8401 - val_custom_mae: 10.8932\n",
      "Epoch 58/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 596.9889 - custom_mae: 10.5734\n",
      "Epoch 00058: val_custom_mae did not improve from 10.89316\n",
      "2500/2500 [==============================] - 259s 104ms/step - loss: 596.8128 - custom_mae: 10.5726 - val_loss: 669.3630 - val_custom_mae: 11.0700\n",
      "Epoch 59/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 590.1307 - custom_mae: 10.5245- ETA: 0s - loss: 590.3861 - custom_mae: 10.5 - ETA: 0s - loss: 590.2706 - custom_mae: \n",
      "Epoch 00059: val_custom_mae did not improve from 10.89316\n",
      "2500/2500 [==============================] - 259s 103ms/step - loss: 590.0519 - custom_mae: 10.5250 - val_loss: 677.1228 - val_custom_mae: 11.4277\n",
      "Epoch 60/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 579.1285 - custom_mae: 10.3809\n",
      "Epoch 00060: val_custom_mae improved from 10.89316 to 10.54770, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 578.9796 - custom_mae: 10.3799 - val_loss: 655.3084 - val_custom_mae: 10.5477\n",
      "Epoch 61/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 577.8735 - custom_mae: 10.3458\n",
      "Epoch 00061: val_custom_mae did not improve from 10.54770\n",
      "2500/2500 [==============================] - 260s 104ms/step - loss: 577.9641 - custom_mae: 10.3469 - val_loss: 654.7770 - val_custom_mae: 10.7199\n",
      "Epoch 62/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 573.7031 - custom_mae: 10.2844  ETA: 12s - loss: 571.0862 - custom_mae\n",
      "Epoch 00062: val_custom_mae did not improve from 10.54770\n",
      "2500/2500 [==============================] - 260s 104ms/step - loss: 573.7397 - custom_mae: 10.2845 - val_loss: 650.7692 - val_custom_mae: 10.8554\n",
      "Epoch 63/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 575.2460 - custom_mae: 10.2379\n",
      "Epoch 00063: val_custom_mae did not improve from 10.54770\n",
      "2500/2500 [==============================] - 260s 104ms/step - loss: 575.3506 - custom_mae: 10.2385 - val_loss: 639.0225 - val_custom_mae: 10.6965\n",
      "Epoch 64/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 567.7297 - custom_mae: 10.1559- ETA: 8s  - ETA: 5s - loss: 568.08 - ETA: 3s - loss: 567.4476 - custom_ma - ETA: 2s - loss: 567.\n",
      "Epoch 00064: val_custom_mae did not improve from 10.54770\n",
      "2500/2500 [==============================] - 259s 104ms/step - loss: 567.7662 - custom_mae: 10.1565 - val_loss: 667.1717 - val_custom_mae: 10.7352\n",
      "Epoch 65/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 567.3514 - custom_mae: 10.0999\n",
      "Epoch 00065: val_custom_mae improved from 10.54770 to 10.42037, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 265s 106ms/step - loss: 567.3221 - custom_mae: 10.0998 - val_loss: 644.8280 - val_custom_mae: 10.4204\n",
      "Epoch 66/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 557.0303 - custom_mae: 10.0156  ETA: 11s - loss: 554.8955 - custom_ - ETA: 10s - loss: 555.0661 - custom_m - ETA: 7s - los - ETA: 5s - ETA: 1s - loss: 555.6333 - cu - ETA: 0s - loss: 556.0823 - custom_mae: 10.\n",
      "Epoch 00066: val_custom_mae did not improve from 10.42037\n",
      "2500/2500 [==============================] - 259s 104ms/step - loss: 557.1563 - custom_mae: 10.0165 - val_loss: 645.1733 - val_custom_mae: 10.5365\n",
      "Epoch 67/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 556.2452 - custom_mae: 9.9695\n",
      "Epoch 00067: val_custom_mae did not improve from 10.42037\n",
      "2500/2500 [==============================] - 260s 104ms/step - loss: 556.3754 - custom_mae: 9.9699 - val_loss: 644.9699 - val_custom_mae: 10.6093\n",
      "Epoch 68/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 551.4666 - custom_mae: 9.9253\n",
      "Epoch 00068: val_custom_mae did not improve from 10.42037\n",
      "2500/2500 [==============================] - 265s 106ms/step - loss: 551.4087 - custom_mae: 9.9254 - val_loss: 637.3724 - val_custom_mae: 10.4419\n",
      "Epoch 69/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 557.1048 - custom_mae: 9.9162\n",
      "Epoch 00069: val_custom_mae improved from 10.42037 to 9.97823, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 271s 108ms/step - loss: 557.4168 - custom_mae: 9.9181 - val_loss: 619.1744 - val_custom_mae: 9.9782\n",
      "Epoch 70/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 554.8393 - custom_mae: 9.8395\n",
      "Epoch 00070: val_custom_mae did not improve from 9.97823\n",
      "2500/2500 [==============================] - 264s 105ms/step - loss: 554.9420 - custom_mae: 9.8398 - val_loss: 615.5787 - val_custom_mae: 10.1333\n",
      "Epoch 71/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 541.6914 - custom_mae: 9.7461 ETA: 0s - loss: 541.7846 - custom_mae: 9.74\n",
      "Epoch 00071: val_custom_mae did not improve from 9.97823\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 541.6245 - custom_mae: 9.7464 - val_loss: 641.4602 - val_custom_mae: 10.3042\n",
      "Epoch 72/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 541.3852 - custom_mae: 9.76 - ETA: 0s - loss: 541.3478 - custom_mae: 9.7643\n",
      "Epoch 00072: val_custom_mae did not improve from 9.97823\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 541.3347 - custom_mae: 9.7642 - val_loss: 656.0709 - val_custom_mae: 10.2998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 542.4243 - custom_mae: 9.69 - ETA: 0s - loss: 542.5411 - custom_mae: 9.6926\n",
      "Epoch 00073: val_custom_mae did not improve from 9.97823\n",
      "2500/2500 [==============================] - 266s 106ms/step - loss: 542.3523 - custom_mae: 9.6911 - val_loss: 635.5489 - val_custom_mae: 10.1774\n",
      "Epoch 74/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 535.5576 - custom_mae: 9.6421 ETA: 0s - loss: 535.7398 - custom_mae: 9.64\n",
      "Epoch 00074: val_custom_mae improved from 9.97823 to 9.77880, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 105ms/step - loss: 535.4032 - custom_mae: 9.6413 - val_loss: 615.2153 - val_custom_mae: 9.7788\n",
      "Epoch 75/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 533.5446 - custom_mae: 9.5756\n",
      "Epoch 00075: val_custom_mae did not improve from 9.77880\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 533.6914 - custom_mae: 9.5765 - val_loss: 614.1455 - val_custom_mae: 9.8985\n",
      "Epoch 76/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 526.5694 - custom_mae: 9.4924\n",
      "Epoch 00076: val_custom_mae did not improve from 9.77880\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 526.5175 - custom_mae: 9.4925 - val_loss: 624.7819 - val_custom_mae: 10.4407\n",
      "Epoch 77/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 528.8815 - custom_mae: 9.5328\n",
      "Epoch 00077: val_custom_mae did not improve from 9.77880\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 528.7839 - custom_mae: 9.5326 - val_loss: 621.9753 - val_custom_mae: 10.0700\n",
      "Epoch 78/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 525.6056 - custom_mae: 9.4671\n",
      "Epoch 00078: val_custom_mae did not improve from 9.77880\n",
      "2500/2500 [==============================] - 262s 105ms/step - loss: 525.4542 - custom_mae: 9.4661 - val_loss: 629.0183 - val_custom_mae: 9.8569\n",
      "Epoch 79/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 528.3815 - custom_mae: 9.4658\n",
      "Epoch 00079: val_custom_mae did not improve from 9.77880\n",
      "2500/2500 [==============================] - 261s 104ms/step - loss: 528.2512 - custom_mae: 9.4651 - val_loss: 620.7140 - val_custom_mae: 10.0493\n",
      "Epoch 80/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 525.1607 - custom_mae: 9.4349 ETA: 1s - loss: 5\n",
      "Epoch 00080: val_custom_mae did not improve from 9.77880\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 525.1037 - custom_mae: 9.4347 - val_loss: 631.6056 - val_custom_mae: 10.0871\n",
      "Epoch 81/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 524.8667 - custom_mae: 9.3544\n",
      "Epoch 00081: val_custom_mae did not improve from 9.77880\n",
      "2500/2500 [==============================] - 260s 104ms/step - loss: 524.9530 - custom_mae: 9.3558 - val_loss: 615.3314 - val_custom_mae: 9.8644\n",
      "Epoch 82/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 526.1049 - custom_mae: 9.3385\n",
      "Epoch 00082: val_custom_mae improved from 9.77880 to 9.72288, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 526.2219 - custom_mae: 9.3394 - val_loss: 616.9545 - val_custom_mae: 9.7229\n",
      "Epoch 83/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 515.4929 - custom_mae: 9.2728\n",
      "Epoch 00083: val_custom_mae did not improve from 9.72288\n",
      "2500/2500 [==============================] - 260s 104ms/step - loss: 516.0679 - custom_mae: 9.2765 - val_loss: 664.0714 - val_custom_mae: 10.0181\n",
      "Epoch 84/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 511.1496 - custom_mae: 9.2074 ETA: 0s - loss: 511.5156 \n",
      "Epoch 00084: val_custom_mae did not improve from 9.72288\n",
      "2500/2500 [==============================] - 259s 104ms/step - loss: 511.0902 - custom_mae: 9.2078 - val_loss: 670.9521 - val_custom_mae: 10.0964\n",
      "Epoch 85/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 510.5471 - custom_mae: 9.2231\n",
      "Epoch 00085: val_custom_mae improved from 9.72288 to 9.51541, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 510.8496 - custom_mae: 9.2241 - val_loss: 606.3428 - val_custom_mae: 9.5154\n",
      "Epoch 86/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 507.9129 - custom_mae: 9.1407 ETA: 0s - loss: 508.0999 - cust\n",
      "Epoch 00086: val_custom_mae did not improve from 9.51541\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 507.8389 - custom_mae: 9.1404 - val_loss: 648.6903 - val_custom_mae: 9.8759\n",
      "Epoch 87/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 506.7102 - custom_mae: 9.1473\n",
      "Epoch 00087: val_custom_mae did not improve from 9.51541\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 506.8653 - custom_mae: 9.1479 - val_loss: 658.6281 - val_custom_mae: 10.1489\n",
      "Epoch 88/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 509.5860 - custom_mae: 9.1292\n",
      "Epoch 00088: val_custom_mae did not improve from 9.51541\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 509.6369 - custom_mae: 9.1301 - val_loss: 650.7893 - val_custom_mae: 10.1015\n",
      "Epoch 89/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 501.7022 - custom_mae: 9.0497\n",
      "Epoch 00089: val_custom_mae improved from 9.51541 to 9.37484, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 501.6747 - custom_mae: 9.0495 - val_loss: 611.6746 - val_custom_mae: 9.3748\n",
      "Epoch 90/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 505.2694 - custom_mae: 9.0877\n",
      "Epoch 00090: val_custom_mae did not improve from 9.37484\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 505.1431 - custom_mae: 9.0870 - val_loss: 611.0493 - val_custom_mae: 9.4306\n",
      "Epoch 91/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 506.7887 - custom_mae: 9.0368 ETA: 0s - loss: 506.8525 - custom_mae\n",
      "Epoch 00091: val_custom_mae did not improve from 9.37484\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 507.0002 - custom_mae: 9.0375 - val_loss: 608.2205 - val_custom_mae: 9.4702\n",
      "Epoch 92/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 497.9320 - custom_mae: 8.9477\n",
      "Epoch 00092: val_custom_mae did not improve from 9.37484\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 497.8198 - custom_mae: 8.9470 - val_loss: 645.6183 - val_custom_mae: 9.8668\n",
      "Epoch 93/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 502.4896 - custom_mae: 8.9693 ETA - ETA: 4s - loss: 502.2 - ETA: 3s - loss: 5 - ETA: 2s - ETA: 0s - loss: 502.8470 - custom_mae - ETA: 0s - loss: 502.4809 - custom_mae: 8.96\n",
      "Epoch 00093: val_custom_mae did not improve from 9.37484\n",
      "2500/2500 [==============================] - 259s 104ms/step - loss: 502.3294 - custom_mae: 8.9683 - val_loss: 591.1245 - val_custom_mae: 9.4108\n",
      "Epoch 94/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 491.8559 - custom_mae: 8.8692\n",
      "Epoch 00094: val_custom_mae did not improve from 9.37484\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 491.7629 - custom_mae: 8.8687 - val_loss: 599.7457 - val_custom_mae: 9.4132\n",
      "Epoch 95/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 488.8580 - custom_mae: 8.8891\n",
      "Epoch 00095: val_custom_mae did not improve from 9.37484\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 489.2162 - custom_mae: 8.8909 - val_loss: 597.9915 - val_custom_mae: 9.5757\n",
      "Epoch 96/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2500 [============================>.] - ETA: 0s - loss: 496.3595 - custom_mae: 8.8600\n",
      "Epoch 00096: val_custom_mae did not improve from 9.37484\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 496.2629 - custom_mae: 8.8597 - val_loss: 612.0865 - val_custom_mae: 9.4244\n",
      "Epoch 97/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 488.9414 - custom_mae: 8.7809\n",
      "Epoch 00097: val_custom_mae improved from 9.37484 to 9.14010, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 488.8062 - custom_mae: 8.7801 - val_loss: 594.6768 - val_custom_mae: 9.1401\n",
      "Epoch 98/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 491.5720 - custom_mae: 8.8141\n",
      "Epoch 00098: val_custom_mae did not improve from 9.14010\n",
      "2500/2500 [==============================] - 258s 103ms/step - loss: 491.4366 - custom_mae: 8.8132 - val_loss: 601.5891 - val_custom_mae: 9.2304\n",
      "Epoch 99/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 484.0790 - custom_mae: 8.7809\n",
      "Epoch 00099: val_custom_mae improved from 9.14010 to 9.02069, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 264s 106ms/step - loss: 483.9159 - custom_mae: 8.7798 - val_loss: 580.2577 - val_custom_mae: 9.0207\n",
      "Epoch 100/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 492.9844 - custom_mae: 8.7227 - ETA: 23s - \n",
      "Epoch 00100: val_custom_mae did not improve from 9.02069\n",
      "2500/2500 [==============================] - 260s 104ms/step - loss: 492.9768 - custom_mae: 8.7221 - val_loss: 577.0620 - val_custom_mae: 9.0287\n",
      "Epoch 101/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 482.3500 - custom_mae: 8.6927 ETA: 0s - loss: 482.9247 - cu\n",
      "Epoch 00101: val_custom_mae did not improve from 9.02069\n",
      "2500/2500 [==============================] - 261s 104ms/step - loss: 482.3416 - custom_mae: 8.6924 - val_loss: 571.3004 - val_custom_mae: 9.1388\n",
      "Epoch 102/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 489.4607 - custom_mae: 8.6930\n",
      "Epoch 00102: val_custom_mae did not improve from 9.02069\n",
      "2500/2500 [==============================] - 261s 104ms/step - loss: 489.5927 - custom_mae: 8.6939 - val_loss: 588.6754 - val_custom_mae: 9.2055\n",
      "Epoch 103/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 481.0818 - custom_mae: 8.6229\n",
      "Epoch 00103: val_custom_mae did not improve from 9.02069\n",
      "2500/2500 [==============================] - 262s 105ms/step - loss: 481.0634 - custom_mae: 8.6231 - val_loss: 574.0635 - val_custom_mae: 9.1418\n",
      "Epoch 104/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 485.6651 - custom_mae: 8.6367\n",
      "Epoch 00104: val_custom_mae did not improve from 9.02069\n",
      "2500/2500 [==============================] - 261s 104ms/step - loss: 485.5861 - custom_mae: 8.6367 - val_loss: 599.8734 - val_custom_mae: 9.4832\n",
      "Epoch 105/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 476.2372 - custom_mae: 8.5840\n",
      "Epoch 00105: val_custom_mae did not improve from 9.02069\n",
      "2500/2500 [==============================] - 261s 104ms/step - loss: 476.0884 - custom_mae: 8.5830 - val_loss: 598.7119 - val_custom_mae: 9.6219\n",
      "Epoch 106/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 478.4113 - custom_mae: 8.5798\n",
      "Epoch 00106: val_custom_mae did not improve from 9.02069\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 478.4918 - custom_mae: 8.5804 - val_loss: 635.5802 - val_custom_mae: 9.8100\n",
      "Epoch 107/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 477.2014 - custom_mae: 8.5375 ETA: 0s - loss: 477.5016 - \n",
      "Epoch 00107: val_custom_mae improved from 9.02069 to 8.87335, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 268s 107ms/step - loss: 477.2457 - custom_mae: 8.5376 - val_loss: 566.3399 - val_custom_mae: 8.8733\n",
      "Epoch 108/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 482.2577 - custom_mae: 8.5415\n",
      "Epoch 00108: val_custom_mae did not improve from 8.87335\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 482.2594 - custom_mae: 8.5414 - val_loss: 561.6231 - val_custom_mae: 9.1681\n",
      "Epoch 109/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 473.2934 - custom_mae: 8.4794\n",
      "Epoch 00109: val_custom_mae did not improve from 8.87335\n",
      "2500/2500 [==============================] - 261s 104ms/step - loss: 473.3356 - custom_mae: 8.4795 - val_loss: 589.4900 - val_custom_mae: 9.0836\n",
      "Epoch 110/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 475.2181 - custom_mae: 8.4629\n",
      "Epoch 00110: val_custom_mae did not improve from 8.87335\n",
      "2500/2500 [==============================] - 262s 105ms/step - loss: 475.1454 - custom_mae: 8.4629 - val_loss: 569.9050 - val_custom_mae: 8.8972\n",
      "Epoch 111/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 473.8530 - custom_mae: 8.4668 ETA: 0s - loss: 474.0994 - custom_mae: 8.\n",
      "Epoch 00111: val_custom_mae did not improve from 8.87335\n",
      "2500/2500 [==============================] - 261s 104ms/step - loss: 473.6972 - custom_mae: 8.4655 - val_loss: 579.3561 - val_custom_mae: 9.0600\n",
      "Epoch 112/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 470.4934 - custom_mae: 8.4390 ETA: 4s - - ETA: 1s - loss: 469.755\n",
      "Epoch 00112: val_custom_mae did not improve from 8.87335\n",
      "2500/2500 [==============================] - 262s 105ms/step - loss: 470.4010 - custom_mae: 8.4383 - val_loss: 600.4864 - val_custom_mae: 9.1678\n",
      "Epoch 113/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 474.7890 - custom_mae: 8.4687\n",
      "Epoch 00113: val_custom_mae improved from 8.87335 to 8.82652, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 267s 107ms/step - loss: 474.6401 - custom_mae: 8.4676 - val_loss: 581.6696 - val_custom_mae: 8.8265\n",
      "Epoch 114/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 472.8064 - custom_mae: 8.3969\n",
      "Epoch 00114: val_custom_mae did not improve from 8.82652\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 472.7884 - custom_mae: 8.3968 - val_loss: 578.0483 - val_custom_mae: 9.0851\n",
      "Epoch 115/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 467.1387 - custom_mae: 8.3762 - ETA: - E - ETA: \n",
      "Epoch 00115: val_custom_mae did not improve from 8.82652\n",
      "2500/2500 [==============================] - 262s 105ms/step - loss: 467.0434 - custom_mae: 8.3758 - val_loss: 584.5627 - val_custom_mae: 8.8301\n",
      "Epoch 116/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 464.9975 - custom_mae: 8.3167\n",
      "Epoch 00116: val_custom_mae did not improve from 8.82652\n",
      "2500/2500 [==============================] - 263s 105ms/step - loss: 464.9626 - custom_mae: 8.3164 - val_loss: 568.3309 - val_custom_mae: 9.0096\n",
      "Epoch 117/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 471.0735 - custom_mae: 8.3339\n",
      "Epoch 00117: val_custom_mae did not improve from 8.82652\n",
      "2500/2500 [==============================] - 262s 105ms/step - loss: 471.1204 - custom_mae: 8.3342 - val_loss: 615.6928 - val_custom_mae: 8.9675\n",
      "Epoch 118/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 460.3077 - custom_mae: 8.2641WARNING:tensorflow:Can save best model only with val_custom_mae available, skipping.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_custom_mae'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m                       total_epochs=1)\n\u001b[0m\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  MemoryError: Unable to allocate 18.4 MiB for an array with shape (32, 224, 224, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 236, in __call__\n    ret = func(*args)\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 789, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\", line 975, in generator_fn\n    yield x[i]\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 222, in _get_batches_of_transformed_samples\n    batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=self.dtype)\n\nMemoryError: Unable to allocate 18.4 MiB for an array with shape (32, 224, 224, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Shape/_6]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  MemoryError: Unable to allocate 18.4 MiB for an array with shape (32, 224, 224, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 236, in __call__\n    ret = func(*args)\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 789, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\", line 975, in generator_fn\n    yield x[i]\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 222, in _get_batches_of_transformed_samples\n    batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=self.dtype)\n\nMemoryError: Unable to allocate 18.4 MiB for an array with shape (32, 224, 224, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_9397]\n\nFunction call stack:\ndistributed_function -> distributed_function\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5a0825006cfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mdisable_progress_bar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mprint_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mclear_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         )\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;31m# start runtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscan_run\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mscan_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\talos\\scan\\scan_run.py\u001b[0m in \u001b[0;36mscan_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# otherwise proceed with next permutation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscan_round\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\talos\\scan\\scan_round.py\u001b[0m in \u001b[0;36mscan_round\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mingest_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\talos\\model\\ingest_model.py\u001b[0m in \u001b[0;36mingest_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                       self.round_params)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-f963f301abeb>\u001b[0m in \u001b[0;36mgrid_model_fine\u001b[1;34m(x, y, x_val, y_val, params)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_reducer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopper\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m     \u001b[0mrow_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2047\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m     \u001b[0mrow_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2047\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_custom_mae'"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    #for top_results_index in range(3):\n",
    "    #for top_results_index in [0, 1]:\n",
    "        top_results_index = 0\n",
    "        _MODEL_TO_LOAD_INDEX = df.iloc[top_results_index].name\n",
    "        _MODEL_TO_LOAD = 'Best_Weights_FC_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        _TMP_DIR = '..\\\\TMP_TALOS_{}'.format(_DEVICE)\n",
    "        _CSV_RESULTS = _LOG_DIR + 'Talos_Results_Fine_Idx{}.csv'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        startTime = datetime.now()\n",
    "        \n",
    "        parameters = get_params(top_results_index)\n",
    "\n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = grid_model_fine,\n",
    "            params = parameters,\n",
    "            experiment_name = _TMP_DIR,\n",
    "            #shuffle=False,\n",
    "            reduction_metric = parameters['reduction_metric'][0],\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = True\n",
    "        )\n",
    "\n",
    "        print(\"Time taken:\", datetime.now() - startTime)\n",
    "        \n",
    "        print('Writing Device File')\n",
    "        device_file.write('Trained Model: {}'.format(_MODEL_TO_LOAD))\n",
    "\n",
    "        df_experiment_results = pd.read_csv(_TMP_DIR + '\\\\' + os.listdir(_TMP_DIR)[0])\n",
    "        df_experiment_results['Base'] = None\n",
    "        for i in range(df_experiment_results.shape[0]):\n",
    "            df_experiment_results['Base'][i] = _MODEL_TO_LOAD_INDEX\n",
    "\n",
    "        if os.path.isfile(_CSV_RESULTS):\n",
    "            df_experiment_results.to_csv(_CSV_RESULTS, mode = 'a', index = False, header = False)\n",
    "        else:\n",
    "            df_experiment_results.to_csv(_CSV_RESULTS, index = False)\n",
    "\n",
    "        shutil.rmtree(_TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Results to NAS if SSD Directory was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_directory(src, dst, symlinks = False, ignore = None):\n",
    "    maxLen = 0\n",
    "    message = ''        \n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        \n",
    "        message = 'Creating Path: {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        os.makedirs(dst)\n",
    "        \n",
    "    for item in os.listdir(src):\n",
    "        \n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        \n",
    "        if os.path.isdir(s):\n",
    "            \n",
    "            message = 'Copying Directory: {}'.format(s)\n",
    "            maxLen = max(maxLen, len(message))\n",
    "            print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "            \n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if not os.path.exists(d): #or os.stat(s).st_mtime - os.stat(d).st_mtime > 1:\n",
    "                \n",
    "                message = 'Copying File: {}'.format(s)\n",
    "                maxLen = max(maxLen, len(message))\n",
    "                print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "                \n",
    "                shutil.copy2(s, d)\n",
    "        \n",
    "        time.sleep(.5)\n",
    "     \n",
    "    message = 'Coyping... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "\n",
    "def delete_directory(src, terminator = '\\n'):\n",
    "    message = ''\n",
    "    maxLen = 0\n",
    "    \n",
    "    try:\n",
    "        message = 'Deleting {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        shutil.rmtree(src)\n",
    "        \n",
    "    except OSError as e:\n",
    "        message = 'Error: {} : {}'.format(src, e.strerror)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "        return\n",
    "    \n",
    "    message = 'Deleting... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = terminator)\n",
    "\n",
    "    \n",
    "def copy_fine_training(src, dst):\n",
    "    copy_directory(src, dst)\n",
    "    delete_directory(src, terminator = '\\r')\n",
    "    delete_directory(src + '..\\\\', terminator = '\\r')\n",
    "    if not os.listdir(src + '..\\\\..\\\\'):\n",
    "        delete_directory(src + '..\\\\..\\\\', terminator = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(storage == OutputDirectory.SSD):\n",
    "    _COPY_DIR = '..\\\\output\\\\{}'.format(_NET_DIR)\n",
    "    copy_fine_training(_LOG_DIR, _COPY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name = \"CMSE.Mixed\"></a><a href = #Top>Up</a></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
