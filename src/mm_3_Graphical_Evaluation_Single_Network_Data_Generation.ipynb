{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation<a name = \"Top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links\n",
    "\n",
    "<ol>\n",
    "    <li><a href = #setup>Setup</a></li>\n",
    "    <li><a href = #store>Save File</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from keras.layers.advanced_activations import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "import ntpath\n",
    "\n",
    "import copy\n",
    "import re\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum f端r Trainingsset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSet(Enum):\n",
    "    SYNTHETIC = 1\n",
    "    REAL = 2\n",
    "    MIXED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum f端r Label-Typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelType(Enum):\n",
    "    ANGULAR = 1\n",
    "    STEREOGRAPHIC = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsset-Typ nach String Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_to_string(ts):\n",
    "    if ts == TrainingSet.SYNTHETIC:\n",
    "        return 'Synth'\n",
    "    elif ts == TrainingSet.REAL:\n",
    "        return 'Real'\n",
    "    elif ts == TrainingSet.MIXED:\n",
    "        return 'Mixed'\n",
    "    else:\n",
    "        print('Unknown TrainingSet')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelType nach String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeltype_to_string(lt):\n",
    "    if lt == LabelType.ANGULAR:\n",
    "        return 'Angular'\n",
    "    if lt == LabelType.STEREOGRAPHIC:\n",
    "        return 'Stereographic'\n",
    "    else:\n",
    "        print('Unknown LabelType')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype = 'float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype = 'float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenpipeline f端r Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data(mode, label_type):\n",
    "    \n",
    "    if mode == TrainingSet.SYNTHETIC:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac=1, random_state = 1)\n",
    "        df_test = df_shuffled[df_shuffled.shape[0] - 10000 : df_shuffled.shape[0]]\n",
    "        \n",
    "    elif mode == TrainingSet.REAL:\n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_test_real = df_shuffled_real[df_shuffled_real.shape[0] - 61 : df_shuffled_real.shape[0]]\n",
    "        df_test = df_test_real\n",
    "        \n",
    "    elif mode == TrainingSet.MIXED:\n",
    "        print('Inferenz auf gemischten Testdaten nicht unbedingt sinnvoll')\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_test = df_shuffled[df_shuffled.shape[0] - 10000 : df_shuffled.shape[0]]\n",
    "        \n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_test_real = df_shuffled_real[df_shuffled_real.shape[0] - 61 : df_shuffled_real.shape[0]]\n",
    "        df_test = df_test.drop(df_test.index[0 : 61])\n",
    "        df_test = df_test.append(df_test_real)        \n",
    "        \n",
    "    else:\n",
    "        print('Invalider Modus :: setup_data(>mode<)')\n",
    "\n",
    "    test_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "\n",
    "    test_generator = test_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_test,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        shuffle = False,\n",
    "        batch_size = 1\n",
    "    )\n",
    "    \n",
    "    return test_generator, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell f端r Inferenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(parameters, prediction_runs = 1):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    test_generator, df_test = setup_data(parameters['dataset_to_use'], parameters['label_type'])\n",
    "\n",
    "    model = Sequential()\n",
    "    cnn = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "\n",
    "    for layer in cnn.layers[:15]:\n",
    "        layer.trainable = False\n",
    "        #print(layer.name, layer.trainable)\n",
    "    \n",
    "    model.add(cnn)\n",
    "    \n",
    "    fc = Sequential()\n",
    "    fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512)\n",
    "    \n",
    "    if parameters['activation_function'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = parameters['leaky_ReLU_alpha'])\n",
    "    elif parameters['activation_function'] == 'relu':\n",
    "        activation_layer = ReLU()  \n",
    "\n",
    "    fc.add(Dense(units = parameters['first_neuron'], kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.add(activation_layer)\n",
    "    if parameters['dropout_rate'] > 0.0:\n",
    "        fc.add(Dropout(rate = parameters['dropout_rate']))\n",
    "\n",
    "    hidden_neuron_fraction = parameters['first_neuron']\n",
    "    for i in range(parameters['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.add(activation_layer)\n",
    "        if parameters['dropout_rate'] > 0.0:\n",
    "            fc.add(Dropout(rate = parameters['dropout_rate']))\n",
    "\n",
    "    fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    \n",
    "    model.add(fc)\n",
    "    model.load_weights(parameters['model_to_load'])\n",
    "\n",
    "    print('Using Optimizer: {} with Learning Rate: {}'.format(parameters['optimizer'][0], parameters['learning_rate']))\n",
    "    model.compile(\n",
    "        optimizer = parameters['optimizer'][0](lr = parameters['learning_rate']),\n",
    "        loss = parameters['loss_function'],\n",
    "        metrics = get_Reduction_Metric(parameters['reduction_metric'])\n",
    "    )\n",
    "\n",
    "    test_generator.reset()\n",
    "\n",
    "    print(\"Predicting using these values:\\nTest Data: {}\\nUsing Loss: {} on Dataset: {}\".format(parameters['dataset_to_use'], parameters['loss_function'], parameters['dataset_name']))\n",
    "    print('LabelType: {} Using Metric: {}'.format(parameters['label_type'], parameters['reduction_metric']))\n",
    "    \n",
    "    duration = 0    \n",
    "    \n",
    "    for n in range(prediction_runs):\n",
    "        startTime = datetime.now()\n",
    "        #predictions = model.predict_generator(generator = test_generator, steps = test_generator.n // test_generator.batch_size, verbose = 0)\n",
    "        predictions = model.predict(x = test_generator, steps = test_generator.n // test_generator.batch_size, verbose = 0)\n",
    "        duration = (datetime.now() - startTime).total_seconds()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    time_per_prediction = duration / prediction_runs\n",
    "    time_per_image = time_per_prediction / len(predictions)\n",
    "    print(\"Prediction repeated {} times at a total time of {}sec. \\nAverage Time per Prediction: {} sec. Average Time per Image: {} sec\".format(prediction_runs, duration, time_per_prediction, time_per_image))\n",
    "\n",
    "    del model\n",
    "    return predictions, df_test, duration, time_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(dataframe, training_set, model_to_load, dataset, label_type = None, loss_function = None, reduction_metric = None, monitor_value = None):\n",
    "    params = copy.deepcopy(p)\n",
    "    params['dataset_to_use'] = training_set\n",
    "    params['model_to_load'] = model_to_load\n",
    "    params['dataset_name']: dataset\n",
    "    # -------------------------------------------------------------\n",
    "    params['optimizer'] = [make_optimizer(dataframe.loc[0]['optimizer'])]\n",
    "    params['learning_rate'] = dataframe.iloc[0]['lr']\n",
    "    params['first_neuron'] = dataframe.iloc[0]['first_neuron']\n",
    "    params['dropout_rate'] = dataframe.iloc[0]['dropout']\n",
    "    params['activation_function'] = dataframe.iloc[0]['activation']\n",
    "    params['leaky_ReLU_alpha'] = dataframe.iloc[0]['leaky_alpha']\n",
    "    params['hidden_layers'] = dataframe.iloc[0]['hidden_layers']\n",
    "    # -------------------------------------------------------------\n",
    "    try:\n",
    "        params['label_type'] = dataframe.iloc[0]['label_type']\n",
    "    except:\n",
    "        print('label_type not available in dataframe, using: ', label_type)\n",
    "        params['label_type'] = label_type\n",
    "    try:\n",
    "        params['loss_function'] = dataframe.iloc[0]['loss_function']\n",
    "    except:\n",
    "        print('loss_function not available in dataframe, using: ', loss_function)\n",
    "        params['loss_function'] = loss_function\n",
    "    try:\n",
    "        params['reduction_metric'] = dataframe.iloc[0]['reduction_metric']\n",
    "    except:\n",
    "        print('reduction_metric not available in dataframe, using: ', reduction_metric)\n",
    "        params['reduction_metric'] = reduction_metric\n",
    "    try:\n",
    "        params['monitor_value'] = dataframe.iloc[0]['monitor_value']\n",
    "    except:\n",
    "        print('monitor_value not available in dataframe, using: ', monitor_value)\n",
    "        params['monitor_value'] = monitor_value\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertierung ($S_x$, $S_y$) $\\rightarrow$ ($\\phi$, $\\theta$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_stereographic(sx, sy, r = 1):\n",
    "    \n",
    "    l = np.sqrt(sx * sx + sy * sy)\n",
    "    theta = 90 - 2 * np.degrees(np.arctan(l / 2))\n",
    "    \n",
    "    if sx < 0:\n",
    "        phi = 180 - np.degrees(np.arcsin(sy / l))\n",
    "        \n",
    "    elif sx >= 0:\n",
    "        if sy > 0:\n",
    "            phi = np.degrees(np.arcsin(sy / l))\n",
    "        elif sy < 0:\n",
    "            phi = 360 + np.degrees(np.arcsin(sy / l))\n",
    "        else:\n",
    "            #phi1 = np.NaN\n",
    "            phi = 0\n",
    "    else:\n",
    "        print('sx and sy undefined. should not have reached here')\n",
    "\n",
    "    return phi, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertierung ($\\phi$, $\\theta$) $\\rightarrow$ ($S_x$, $S_y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_spheric(phi, theta, r = 1):\n",
    "    m = 2 * r * np.tan(np.radians((90 - theta) / 2))\n",
    "    sy = m * np.sin(np.radians(phi))\n",
    "    sx = m * np.cos(np.radians(phi))\n",
    "    return sx, sy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radians $\\rightarrow$ Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_degree(angle_in_rad):\n",
    "    return angle_in_rad * 180 / np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree $\\rightarrow$ Radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_radians(angle_in_deg):\n",
    "    return angle_in_deg * np.pi / 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sph辰rische $\\rightarrow$ Karthesische Koordinaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spheric_cartesian_polar(phi_d, theta_d):\n",
    "    x = np.sin(np.radians(90.0 - theta_d)) * np.cos(np.radians(phi_d))\n",
    "    y = np.sin(np.radians(90.0 - theta_d)) * np.sin(np.radians(phi_d))\n",
    "    z = np.cos(np.radians(90.0 - theta_d))\n",
    "    return array([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spheric_cartesian_elevation(phi_d, theta_d):\n",
    "    x = np.cos(np.radians(theta_d)) * np.cos(np.radians(phi_d))\n",
    "    y = np.cos(np.radians(theta_d)) * np.sin(np.radians(phi_d))\n",
    "    z = np.sin(np.radians(theta_d))\n",
    "    return array([x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorlength(vector):\n",
    "    return np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculated Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angular_error(deg_e_phi, deg_e_theta):\n",
    "    return np.degrees(np.arccos(np.cos(np.radians(deg_e_phi)) * np.cos(np.radians(deg_e_theta))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalarprodukt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDot(a, b):\n",
    "    dot = 0;\n",
    "    it = np.nditer(a, flags=['f_index'])\n",
    "    for x in it:\n",
    "        dot = dot + (x * b[it.index])\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_angular_error_elevation(predicted_deg_vector, true_deg_vector):    \n",
    "    c_predicted = spheric_cartesian_elevation(predicted_deg_vector[0], predicted_deg_vector[1])\n",
    "    c_true = spheric_cartesian_elevation(true_deg_vector[0], true_deg_vector[1])\n",
    "    \n",
    "    len_prediction = vectorlength(c_predicted)\n",
    "    len_true = vectorlength(c_true)\n",
    "    \n",
    "    cos_angle = np.dot(c_true, c_predicted) / len_prediction / len_true\n",
    "    \n",
    "    return abs(np.degrees(np.arccos(cos_angle)))\n",
    "\n",
    "def dot_angular_error_polar(predicted_deg_vector, true_deg_vector):\n",
    "    c_predicted = spheric_cartesian_polar(predicted_deg_vector[0], predicted_deg_vector[1])\n",
    "    c_true = spheric_cartesian_polar(true_deg_vector[0], true_deg_vector[1])\n",
    "    \n",
    "    len_prediction = vectorlength(c_predicted)\n",
    "    len_true = vectorlength(c_true)\n",
    "    \n",
    "    cos_angle = np.dot(c_true, c_predicted) / len_prediction / len_true\n",
    "    \n",
    "    return abs(np.degrees(np.arccos(cos_angle)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'keras.optimizers.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Nadam'>\":\n",
    "        return Nadam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Adagard'>\":\n",
    "        return Adagard\n",
    "    elif optimizer == \"<class 'keras.optimizers.RMSprop'>\":\n",
    "        return RMSprop\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Label_Type into suitable label names.\n",
    "$\\Rightarrow$ Angular / Normalized $\\rightarrow$ ['Elevation', 'Azimuth']\n",
    "\n",
    "$\\Rightarrow$ Stereographic $\\rightarrow$ ['S_x', 'S_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Label_Names(label_type):\n",
    "    if label_type == 'Angular' or label_type == 'Normalized':\n",
    "        return ['Elevation', 'Azimuth']\n",
    "    elif label_type == 'Stereographic':\n",
    "        return ['S_x', 'S_y']\n",
    "    else:\n",
    "        assert(True, 'LabelType Invalid')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reduction_Metric(metric):\n",
    "    \n",
    "    if metric == 'custom_mae':\n",
    "        return [custom_mae]\n",
    "    elif metric == 'tf.keras.metrics.MeanAbsoluteError()':\n",
    "        return [tf.keras.metrics.MeanAbsoluteError()]\n",
    "    elif metric == 'circular_mae':\n",
    "        return [circular_mae]\n",
    "    elif metric == 'mean_squared_error':\n",
    "        return ['mean_squared_error']\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normierte sph辰rische Koordinaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normierte $\\rightarrow$ Sph辰rische"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_to_spheric(n_phi, n_theta):\n",
    "    phi = n_phi * 180 + 180\n",
    "    theta = n_theta * 45 + 45\n",
    "    \n",
    "    return phi, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_df, eval_predictions, save_dir = None, save_to_file = False):\n",
    "    num_predictions = eval_df.shape[0]\n",
    "    \n",
    "    df_result = pd.DataFrame({\n",
    "        'Filename': eval_df['Filename'][0:num_predictions],\n",
    "        'Elevation_true': None if _STEREOGRAPHIC else eval_df['Elevation'][0:num_predictions],\n",
    "        'Elevation_pred': None,\n",
    "        'Elevation_err': None,\n",
    "        'Azimuth_true': None if _STEREOGRAPHIC else eval_df['Azimuth'][0:num_predictions],\n",
    "        'Azimuth_pred': None,\n",
    "        'Azimuth_err': None,\n",
    "        'Angular_MAE': None,\n",
    "        'dot_angular_err_elevation': None,\n",
    "        'dot_angular_err_polar': None,\n",
    "        'S_x_true': eval_df['S_x'][0:num_predictions] if _STEREOGRAPHIC else None, # 'S_x' replace later\n",
    "        'S_x_pred': None,\n",
    "        'S_y_true': eval_df['S_y'][0:num_predictions] if _STEREOGRAPHIC else None, # 'S_y' replace later\n",
    "        'S_y_pred': None,\n",
    "        'S_x_err': None,\n",
    "        'S_y_err': None\n",
    "    })\n",
    "    \n",
    "    angular_calculated = ''\n",
    "    stereo_calculated = ''\n",
    "    \n",
    "    \n",
    "    if _STEREOGRAPHIC:\n",
    "        angular_calculated = '*'\n",
    "        stereo_calculated = ''\n",
    "        \n",
    "        df_result['S_x_pred'] = eval_predictions[:, 0]\n",
    "        df_result['S_y_pred'] = eval_predictions[:, 1]\n",
    "        \n",
    "        for index, row in df_result.iterrows():\n",
    "            predicted_sx = row['S_x_pred']\n",
    "            predicted_sy = row['S_y_pred']\n",
    "            \n",
    "            true_sx = row['S_x_true']\n",
    "            true_sy = row['S_y_true']\n",
    "            \n",
    "            # stereographische Koordinaten - Fehler\n",
    "            error_sx = abs(predicted_sx - true_sx)\n",
    "            error_sy = abs(predicted_sy - true_sy)\n",
    "            \n",
    "            df_result.at[index, 'S_x_err'] = error_sx\n",
    "            df_result.at[index, 'S_y_err'] = error_sy\n",
    "            \n",
    "            \n",
    "            # Kugelkoordinaten\n",
    "            predicted_azimuth, predicted_elevation = convert_from_stereographic(predicted_sx, predicted_sy)\n",
    "            true_azimuth, true_elevation = convert_from_stereographic(true_sx, true_sy)\n",
    "            \n",
    "            df_result.at[index, 'Elevation_true'] = true_elevation\n",
    "            df_result.at[index, 'Elevation_pred'] = predicted_elevation\n",
    "            df_result.at[index, 'Azimuth_true'] = true_azimuth\n",
    "            df_result.at[index, 'Azimuth_pred'] = predicted_azimuth\n",
    "            \n",
    "            error_elevation = abs(predicted_elevation - true_elevation)\n",
    "            error_azimuth = min(abs(predicted_azimuth - true_azimuth), abs(360 - abs(predicted_azimuth - true_azimuth)))\n",
    "            \n",
    "            df_result.at[index, 'Elevation_err'] = error_elevation\n",
    "            df_result.at[index, 'Azimuth_err'] = error_azimuth\n",
    "            \n",
    "            \n",
    "            # dot_angular_error\n",
    "            predicted_direction = array([predicted_azimuth, predicted_elevation])\n",
    "            true_direction = array([true_azimuth, true_elevation])\n",
    "            \n",
    "            error_direction_elevation = dot_angular_error_elevation(predicted_direction, true_direction)\n",
    "            df_result.at[index, 'dot_angular_err_elevation'] = error_direction_elevation\n",
    "            error_direction_polar = dot_angular_error_polar(predicted_direction, true_direction)\n",
    "            df_result.at[index, 'dot_angular_err_polar'] = error_direction_polar\n",
    "            \n",
    "            \n",
    "            # Angular Mean Absolute Error\n",
    "            y_pred = K.constant(predicted_direction)\n",
    "            y_true = K.constant (true_direction)\n",
    "            mae = custom_mae(y_true, y_pred)\n",
    "            df_result.at[index, 'Angular_MAE'] = mae\n",
    "        \n",
    "    else:\n",
    "        angular_calculated = ''\n",
    "        stereo_calculated = '*'\n",
    "        \n",
    "        df_result['Elevation_pred'] = eval_predictions[:, 0]\n",
    "        df_result['Azimuth_pred'] = eval_predictions[:, 1]\n",
    "        \n",
    "        for index, row in df_result.iterrows():\n",
    "            #print('=======================================================================')\n",
    "            if(current_params['label_type'] == 'Angular'):\n",
    "                predicted_azimuth = row['Azimuth_pred']\n",
    "                predicted_elevation = row['Elevation_pred']\n",
    "                \n",
    "                true_azimuth = row['Azimuth_true']\n",
    "                true_elevation = row['Elevation_true']\n",
    "                \n",
    "            elif(current_params['label_type'] == 'Normalized'):\n",
    "                predicted_azimuth, predicted_elevation = normalized_to_spheric(row['Azimuth_pred'], row['Elevation_pred'])\n",
    "\n",
    "                true_azimuth, true_elevation = normalized_to_spheric(row['Azimuth_true'], row['Elevation_true'])\n",
    "                \n",
    "            \n",
    "            # azimuth and elevation error\n",
    "            error_elevation = abs((predicted_elevation - true_elevation))\n",
    "            df_result.at[index, 'Elevation_err'] = error_elevation\n",
    "\n",
    "            error_azimuth = min(abs(predicted_azimuth - true_azimuth), abs(360 - abs(predicted_azimuth - true_azimuth)))\n",
    "            df_result.at[index, 'Azimuth_err'] = error_azimuth\n",
    "            \n",
    "            \n",
    "            # stereographische Koordinaten - Fehler\n",
    "            sx_t, sy_t = convert_from_spheric(true_azimuth, true_elevation)\n",
    "            sx_p, sy_p = convert_from_spheric(predicted_azimuth, predicted_elevation)\n",
    "            \n",
    "            df_result.at[index, 'S_x_true'] = sx_t\n",
    "            df_result.at[index, 'S_y_true'] = sy_t\n",
    "            df_result.at[index, 'S_x_pred'] = sx_p\n",
    "            df_result.at[index, 'S_y_pred'] = sy_p\n",
    "            \n",
    "            error_sx = abs(sx_p - sx_t)\n",
    "            error_sy = abs(sy_p - sy_t)\n",
    "            \n",
    "            df_result.at[index, 'S_x_err'] = error_sx\n",
    "            df_result.at[index, 'S_y_err'] = error_sy\n",
    "            \n",
    "            \n",
    "            # dot angular error\n",
    "            predicted_direction = array([predicted_azimuth, predicted_elevation])\n",
    "            true_direction = array([true_azimuth, true_elevation])\n",
    "            \n",
    "            error_direction_elevation = dot_angular_error_elevation(predicted_direction, true_direction)\n",
    "            df_result.at[index, 'dot_angular_err_elevation'] = error_direction_elevation\n",
    "            error_direction_polar = dot_angular_error_polar(predicted_direction, true_direction)\n",
    "            df_result.at[index, 'dot_angular_err_polar'] = error_direction_polar\n",
    "            \n",
    "            \n",
    "            # Angular Mean Absolute Error\n",
    "            y_pred = K.constant(predicted_direction)\n",
    "            y_true = K.constant (true_direction)\n",
    "            mae = custom_mae(y_true, y_pred)\n",
    "            df_result.at[index, 'Angular_MAE'] = mae\n",
    "            #print('=======================================================================')\n",
    "    \n",
    "    error_elevation_avg = np.mean(df_result['Elevation_err'], axis = 0)\n",
    "    error_azimuth_avg = np.mean(df_result['Azimuth_err'], axis = 0)\n",
    "    error_sx_avg = np.mean(df_result['S_x_err'], axis = 0)\n",
    "    error_sy_avg = np.mean(df_result['S_y_err'], axis = 0)\n",
    "    \n",
    "    \n",
    "    # Box'n Whiskers Diagram Data - Angular Error\n",
    "    e_angular_median = np.quantile(df_result['dot_angular_err_elevation'], 0.5)\n",
    "    e_angular_lower_q = np.quantile(df_result['dot_angular_err_elevation'], 0.25)\n",
    "    e_angular_upper_q = np.quantile(df_result['dot_angular_err_elevation'], 0.75)\n",
    "    e_angular_min = np.amin(df_result['dot_angular_err_elevation'])\n",
    "    e_angular_max = np.amax(df_result['dot_angular_err_elevation'])\n",
    "    \n",
    "    max_angular_error = df_result.loc[df_result['dot_angular_err_elevation'] == e_angular_max]\n",
    "    max_angular_error_file = max_angular_error.iloc[0]['Filename']\n",
    "    max_angular_error_theta_true = max_angular_error.iloc[0]['Elevation_true']\n",
    "    max_angular_error_phi_true = max_angular_error.iloc[0]['Azimuth_true']\n",
    "    max_angular_error_theta_pred = max_angular_error.iloc[0]['Elevation_pred']\n",
    "    max_angular_error_phi_pred = max_angular_error.iloc[0]['Azimuth_pred']\n",
    "    \n",
    "    \n",
    "    # Box'n Whiskers Diagram Data - Mean Absolute Error\n",
    "    e_mae_median = np.quantile(df_result['Angular_MAE'], 0.5)\n",
    "    e_mae_lower_q = np.quantile(df_result['Angular_MAE'], 0.25)\n",
    "    e_mae_upper_q = np.quantile(df_result['Angular_MAE'], 0.75)\n",
    "    e_mae_min = np.amin(df_result['Angular_MAE'])\n",
    "    e_mae_max = np.amax(df_result['Angular_MAE'])\n",
    "    \n",
    "    max_mae_error = df_result.loc[df_result['Angular_MAE'] == e_mae_max]\n",
    "    max_mae_error_file = max_mae_error.iloc[0]['Filename']\n",
    "    max_mae_error_theta_true = max_mae_error.iloc[0]['Elevation_true']\n",
    "    max_mae_error_phi_true = max_mae_error.iloc[0]['Azimuth_true']\n",
    "    max_mae_error_theta_pred = max_mae_error.iloc[0]['Elevation_pred']\n",
    "    max_mae_error_phi_pred = max_mae_error.iloc[0]['Azimuth_pred']\n",
    "    \n",
    "    # Avg Angular Error\n",
    "    error_dot_angular_elevation_avg = np.mean(df_result['dot_angular_err_elevation'], axis = 0)\n",
    "    angular_variance = np.var(df_result['dot_angular_err_elevation'], axis = 0)\n",
    "    avg_angular_lower_mean = np.mean(df_result['dot_angular_err_elevation'][df_result['dot_angular_err_elevation'] < error_dot_angular_elevation_avg])\n",
    "    avg_angular_upper_mean = np.mean(df_result['dot_angular_err_elevation'][df_result['dot_angular_err_elevation'] > error_dot_angular_elevation_avg])\n",
    "    \n",
    "    # Avg MAE Error\n",
    "    avg_mae_mean = np.mean(df_result['Angular_MAE'])\n",
    "    avg_mae_lower_mean = np.mean(df_result['Angular_MAE'][df_result['Angular_MAE'] < avg_mae_mean])\n",
    "    avg_mae_upper_mean = np.mean(df_result['Angular_MAE'][df_result['Angular_MAE'] > avg_mae_mean])\n",
    "    \n",
    "\n",
    "    print('Durchschnnittlicher Fehler Elevation{}: {:.1f}'.format(angular_calculated, error_elevation_avg))\n",
    "    print('Durchschnnittlicher Fehler Azimut{}: {:.1f}'.format(angular_calculated, error_azimuth_avg))\n",
    "    print('Durchschnnittlicher Fehler S_x{}: {:.2f}'.format(stereo_calculated, error_sx_avg))\n",
    "    print('Durchschnnittlicher Fehler S_y{}: {:.2f}'.format(stereo_calculated, error_sy_avg))\n",
    "    print()\n",
    "    print('Durchschnnittlicher Winkelfehler{} (Elevation): {:.1f}'.format(angular_calculated, error_dot_angular_elevation_avg))\n",
    "    print('Varianz Winkelfehler{}: {:.1f}'.format(angular_calculated, angular_variance))\n",
    "    print()\n",
    "    print('Box-Whisker Angular: max - {:.1f}, upper - {:.1f}, median - {:.1f}, lower - {:.1f}, min - {:.1f}'.format(e_angular_max, e_angular_upper_q, e_angular_median, e_angular_lower_q, e_angular_min))\n",
    "    print('Maxium Angular Error on: {}, True ({:.1f}, {:.1f}), Pred ({:.1f}, {:.1f})'.format(max_angular_error_file, max_angular_error_phi_true, max_angular_error_theta_true, max_angular_error_phi_pred, max_angular_error_theta_pred))\n",
    "    print('Box-Whisker MAE: max - {:.1f}, upper - {:.1f}, median - {:.1f}, lower - {:.1f}, min - {:.1f}'.format(e_mae_max, e_mae_upper_q, e_mae_median, e_mae_lower_q, e_mae_min))\n",
    "    print('Maxium MAE Error on: {}, True ({:.1f}, {:.1f}), Pred ({:.1f}, {:.1f})'.format(max_mae_error_file, max_mae_error_phi_true, max_mae_error_theta_true, max_mae_error_phi_pred, max_mae_error_theta_pred))\n",
    "    print()\n",
    "    print('Avg Angular Range: lower - {:.1f}, avg - {:.1f}, upper - {:.1f}'.format(avg_angular_lower_mean, error_dot_angular_elevation_avg, avg_angular_upper_mean))\n",
    "    print('Avg MAE Range: lower - {:.1f}, avg - {:.1f}, upper - {:.1f}'.format(avg_mae_lower_mean, avg_mae_mean, avg_mae_upper_mean))\n",
    "    \n",
    "        \n",
    "    df_avg = pd.DataFrame({\n",
    "        'Avg_Elevation_Err': [error_elevation_avg],\n",
    "        'Avg_Azimuth_Err': [error_azimuth_avg],\n",
    "        'Avg_Angular_Err': [error_dot_angular_elevation_avg],\n",
    "        'Angular_Variance': [angular_variance],\n",
    "        'Avg_Sx_Err': [error_sx_avg],\n",
    "        'Avg_Sy_Err': [error_sy_avg],\n",
    "        'Avg_Inference_Time': [image_time],\n",
    "        'box_angular_medium': [e_angular_median],\n",
    "        'box_angular_lower': [e_angular_lower_q],\n",
    "        'box_angular_upper': [e_angular_upper_q],\n",
    "        'box_angular_min': [e_angular_min],\n",
    "        'box_angular_max': [e_angular_max],\n",
    "        'box_mae_medium': [e_mae_median],\n",
    "        'box_mae_lower': [e_mae_lower_q],\n",
    "        'box_mae_upper': [e_mae_upper_q],\n",
    "        'box_mae_min': [e_mae_min],\n",
    "        'box_mae_max': [e_mae_max],\n",
    "        'avg_angular_lower_mean': [avg_angular_lower_mean],\n",
    "        'avg_angular_upper_mean': [avg_angular_upper_mean],\n",
    "        'avg_mae_mean': [avg_mae_mean],\n",
    "        'avg_mae_lower_mean': [avg_mae_lower_mean],\n",
    "        'avg_mae_lower_mean': [avg_mae_upper_mean],\n",
    "        'max_angular_error_file': [max_angular_error_file],\n",
    "        'max_angular_error_theta_true': [max_angular_error_theta_true],\n",
    "        'max_angular_error_phi_true': [max_angular_error_phi_true],\n",
    "        'max_angular_error_theta_pred': [max_angular_error_theta_pred],\n",
    "        'max_angular_error_phi_pred': [max_angular_error_phi_pred],\n",
    "        'max_mae_error_file': [max_mae_error_file],\n",
    "        'max_mae_error_theta_true': [max_mae_error_theta_true],\n",
    "        'max_mae_error_phi_true': [max_mae_error_phi_true],\n",
    "        'max_mae_error_theta_pred': [max_mae_error_theta_pred],\n",
    "        'max_mae_error_phi_pred': [max_mae_error_phi_pred]\n",
    "    })\n",
    "\n",
    "    if save_to_file:\n",
    "        df_result.to_csv(save_dir + 'Prognosen_ErrE_{}_ErrA_{}.csv'.format('%.2f'%error_elevation_avg, '%.2f'%error_azimuth_avg), index=False)\n",
    "    \n",
    "    model_path, current_model = ntpath.split(model)\n",
    "    evaluation_path = model_path + '\\\\Evaluation\\\\'\n",
    "    if(not os.path.exists(evaluation_path)):\n",
    "        os.makedirs(evaluation_path)\n",
    "    df_result.to_csv(evaluation_path + 'Model-{}_Testset-{}_Prediction_Results.csv'.format(net_index, trainingset_to_string(test_set)), index=False)\n",
    "    df_avg.to_csv(evaluation_path + 'Model-{}_Testset-{}_Average_Results.csv'.format(net_index, trainingset_to_string(test_set)), index=False)\n",
    "    \n",
    "    return df_result, df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model <a name = \"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Setup\n",
    "<p><a href = #Top>Top</a> \n",
    "<p><a href = #store>Save File</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required format of parameters parameter for _model_predict_ (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'dataset_to_use':[],\n",
    "    'model_to_load':[],\n",
    "    'dataset_name':'combined_dataset',\n",
    "    # --------------------------------\n",
    "    'optimizer':[],\n",
    "    'learning_rate':[],\n",
    "    'first_neuron':[],\n",
    "    'dropout_rate':[],\n",
    "    'activation_function':[],\n",
    "    'leaky_ReLU_alpha':[],\n",
    "    'hidden_layers':[],\n",
    "    # --------------------------------\n",
    "    'label_type':['Angular'],\n",
    "    'loss_function':[],\n",
    "    'reduction_metric':[],\n",
    "    'monitor_value':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run = 'SYNTH'\n",
    "loss = 'MSE'\n",
    "dataset_name = '2020-05-28' # -no_shadow\n",
    "net_index = 31\n",
    "synth_samples = 80000\n",
    "real_samples = 640\n",
    "mixed_samples = 80000\n",
    "talos_results = 'Talos_Results_Fine_Idx{}.csv'.format(net_index)\n",
    "\n",
    "#APPENDIX = 'Stereographic'\n",
    "\n",
    "_note = '_Custom-MAE' # _Custom-MAE, _cmae-base, _Verification\n",
    "\n",
    "#FUNCTION_OVERRIDE = ['mean_squared_error', [custom_mae], 'val_custom_mae'] # e. g. ['mean_squared_error', [circular_mae], 'val_circular_mae']\n",
    "\n",
    "\n",
    "if p['label_type'][0] == 'Stereographic':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_stereographic.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_stereographic.csv'\n",
    "    _STEREOGRAPHIC = True\n",
    "    #label_type = LabelType.STEREOGRAPHIC\n",
    "elif p['label_type'][0] == 'Angular':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real.csv'\n",
    "    _STEREOGRAPHIC = False\n",
    "    #label_type = LabelType.ANGULAR\n",
    "elif p['label_type'][0] == 'Normalized':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_normalized.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_normalized.csv'\n",
    "    _STEREOGRAPHIC = False\n",
    "    #label_type = LabelType.ANGULAR\n",
    "else:\n",
    "    assert(True, 'Appendix Invalid')\n",
    "\n",
    "\n",
    "network_path = '..\\\\output\\\\{}_Regression_{}\\\\{}_{}_Top_1{}\\\\{}\\\\'\n",
    "    \n",
    "syn_trained_model_dir = network_path.format(run, loss, dataset_name, p['label_type'][0], _note, 'Synth_TD')\n",
    "real_trained_model_dir = network_path.format(run, loss, dataset_name, p['label_type'][0], _note, 'Real_TD')\n",
    "mixed_trained_model_dir = network_path.format(run, loss, dataset_name, p['label_type'][0], _note, 'Mixed_TD')\n",
    "\n",
    "p['dataset_name'] = dataset_name\n",
    "_IMAGE_DIR = '..\\\\dataset\\\\{}\\\\'.format(p['dataset_name'])\n",
    "\n",
    "_CSV_FILE = _IMAGE_DIR + _CSV_SYNTH_FILE_NAME\n",
    "_CSV_FILE_REAL = _IMAGE_DIR + _CSV_REAL_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TrainingSet.SYNTHETIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Trainiert mit: Synthetische Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = syn_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_type = None\n",
    "loss_function = None\n",
    "reduction_metric = None\n",
    "monitor_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = syn_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, synth_samples)\n",
    "current_params = load_params(dataframe, test_set, model, dataset_name, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainiert mit: Reale Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = real_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = real_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, real_samples)\n",
    "current_params = load_params(dataframe, test_set, model, dataset_name, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainiert mit: Gemischte Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = mixed_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mixed_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, mixed_samples)\n",
    "current_params = load_params(dataframe, test_set, model, dataset_name, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reale Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TrainingSet.REAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainiert mit: Synthetische Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = syn_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = syn_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, synth_samples)\n",
    "current_params = load_params(dataframe, test_set, model, dataset_name, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainiert mit: Reale Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = real_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = real_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, real_samples)\n",
    "current_params = load_params(dataframe, test_set, model, dataset_name, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Traininert mit: Gemischte Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = mixed_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mixed_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, mixed_samples)\n",
    "current_params = load_params(dataframe, test_set, model, dataset_name, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation <a name = \"store\">\n",
    "<p></a><a href = #Top>Top</a>\n",
    "<p><a href = #setup>Setup</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = '..\\\\output\\\\{}_Regression_{}\\\\Graphical_Evaluation\\\\'.format(run, loss)\n",
    "\n",
    "if(not os.path.exists(eval_dir)):\n",
    "    os.makedirs(eval_dir)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(eval_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(eval_dir + '{}_Net{}_{}{}_Results.pickle'.format(dataset_name, net_index, p['label_type'][0], _note), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mse_results, fp) # uncomment if you are REALLY sure to overwrite the file\n",
    "\n",
    "with open(eval_dir + '{}_Net{}_{}{}_Results.pickle'.format(dataset_name, net_index, p['label_type'][0], _note), \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_test]",
   "language": "python",
   "name": "conda-env-tf_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
