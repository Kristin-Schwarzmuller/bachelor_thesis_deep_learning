{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kristins Alex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  2 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 242201275468890859\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9105744200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8181197128574531938\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9104897474\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7537298592001109504\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\"\n",
      "]\n",
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data_pipline(params['batch_size'], params['samples'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = params['leaky_alpha'])\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(224,224,Global.num_image_channels)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(params['dropout']),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(params['dropout']),\n",
    "        Dense(units = 2, activation=activation_layer)\n",
    "        #Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer'])), \n",
    "        loss = Global.loss_function, \n",
    "        metrics = get_reduction_metric(Global.reduction_metric)\n",
    "    )\n",
    "\n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    print('Model was compiled')\n",
    "    print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = Global.target_dir + 'CNN_ALEX_Model_and_Weights_{}.hdf5'.format(Global.image_channels),\n",
    "        monitor =  Global.monitor_value,\n",
    "        verbose = 1,\n",
    "        save_weights_only = False,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    print('Checkpointer was created')\n",
    "    \n",
    "    csv_logger = callbacks.CSVLogger(\n",
    "        filename = Global.target_dir + 'CNN_ALEX_Logger_{}.csv'.format(Global.image_channels),\n",
    "        separator = ',',\n",
    "        append = False\n",
    "    )\n",
    "    print('CSV Logger was created')\n",
    "\n",
    "    lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 13,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        min_delta = 0.0001\n",
    "    )\n",
    "    print('Learning Rate Reducer was created')\n",
    "    \n",
    "    early_stopper = callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0,\n",
    "        #patience = 15,\n",
    "        patience = 20,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    print('Early Stopper was created')\n",
    "    \n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        epochs = params['epochs'],\n",
    "        validation_data = valid_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        callbacks = [checkpointer, csv_logger, lr_reducer, early_stopper]\n",
    "        #workers = 8\n",
    "    )\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolut_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduction_metric(metric):\n",
    "    \n",
    "    if metric == 'mean_absolut_error':\n",
    "        return [mean_absolut_error]\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct for global parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class global_parameter:\n",
    "    loss_function: str = 'mean_squared_error'\n",
    "    reduction_metric: str = 'mean_absolut_error'\n",
    "    monitor_value: str = 'val_mean_absolut_error'\n",
    "    \n",
    "    dataset: str = '201019_2253_final'\n",
    "    device: str = 'RTX_2080_Ti'\n",
    "    data_augmentation: bool = True\n",
    "    image_channels: str = 'rgb' # jsut change this, everything else will automaticlly adjusted\n",
    "    num_image_channels: int = 3\n",
    "    image_dir: str = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(dataset)\n",
    "    \n",
    "    csv_file_name: str = 'labels_ks_RGB.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}\\\\'.format(dataset, image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(dataset, image_channels)\n",
    "    results_man: str = '\\\\..\\\\201019_2253_final_rgb_Results_manual.csv'\n",
    "\n",
    "        \n",
    "Global = global_parameter\n",
    "\n",
    "if(Global.image_channels == 'rgba'):\n",
    "    Global.num_image_channels = 4\n",
    "    csv_file_name: str = 'lables_ks_RGBD.csv'\n",
    "    csv_file: str = image_dir + csv_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_pipline(batch_size, num_samples):\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(Global.csv_file)\n",
    "    df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "    df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "    df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "    #print(df_valid)\n",
    "    \n",
    "    if Global.data_augmentation:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.5, 1.0), \n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    #print(df_train)\n",
    "        \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    #print(df_train)\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\output\\201019_2253_final_rgb\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n"
     ]
    }
   ],
   "source": [
    "if(not os.path.exists(Global.target_dir)):\n",
    "    os.makedirs(Global.target_dir)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(Global.target_dir))\n",
    "\n",
    "device_file = open(Global.target_dir + '{}.txt'.format(Global.device), \"a+\")\n",
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: ..\\output\\201019_2253_final_rgb\\\\..\\201019_2253_final_rgb_Results_manual.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_absolut_error</th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolut_error</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>leaky_alpha</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>10</td>\n",
       "      <td>11/10/20-141428</td>\n",
       "      <td>11/10/20-141525</td>\n",
       "      <td>57.297635</td>\n",
       "      <td>4108.879395</td>\n",
       "      <td>41.778568</td>\n",
       "      <td>6010.239789</td>\n",
       "      <td>53.150177</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>23</td>\n",
       "      <td>11/09/20-160436</td>\n",
       "      <td>11/09/20-160539</td>\n",
       "      <td>63.461800</td>\n",
       "      <td>4651.241211</td>\n",
       "      <td>42.399044</td>\n",
       "      <td>6957.538744</td>\n",
       "      <td>55.006474</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>11/09/20-152438</td>\n",
       "      <td>11/09/20-152541</td>\n",
       "      <td>62.956922</td>\n",
       "      <td>4807.504883</td>\n",
       "      <td>42.973434</td>\n",
       "      <td>6722.161218</td>\n",
       "      <td>54.768200</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2</td>\n",
       "      <td>11/10/20-124118</td>\n",
       "      <td>11/10/20-124213</td>\n",
       "      <td>54.611791</td>\n",
       "      <td>4609.726074</td>\n",
       "      <td>43.298336</td>\n",
       "      <td>6051.426510</td>\n",
       "      <td>52.527298</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11/09/20-150631</td>\n",
       "      <td>11/09/20-150724</td>\n",
       "      <td>53.074127</td>\n",
       "      <td>4551.695312</td>\n",
       "      <td>44.134876</td>\n",
       "      <td>6316.580711</td>\n",
       "      <td>54.332478</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>5</td>\n",
       "      <td>11/09/20-181241</td>\n",
       "      <td>11/09/20-181336</td>\n",
       "      <td>55.279167</td>\n",
       "      <td>5065.450195</td>\n",
       "      <td>44.154381</td>\n",
       "      <td>7182.103573</td>\n",
       "      <td>57.703896</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>11/09/20-151007</td>\n",
       "      <td>11/09/20-151101</td>\n",
       "      <td>54.582515</td>\n",
       "      <td>4447.951660</td>\n",
       "      <td>44.235825</td>\n",
       "      <td>5920.551208</td>\n",
       "      <td>52.696514</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>8</td>\n",
       "      <td>11/10/20-124653</td>\n",
       "      <td>11/10/20-124751</td>\n",
       "      <td>58.295156</td>\n",
       "      <td>5182.400879</td>\n",
       "      <td>45.300297</td>\n",
       "      <td>6063.520243</td>\n",
       "      <td>53.520336</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>11/09/20-172013</td>\n",
       "      <td>11/09/20-172107</td>\n",
       "      <td>54.318614</td>\n",
       "      <td>4049.460205</td>\n",
       "      <td>45.346073</td>\n",
       "      <td>5809.329929</td>\n",
       "      <td>52.325111</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2</td>\n",
       "      <td>11/10/20-133846</td>\n",
       "      <td>11/10/20-133941</td>\n",
       "      <td>54.370764</td>\n",
       "      <td>5233.627930</td>\n",
       "      <td>45.512722</td>\n",
       "      <td>6016.750846</td>\n",
       "      <td>52.590275</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0            start              end   duration     val_loss  \\\n",
       "298          10  11/10/20-141428  11/10/20-141525  57.297635  4108.879395   \n",
       "55           23  11/09/20-160436  11/09/20-160539  63.461800  4651.241211   \n",
       "21           21  11/09/20-152438  11/09/20-152541  62.956922  4807.504883   \n",
       "258           2  11/10/20-124118  11/10/20-124213  54.611791  4609.726074   \n",
       "2             2  11/09/20-150631  11/09/20-150724  53.074127  4551.695312   \n",
       "133           5  11/09/20-181241  11/09/20-181336  55.279167  5065.450195   \n",
       "6             6  11/09/20-151007  11/09/20-151101  54.582515  4447.951660   \n",
       "264           8  11/10/20-124653  11/10/20-124751  58.295156  5182.400879   \n",
       "96            0  11/09/20-172013  11/09/20-172107  54.318614  4049.460205   \n",
       "274           2  11/10/20-133846  11/10/20-133941  54.370764  5233.627930   \n",
       "\n",
       "     val_mean_absolut_error         loss  mean_absolut_error activation  \\\n",
       "298               41.778568  6010.239789           53.150177       relu   \n",
       "55                42.399044  6957.538744           55.006474       relu   \n",
       "21                42.973434  6722.161218           54.768200       relu   \n",
       "258               43.298336  6051.426510           52.527298  leakyrelu   \n",
       "2                 44.134876  6316.580711           54.332478  leakyrelu   \n",
       "133               44.154381  7182.103573           57.703896  leakyrelu   \n",
       "6                 44.235825  5920.551208           52.696514  leakyrelu   \n",
       "264               45.300297  6063.520243           53.520336       relu   \n",
       "96                45.346073  5809.329929           52.325111  leakyrelu   \n",
       "274               45.512722  6016.750846           52.590275  leakyrelu   \n",
       "\n",
       "     batch_size  dropout  first_neuron  hidden_layers  leaky_alpha  lr  \\\n",
       "298          32     0.50          4096              3          0.1   1   \n",
       "55           32     0.50          2048              1          0.1   2   \n",
       "21           32     0.50          1024              0          0.1   2   \n",
       "258          32     0.50          4096              1          0.1   1   \n",
       "2            32     0.25          2048              0          0.1   1   \n",
       "133          32     0.50          1024              4          0.1   2   \n",
       "6            32     0.50          2048              0          0.1   1   \n",
       "264          32     0.25          4096              1          0.1   1   \n",
       "96           32     0.25          1024              3          0.1   1   \n",
       "274          32     0.50          4096              2          0.1   1   \n",
       "\n",
       "                                             optimizer  \n",
       "298  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "55   <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "21   <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "258  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "2    <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "133  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "6    <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "264  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "96   <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "274  <class 'tensorflow.python.keras.optimizer_v2.a...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_results = Global.target_dir + Global.results_man\n",
    "df = pd.read_csv(base_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "sort_value = 'val_mean_absolut_error'\n",
    "df = df.sort_values(sort_value, axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(base_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "global_hyper_parameter = {\n",
    "    'samples': None,\n",
    "    'epochs': None,\n",
    "    'batch_size': None,\n",
    "    'optimizer': None,\n",
    "    'lr': None,\n",
    "    'first_neuron': None,\n",
    "    'dropout': None,\n",
    "    'activation': None,\n",
    "    'leaky_alpha': None,\n",
    "    'hidden_layers': None,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(top_results_index):\n",
    "    \n",
    "    #     Adam = RMSprop + Momentum (lr=0.001)\n",
    "    #     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "    #     RMSprop = (lr=0.001)\n",
    "    #     SGD = (lr=0.01)\n",
    "    #     Adagrad\n",
    "\n",
    "    hyper_parameter = global_hyper_parameter\n",
    "\n",
    "    hyper_parameter['samples'] = [100000]\n",
    "    hyper_parameter['epochs'] = [400]\n",
    "    hyper_parameter['batch_size'] = [df.iloc[top_results_index]['batch_size']]\n",
    "    hyper_parameter['optimizer'] = [make_optimizer(df.loc[top_results_index]['optimizer'])]\n",
    "    hyper_parameter['lr'] = [df.iloc[top_results_index]['lr']]\n",
    "    hyper_parameter['first_neuron'] = [df.iloc[top_results_index]['first_neuron']]\n",
    "    hyper_parameter['dropout'] = [df.iloc[top_results_index]['dropout']]\n",
    "    hyper_parameter['activation'] = [df.iloc[top_results_index]['activation']]\n",
    "    hyper_parameter['leaky_alpha'] = [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "    hyper_parameter['hidden_layers'] = [df.iloc[top_results_index]['hidden_layers']]\n",
    "    \n",
    "    return hyper_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 400, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 100000}\n",
      "Found 80000 validated image filenames.\n",
      "Found 20000 validated image filenames.\n",
      "Steps per Epoch: 2500, Validation Steps: 625\n",
      "Model was compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 54, 54, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 26, 26, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 256)       614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 26, 26, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 384)       147840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 256)       98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              26218496  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 44,794,626\n",
      "Trainable params: 44,791,874\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Checkpointer was created\n",
      "CSV Logger was created\n",
      "Learning Rate Reducer was created\n",
      "Early Stopper was created\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2500 steps, validate for 625 steps\n",
      "Epoch 1/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 4769.6852 - mean_absolut_error: 48.2173\n",
      "Epoch 00001: val_mean_absolut_error improved from inf to 31.29494, saving model to ..\\output\\201019_2253_final_rgb\\CNN_ALEX_Model_and_Weights_rgb.hdf5\n",
      "2500/2500 [==============================] - 1219s 488ms/step - loss: 4769.0999 - mean_absolut_error: 48.2128 - val_loss: 2463.5679 - val_mean_absolut_error: 31.2949\n",
      "Epoch 2/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 2878.5464 - mean_absolut_error: 33.4100\n",
      "Epoch 00002: val_mean_absolut_error did not improve from 31.29494\n",
      "2500/2500 [==============================] - 835s 334ms/step - loss: 2878.6645 - mean_absolut_error: 33.4103 - val_loss: 3540.2220 - val_mean_absolut_error: 37.7034\n",
      "Epoch 3/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 2265.6485 - mean_absolut_error: 28.7575\n",
      "Epoch 00003: val_mean_absolut_error improved from 31.29494 to 24.80275, saving model to ..\\output\\201019_2253_final_rgb\\CNN_ALEX_Model_and_Weights_rgb.hdf5\n",
      "2500/2500 [==============================] - 832s 333ms/step - loss: 2265.3592 - mean_absolut_error: 28.7557 - val_loss: 1956.2862 - val_mean_absolut_error: 24.8028\n",
      "Epoch 4/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1899.6271 - mean_absolut_error: 25.7908\n",
      "Epoch 00004: val_mean_absolut_error did not improve from 24.80275\n",
      "2500/2500 [==============================] - 832s 333ms/step - loss: 1899.3938 - mean_absolut_error: 25.7898 - val_loss: 2370.6653 - val_mean_absolut_error: 28.0604\n",
      "Epoch 5/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1661.1330 - mean_absolut_error: 23.7146\n",
      "Epoch 00005: val_mean_absolut_error improved from 24.80275 to 20.55710, saving model to ..\\output\\201019_2253_final_rgb\\CNN_ALEX_Model_and_Weights_rgb.hdf5\n",
      "2500/2500 [==============================] - 827s 331ms/step - loss: 1660.9968 - mean_absolut_error: 23.7133 - val_loss: 1590.9969 - val_mean_absolut_error: 20.5571\n",
      "Epoch 6/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1491.5419 - mean_absolut_error: 21.9934\n",
      "Epoch 00006: val_mean_absolut_error improved from 20.55710 to 16.41084, saving model to ..\\output\\201019_2253_final_rgb\\CNN_ALEX_Model_and_Weights_rgb.hdf5\n",
      "2500/2500 [==============================] - 833s 333ms/step - loss: 1491.2539 - mean_absolut_error: 21.9913 - val_loss: 1131.0165 - val_mean_absolut_error: 16.4108\n",
      "Epoch 7/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1419.0407 - mean_absolut_error: 21.2584\n",
      "Epoch 00007: val_mean_absolut_error did not improve from 16.41084\n",
      "2500/2500 [==============================] - 829s 332ms/step - loss: 1418.7021 - mean_absolut_error: 21.2561 - val_loss: 1497.9218 - val_mean_absolut_error: 19.9416\n",
      "Epoch 8/400\n",
      "1771/2500 [====================>.........] - ETA: 3:55 - loss: 1310.3420 - mean_absolut_error: 20.2894"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, Global.num_image_channels, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    for top_results_index in [0, 1]:\n",
    "        #_MODEL_TO_LOAD_INDEX = df.iloc[top_results_index].name\n",
    "        #_MODEL_TO_LOAD = 'Best_Weights_FC_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        tmp_dir = '..\\\\TMP_TALOS_{}'.format(Global.device)\n",
    "        trained_results = Global.target_dir + 'Talos_Results_Fine_Idx{}.csv'.format(Global.image_channels)\n",
    "\n",
    "        startTime = datetime.now()\n",
    "        \n",
    "        parameters = get_params(top_results_index)\n",
    "\n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = alexnet,\n",
    "            params = parameters,\n",
    "            experiment_name = tmp_dir,\n",
    "            #shuffle=False,\n",
    "            reduction_metric = get_reduction_metric(Global.reduction_metric),\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = True\n",
    "        )\n",
    "\n",
    "        print(\"Time taken:\", datetime.now() - startTime)\n",
    "        \n",
    "        print('Writing Device File')\n",
    "        device_file.write('Trained Model: {}'.format(Global.image_channels))\n",
    "\n",
    "        df_experiment_results = pd.read_csv(tmp_dir + '\\\\' + os.listdir(tmp_dir)[0])\n",
    "        df_experiment_results['Base'] = None\n",
    "        for i in range(df_experiment_results.shape[0]):\n",
    "            df_experiment_results['Base'][i] = Global.image_channles\n",
    "\n",
    "        if os.path.isfile(trained_results):\n",
    "            df_experiment_results.to_csv(trained_results, mode = 'a', index = False, header = False)\n",
    "        else:\n",
    "            df_experiment_results.to_csv(trained_results, index = False)\n",
    "\n",
    "        shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
