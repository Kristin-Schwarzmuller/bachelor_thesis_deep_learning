{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Training on GPU 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links <a name = \"Top\"></a>\n",
    "\n",
    "<ol>\n",
    "<li><a href = #setup>Begin Training</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Conda Environment: tf_ks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "print('Current Conda Environment: {}'.format(os.environ['CONDA_DEFAULT_ENV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  2 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10473415311927647579\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9105744200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6425795629863405869\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9104897474\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9219620345236701187\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum für Training-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TrainingSet(Enum):\n",
    "    SYNTHETIC = 1\n",
    "    REAL = 2\n",
    "    MIXED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "\n",
    "* <i>SSD</i>, falls genug Speicher auf SSD im SymLink <i>fast_output</i> verfügbar ist\n",
    "* <i>HDD</i>, falls möglicherweise zu wenig SSD-Speicher verfügbar ist $\\rightarrow$ <i>output</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class OutputDirectory(IntEnum):\n",
    "    HDD = 0\n",
    "    SSD = 1\n",
    "    \n",
    "output_path = ['output', 'fast_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Label_Type into suitable label names.\n",
    "$\\Rightarrow$ Angular / Normalized $\\rightarrow$ ['Elevation', 'Azimuth']\n",
    "\n",
    "$\\Rightarrow$ Stereographic $\\rightarrow$ ['S_x', 'S_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Label_Names(label_type):\n",
    "    if label_type == 'Angular' or label_type == 'Normalized':\n",
    "        return ['Elevation', 'Azimuth']\n",
    "    elif label_type == 'Stereographic':\n",
    "        return ['S_x', 'S_y']\n",
    "    else:\n",
    "        assert(True, 'LabelType Invalid')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reduction_Metric(metric):\n",
    "    \n",
    "    if metric == 'custom_mae':\n",
    "        return [custom_mae]\n",
    "    elif metric == 'tf.keras.metrics.MeanAbsoluteError()':\n",
    "        return [tf.keras.metrics.MeanAbsoluteError()]\n",
    "    elif metric == 'circular_mae':\n",
    "        return [circular_mae]\n",
    "    elif metric == 'mean_squared_error':\n",
    "        return ['mean_squared_error']\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'keras.optimizers.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Nadam'>\":\n",
    "        return Nadam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Adagard'>\":\n",
    "        return Adagard\n",
    "    elif optimizer == \"<class 'keras.optimizers.RMSprop'>\":\n",
    "        return RMSprop\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsset-Typ nach String Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_to_string(ts):\n",
    "    if ts == TrainingSet.SYNTHETIC:\n",
    "        return 'Synth'\n",
    "    elif ts == TrainingSet.REAL:\n",
    "        return 'Real'\n",
    "    elif ts == TrainingSet.MIXED:\n",
    "        return 'Mixed'\n",
    "    else:\n",
    "        print('Unknown TrainingSet')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(batch_size, num_samples, label_type):\n",
    "    # if Block für synthetische Daten, um nur auf realen Daten zu trainieren _USE_SYNTHETIC_TRAIN_DATA\n",
    "    # 1. lege df_train und df_valid als leere Liste an\n",
    "    # 2. If-block um Zeile df = ... bis df_valid\n",
    "    \n",
    "    if trainingset == TrainingSet.SYNTHETIC:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "    elif trainingset == TrainingSet.MIXED:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train_real = df_shuffled_real[0: int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid_real = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train_real.shape[0]])\n",
    "        df_train = df_train.drop(df_train.index[df_train.shape[0] - df_train_real.shape[0] : df_train.shape[0]])\n",
    "        df_valid = df_valid.drop(df_valid.index[df_valid.shape[0] - df_valid_real.shape[0] : df_valid.shape[0]])\n",
    "        df_train = df_train.append(df_train_real)\n",
    "        df_valid= df_valid.append(df_valid_real)\n",
    "    \n",
    "    elif trainingset == TrainingSet.REAL: # Add check for num_samples, once the real dataset increases\n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train = df_shuffled_real[0 : int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train.shape[0]])\n",
    "        \n",
    "    else:\n",
    "        print('Create_Data :: should not have reached here')\n",
    "        \n",
    "\n",
    "        \n",
    "    if _USE_DATA_AUGMENTATION:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.25, 0.75),\n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    print('Y-Col: {}'.format(get_Label_Names(label_type)))\n",
    "    print('Train Data Generator: ', end = '')\n",
    "    \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename RGB',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    print('Validation Data Generator: ', end = '')\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename RGB',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Modell (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_model_fine(x, y, x_val, y_val, params):\n",
    "    print('==========================Params:')\n",
    "    print(params)\n",
    "    print('==========================')\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data(params['batch_size'], params['samples'], params['label_type'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    "    dropout_rate = params['dropout']\n",
    "    first_neuron = params['first_neuron']\n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = params['leaky_alpha'])\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    cnn = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "    #cnn = FULL_VGG16(weights = 'None', include_top = False, input_shape = (224, 224, 3))\n",
    "    \n",
    "    for layer in cnn.layers[:15]:\n",
    "        layer.trainable = False\n",
    "        #print(layer.name, layer.trainable)\n",
    "        \n",
    "    print('_________________________________________________________________')\n",
    "    print('{:>16} {:>16}'.format('Network Layer', 'Trainable'))\n",
    "    print('=================================================================')\n",
    "    for layer in cnn.layers:\n",
    "        print('{:>16} {:>16}'.format(layer.name, layer.trainable))\n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    model.add(cnn)\n",
    "    \n",
    "    fc = Sequential()\n",
    "    fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512)\n",
    "    \n",
    "    fc.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    print('Number Hidden Layers {}'.format(params['hidden_layers']))\n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(params['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.load_weights(_MODEL_DIR + _MODEL_TO_LOAD)\n",
    "    model.add(fc)\n",
    "    print('Fully Connected Layers added to Base Network')\n",
    "    \n",
    "    print('Using Loss: {} \\nand Reduction Metric: {}'.format(\n",
    "        params['loss_function'], \n",
    "        get_Reduction_Metric(params['reduction_metric'])))\n",
    "    \n",
    "    model.compile(\n",
    "        #optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])*1e-2),\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer']) * 1e-3),\n",
    "        loss = params['loss_function'],\n",
    "        metrics = get_Reduction_Metric(params['reduction_metric'])\n",
    "    )\n",
    "    print('Model was compiled')\n",
    "    print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = _LOG_DIR + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        monitor =  params['monitor_value'],\n",
    "        verbose = 1,\n",
    "        save_weights_only = False,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    print('Checkpointer was created')\n",
    "    \n",
    "    csv_logger = callbacks.CSVLogger(\n",
    "        filename = _LOG_DIR + 'CNN_Base_{}_Logger_{}.csv'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        separator = ',',\n",
    "        append = False\n",
    "    )\n",
    "    print('CSV Logger was created')\n",
    "\n",
    "    lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 13,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        min_delta = 0.0001\n",
    "    )\n",
    "    print('Learning Rate Reducer was created')\n",
    "    \n",
    "    early_stopper = callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0,\n",
    "        #patience = 15,\n",
    "        patience = 20,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    print('Early Stopper was created')\n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_data = valid_generator,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        callbacks = [checkpointer, csv_logger, lr_reducer, early_stopper],\n",
    "        epochs = params['epochs'],\n",
    "        workers = 8\n",
    "    )\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_MODEL:\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x))\n",
    "\n",
    "    model.add(ock 2\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x))\n",
    "\n",
    "    model.add(ock 3\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x))\n",
    "             \n",
    "    model.add(ock 4)\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x))\n",
    "             \n",
    "    model.add(ock 5)\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG6_MODEL:\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), strides = 1, padding = \"same\", activation = \"relu\", input_shape = (32, 32, 3)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(32, (3, 3), strides = 1, padding = \"same\", activation = \"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), strides = 1, padding = \"same\", activation = \"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3, 3), strides = 1, padding = \"same\", activation = \"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), strides = 1, padding = \"same\", activation = \"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(128, (3, 3), strides = 1, padding = \"same\", activation = \"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FULL_VGG16(include_top=0, weights='None',\n",
    "          input_tensor=None, input_shape=None,\n",
    "          pooling=None):\n",
    "    \"\"\"\"\n",
    "    # Arguments\n",
    "        include_top: whether to include the 3 fully-connected\n",
    "            layers at the top of the network.\n",
    "        weights: one of `None` (random initialization)\n",
    "            or 'imagenet' (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)+\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 input channels,\n",
    "            and width and height should be no smaller than 48.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=48,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    \n",
    "    x = VGG16_MODEL()\n",
    "\n",
    "    if (include_top==0):\n",
    "            fc = Sequential()\n",
    "            fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512)\n",
    "\n",
    "            fc.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "            fc.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            fc.add(Dropout(rate = dropout_rate))\n",
    "\n",
    "        print('Number Hidden Layers {}'.format(params['hidden_layers']))\n",
    "        hidden_neuron_fraction = first_neuron\n",
    "        for i in range(params['hidden_layers']):\n",
    "            hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "            fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "            fc.add(activation_layer)\n",
    "            if dropout_rate > 0.0:\n",
    "                fc.add(Dropout(rate = dropout_rate))\n",
    "\n",
    "        fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.load_weights(_MODEL_DIR + _MODEL_TO_LOAD)\n",
    "        x.add(fc)\n",
    "        print('Fully Connected Layers added to Base Network')\n",
    "\n",
    "    if (include_top==1):\n",
    "        # Classification block\n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    if (include_top==2):\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "    else:\n",
    "        raise ValueError('Error: include_top must be 0 for Azimuth and Elevation '\n",
    "                         'OR 1 for classification OR 2 for no top. Nothing else')\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='vgg16')\n",
    "\n",
    "        if K.backend() == 'theano':\n",
    "            layer_utils.convert_all_kernels_in_model(model)\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            if include_top:\n",
    "                maxpool = model.get_layer(name='block5_pool')\n",
    "                shape = maxpool.output_shape[1:]\n",
    "                dense = model.get_layer(name='fc1')\n",
    "                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n",
    "\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image data format convention '\n",
    "                              '(`image_data_format=\"channels_first\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_data_format=\"channels_last\"` in '\n",
    "                              'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feinoptimierung <a name = \"setup\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "global_hyper_parameter = {\n",
    "    'samples': None,\n",
    "    'epochs': None,\n",
    "    'batch_size': None,\n",
    "    'optimizer': None,\n",
    "    'lr': None,\n",
    "    'first_neuron': None,\n",
    "    'dropout': None,\n",
    "    'activation': None,\n",
    "    'leaky_alpha': None,\n",
    "    'hidden_layers': None,\n",
    "    # beginning from here, Values should only contain one single entry:\n",
    "    # ===============================================================\n",
    "    'label_type': ['Angular'], # Stereographic, Angular, Normalized\n",
    "    'loss_function': None,\n",
    "    'reduction_metric': None,\n",
    "    'monitor_value': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RUN = 'SYNTH'\n",
    "_LOSS = 'MSE'\n",
    "_DATASET_NAME = '201019_2253_final'#'2020-05-28'\n",
    "_DEVICE = 'GeForce_RTX_2080_Ti'#'TITAN_GPU1'\n",
    "\n",
    "storage = OutputDirectory.SSD # 'fast_output' if ssd storage may suffice, 'output' otherwise\n",
    "\n",
    "if global_hyper_parameter['label_type'][0] == 'Stereographic':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_stereographic.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_stereographic.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Angular':\n",
    "    _CSV_SYNTH_FILE_NAME = 'labels_ks.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Normalized':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_normalized.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_normalized.csv'\n",
    "    \n",
    "else:\n",
    "    assert(True, 'Label Type Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = TrainingSet.SYNTHETIC\n",
    "_USE_DATA_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n"
     ]
    }
   ],
   "source": [
    "_IMAGE_DIR = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(_DATASET_NAME)\n",
    "_CSV_FILE = _IMAGE_DIR + _CSV_SYNTH_FILE_NAME\n",
    "_CSV_FILE_REAL = _IMAGE_DIR + _CSV_REAL_FILE_NAME\n",
    "\n",
    "_note = '_Custom-MAE'\n",
    "\n",
    "_MODEL_DIR = '..\\\\output\\\\{}_Regression_{}\\\\{}_{}_Base{}\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "_NET_DIR = '{}_Regression_{}\\\\{}_{}_Top_1{}\\\\{}_TD\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note, trainingset_to_string(trainingset))\n",
    "_LOG_DIR = '..\\\\{}\\\\{}'.format(output_path[storage], _NET_DIR)\n",
    "\n",
    "if(not os.path.exists(_LOG_DIR)):\n",
    "    os.makedirs(_LOG_DIR)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(_LOG_DIR))\n",
    "\n",
    "device_file = open(_LOG_DIR + '{}.txt'.format(_DEVICE), \"a+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 FC-Gewichte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: ..\\output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Base_Custom-MAE\\..\\201019_2253_final_Angular_Base_Custom-MAE_Results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>loss</th>\n",
       "      <th>custom_mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_custom_mae</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>label_type</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>lr</th>\n",
       "      <th>monitor_value</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>reduction_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>10/22/20-210538</td>\n",
       "      <td>10/22/20-210553</td>\n",
       "      <td>14.862882</td>\n",
       "      <td>4383.826411</td>\n",
       "      <td>40.741211</td>\n",
       "      <td>2547.475186</td>\n",
       "      <td>28.446991</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>5</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>10/22/20-212354</td>\n",
       "      <td>10/22/20-212407</td>\n",
       "      <td>12.900292</td>\n",
       "      <td>4370.115269</td>\n",
       "      <td>41.885189</td>\n",
       "      <td>2500.798960</td>\n",
       "      <td>28.509195</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>10/22/20-210249</td>\n",
       "      <td>10/22/20-210259</td>\n",
       "      <td>9.804774</td>\n",
       "      <td>4001.966997</td>\n",
       "      <td>39.212330</td>\n",
       "      <td>2567.742597</td>\n",
       "      <td>28.672417</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>5</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>10/22/20-210657</td>\n",
       "      <td>10/22/20-210713</td>\n",
       "      <td>16.076730</td>\n",
       "      <td>5342.255862</td>\n",
       "      <td>44.598431</td>\n",
       "      <td>2508.385596</td>\n",
       "      <td>29.009453</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>10/22/20-212434</td>\n",
       "      <td>10/22/20-212447</td>\n",
       "      <td>12.947577</td>\n",
       "      <td>4892.983958</td>\n",
       "      <td>44.019562</td>\n",
       "      <td>2545.006084</td>\n",
       "      <td>29.102833</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109</td>\n",
       "      <td>10/22/20-212138</td>\n",
       "      <td>10/22/20-212146</td>\n",
       "      <td>8.363558</td>\n",
       "      <td>4502.218566</td>\n",
       "      <td>42.888409</td>\n",
       "      <td>2549.238323</td>\n",
       "      <td>29.138407</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>10/22/20-210641</td>\n",
       "      <td>10/22/20-210657</td>\n",
       "      <td>15.954672</td>\n",
       "      <td>4115.395587</td>\n",
       "      <td>41.639866</td>\n",
       "      <td>2575.603947</td>\n",
       "      <td>29.298376</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>10/22/20-212129</td>\n",
       "      <td>10/22/20-212137</td>\n",
       "      <td>8.271264</td>\n",
       "      <td>3881.180195</td>\n",
       "      <td>40.146988</td>\n",
       "      <td>2558.554630</td>\n",
       "      <td>29.299927</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>10/22/20-210300</td>\n",
       "      <td>10/22/20-210310</td>\n",
       "      <td>10.109706</td>\n",
       "      <td>3871.796719</td>\n",
       "      <td>39.797153</td>\n",
       "      <td>2617.595140</td>\n",
       "      <td>29.322357</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>10/22/20-210403</td>\n",
       "      <td>10/22/20-210413</td>\n",
       "      <td>10.425704</td>\n",
       "      <td>4304.128398</td>\n",
       "      <td>43.794651</td>\n",
       "      <td>2461.137874</td>\n",
       "      <td>29.322683</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0            start              end   duration         loss  \\\n",
       "32           32  10/22/20-210538  10/22/20-210553  14.862882  4383.826411   \n",
       "123         123  10/22/20-212354  10/22/20-212407  12.900292  4370.115269   \n",
       "17           17  10/22/20-210249  10/22/20-210259   9.804774  4001.966997   \n",
       "37           37  10/22/20-210657  10/22/20-210713  16.076730  5342.255862   \n",
       "126         126  10/22/20-212434  10/22/20-212447  12.947577  4892.983958   \n",
       "109         109  10/22/20-212138  10/22/20-212146   8.363558  4502.218566   \n",
       "36           36  10/22/20-210641  10/22/20-210657  15.954672  4115.395587   \n",
       "108         108  10/22/20-212129  10/22/20-212137   8.271264  3881.180195   \n",
       "18           18  10/22/20-210300  10/22/20-210310  10.109706  3871.796719   \n",
       "24           24  10/22/20-210403  10/22/20-210413  10.425704  4304.128398   \n",
       "\n",
       "     custom_mae     val_loss  val_custom_mae activation  batch_size  dropout  \\\n",
       "32    40.741211  2547.475186       28.446991  leakyrelu          32     0.25   \n",
       "123   41.885189  2500.798960       28.509195  leakyrelu          64     0.25   \n",
       "17    39.212330  2567.742597       28.672417  leakyrelu          32     0.25   \n",
       "37    44.598431  2508.385596       29.009453  leakyrelu          32     0.25   \n",
       "126   44.019562  2545.006084       29.102833  leakyrelu          64     0.25   \n",
       "109   42.888409  2549.238323       29.138407  leakyrelu          64     0.25   \n",
       "36    41.639866  2575.603947       29.298376  leakyrelu          32     0.25   \n",
       "108   40.146988  2558.554630       29.299927  leakyrelu          64     0.25   \n",
       "18    39.797153  2617.595140       29.322357  leakyrelu          32     0.25   \n",
       "24    43.794651  2461.137874       29.322683  leakyrelu          32     0.25   \n",
       "\n",
       "     first_neuron  hidden_layers label_type       loss_function  lr  \\\n",
       "32           4096              0    Angular  mean_squared_error   5   \n",
       "123          4096              1    Angular  mean_squared_error   1   \n",
       "17           2048              0    Angular  mean_squared_error   5   \n",
       "37           4096              2    Angular  mean_squared_error   2   \n",
       "126          4096              2    Angular  mean_squared_error   1   \n",
       "109          2048              1    Angular  mean_squared_error   2   \n",
       "36           4096              2    Angular  mean_squared_error   1   \n",
       "108          2048              1    Angular  mean_squared_error   1   \n",
       "18           2048              1    Angular  mean_squared_error   1   \n",
       "24           2048              3    Angular  mean_squared_error   1   \n",
       "\n",
       "      monitor_value                                          optimizer  \\\n",
       "32   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "123  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "17   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "37   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "126  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "109  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "36   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "108  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "18   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "24   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "\n",
       "    reduction_metric  \n",
       "32        custom_mae  \n",
       "123       custom_mae  \n",
       "17        custom_mae  \n",
       "37        custom_mae  \n",
       "126       custom_mae  \n",
       "109       custom_mae  \n",
       "36        custom_mae  \n",
       "108       custom_mae  \n",
       "18        custom_mae  \n",
       "24        custom_mae  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_results = _MODEL_DIR + '..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "df = pd.read_csv(base_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "sort_value = df['monitor_value'][0]\n",
    "df = df.sort_values(sort_value, axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(base_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(top_results_index):\n",
    "    \n",
    "    #     Adam = RMSprop + Momentum (lr=0.001)\n",
    "    #     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "    #     RMSprop = (lr=0.001)\n",
    "    #     SGD = (lr=0.01)\n",
    "    #     Adagrad\n",
    "\n",
    "    hyper_parameter = global_hyper_parameter\n",
    "\n",
    "    hyper_parameter['samples'] = [100000] \n",
    "    hyper_parameter['epochs'] = [400]\n",
    "    hyper_parameter['batch_size'] = [df.iloc[top_results_index]['batch_size']]\n",
    "    hyper_parameter['optimizer'] = [make_optimizer(df.loc[top_results_index]['optimizer'])]\n",
    "    hyper_parameter['lr'] = [df.iloc[top_results_index]['lr']]\n",
    "    hyper_parameter['first_neuron'] = [df.iloc[top_results_index]['first_neuron']]\n",
    "    hyper_parameter['dropout'] = [df.iloc[top_results_index]['dropout']]\n",
    "    hyper_parameter['activation'] = [df.iloc[top_results_index]['activation']]\n",
    "    hyper_parameter['leaky_alpha'] = [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "    hyper_parameter['hidden_layers'] = [df.iloc[top_results_index]['hidden_layers']]\n",
    "    \n",
    "    hyper_parameter['loss_function'] = [df.iloc[top_results_index]['loss_function']]\n",
    "    hyper_parameter['reduction_metric'] = [df.iloc[top_results_index]['reduction_metric']]\n",
    "    hyper_parameter['monitor_value'] = [df.iloc[top_results_index]['monitor_value']]\n",
    "\n",
    "    return hyper_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 4096, 'hidden_layers': 0, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================Params:\n",
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 4096, 'hidden_layers': 0, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================\n",
      "Y-Col: ['Elevation', 'Azimuth']\n",
      "Train Data Generator: Found 80000 validated image filenames.\n",
      "Validation Data Generator: Found 20000 validated image filenames.\n",
      "Steps per Epoch: 2500, Validation Steps: 625\n",
      "_________________________________________________________________\n",
      "   Network Layer        Trainable\n",
      "=================================================================\n",
      "         input_1                0\n",
      "    block1_conv1                0\n",
      "    block1_conv2                0\n",
      "     block1_pool                0\n",
      "    block2_conv1                0\n",
      "    block2_conv2                0\n",
      "     block2_pool                0\n",
      "    block3_conv1                0\n",
      "    block3_conv2                0\n",
      "    block3_conv3                0\n",
      "     block3_pool                0\n",
      "    block4_conv1                0\n",
      "    block4_conv2                0\n",
      "    block4_conv3                0\n",
      "     block4_pool                0\n",
      "    block5_conv1                1\n",
      "    block5_conv2                1\n",
      "    block5_conv3                1\n",
      "     block5_pool                1\n",
      "_________________________________________________________________\n",
      "\n",
      "Number Hidden Layers 0\n",
      "Fully Connected Layers added to Base Network\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000177856DB1F8>]\n",
      "Model was compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 2)                 102772738 \n",
      "=================================================================\n",
      "Total params: 117,487,426\n",
      "Trainable params: 109,852,162\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Checkpointer was created\n",
      "CSV Logger was created\n",
      "Learning Rate Reducer was created\n",
      "Early Stopper was created\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2500 steps, validate for 625 steps\n",
      "Epoch 1/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 4513.6754 - custom_mae: 45.0521\n",
      "Epoch 00001: val_custom_mae improved from inf to 33.89689, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 948s 379ms/step - loss: 4512.9927 - custom_mae: 45.0472 - val_loss: 2811.0408 - val_custom_mae: 33.8969\n",
      "Epoch 2/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 2599.5520 - custom_mae: 31.8191\n",
      "Epoch 00002: val_custom_mae improved from 33.89689 to 28.30746, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 867s 347ms/step - loss: 2599.3764 - custom_mae: 31.8191 - val_loss: 2119.7826 - val_custom_mae: 28.3075\n",
      "Epoch 3/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 2082.9407 - custom_mae: 27.3272\n",
      "Epoch 00003: val_custom_mae improved from 28.30746 to 24.70516, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 871s 349ms/step - loss: 2082.5527 - custom_mae: 27.3251 - val_loss: 1760.5811 - val_custom_mae: 24.7052\n",
      "Epoch 4/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1811.1874 - custom_mae: 24.8435 ETA: 0s - loss: 1811.3034 - custom_mae: 24.84\n",
      "Epoch 00004: val_custom_mae improved from 24.70516 to 23.04770, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 874s 349ms/step - loss: 1811.0353 - custom_mae: 24.8429 - val_loss: 1604.2285 - val_custom_mae: 23.0477\n",
      "Epoch 5/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1642.7328 - custom_mae: 23.2427\n",
      "Epoch 00005: val_custom_mae improved from 23.04770 to 21.93037, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 868s 347ms/step - loss: 1642.9723 - custom_mae: 23.2447 - val_loss: 1508.6811 - val_custom_mae: 21.9304\n",
      "Epoch 6/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1521.5072 - custom_mae: 21.9888\n",
      "Epoch 00006: val_custom_mae improved from 21.93037 to 21.00656, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 869s 348ms/step - loss: 1521.6193 - custom_mae: 21.9879 - val_loss: 1432.4077 - val_custom_mae: 21.0066\n",
      "Epoch 7/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1419.8082 - custom_mae: 20.9531\n",
      "Epoch 00007: val_custom_mae improved from 21.00656 to 20.32517, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 871s 349ms/step - loss: 1419.6315 - custom_mae: 20.9520 - val_loss: 1348.1675 - val_custom_mae: 20.3252\n",
      "Epoch 8/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1344.4621 - custom_mae: 20.1620\n",
      "Epoch 00008: val_custom_mae improved from 20.32517 to 20.10365, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 871s 348ms/step - loss: 1344.3865 - custom_mae: 20.1617 - val_loss: 1341.4769 - val_custom_mae: 20.1037\n",
      "Epoch 9/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1276.0187 - custom_mae: 19.4084\n",
      "Epoch 00009: val_custom_mae improved from 20.10365 to 19.51111, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 871s 348ms/step - loss: 1276.1473 - custom_mae: 19.4092 - val_loss: 1274.6355 - val_custom_mae: 19.5111\n",
      "Epoch 10/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1231.9458 - custom_mae: 18.9302\n",
      "Epoch 00010: val_custom_mae improved from 19.51111 to 18.23300, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 871s 348ms/step - loss: 1231.6987 - custom_mae: 18.9296 - val_loss: 1183.4507 - val_custom_mae: 18.2330\n",
      "Epoch 11/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1177.4133 - custom_mae: 18.3722\n",
      "Epoch 00011: val_custom_mae did not improve from 18.23300\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 1177.3336 - custom_mae: 18.3731 - val_loss: 1205.6909 - val_custom_mae: 18.7871\n",
      "Epoch 12/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1143.7153 - custom_mae: 17.8689\n",
      "Epoch 00012: val_custom_mae improved from 18.23300 to 18.14558, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 876s 350ms/step - loss: 1143.8502 - custom_mae: 17.8695 - val_loss: 1169.9678 - val_custom_mae: 18.1456\n",
      "Epoch 13/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1098.8324 - custom_mae: 17.4235\n",
      "Epoch 00013: val_custom_mae did not improve from 18.14558\n",
      "2500/2500 [==============================] - 865s 346ms/step - loss: 1098.9736 - custom_mae: 17.4249 - val_loss: 1155.7278 - val_custom_mae: 18.3734\n",
      "Epoch 14/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1080.4896 - custom_mae: 17.1546\n",
      "Epoch 00014: val_custom_mae improved from 18.14558 to 17.58828, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 877s 351ms/step - loss: 1080.6119 - custom_mae: 17.1549 - val_loss: 1112.5486 - val_custom_mae: 17.5883\n",
      "Epoch 15/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1049.6479 - custom_mae: 16.7721 ETA: 5s - loss:\n",
      "Epoch 00015: val_custom_mae did not improve from 17.58828\n",
      "2500/2500 [==============================] - 863s 345ms/step - loss: 1049.5618 - custom_mae: 16.7719 - val_loss: 1122.6586 - val_custom_mae: 17.7345\n",
      "Epoch 16/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1012.3475 - custom_mae: 16.4333\n",
      "Epoch 00016: val_custom_mae improved from 17.58828 to 16.31562, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 868s 347ms/step - loss: 1012.2585 - custom_mae: 16.4327 - val_loss: 1015.0088 - val_custom_mae: 16.3156\n",
      "Epoch 17/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 991.3835 - custom_mae: 16.0989\n",
      "Epoch 00017: val_custom_mae did not improve from 16.31562\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 991.2985 - custom_mae: 16.0977 - val_loss: 1068.5834 - val_custom_mae: 16.5567\n",
      "Epoch 18/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 977.9304 - custom_mae: 15.8801\n",
      "Epoch 00018: val_custom_mae improved from 16.31562 to 15.92825, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 875s 350ms/step - loss: 978.2733 - custom_mae: 15.8823 - val_loss: 1003.5562 - val_custom_mae: 15.9283\n",
      "Epoch 19/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 951.7902 - custom_mae: 15.6398\n",
      "Epoch 00019: val_custom_mae did not improve from 15.92825\n",
      "2500/2500 [==============================] - 863s 345ms/step - loss: 952.0834 - custom_mae: 15.6411 - val_loss: 983.6857 - val_custom_mae: 15.9297\n",
      "Epoch 20/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 928.9851 - custom_mae: 15.2781\n",
      "Epoch 00020: val_custom_mae improved from 15.92825 to 15.35479, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 875s 350ms/step - loss: 928.7924 - custom_mae: 15.2766 - val_loss: 944.5374 - val_custom_mae: 15.3548\n",
      "Epoch 21/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 916.5181 - custom_mae: 15.1015\n",
      "Epoch 00021: val_custom_mae improved from 15.35479 to 15.13170, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 867s 347ms/step - loss: 916.6075 - custom_mae: 15.1013 - val_loss: 943.0638 - val_custom_mae: 15.1317\n",
      "Epoch 22/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 895.2415 - custom_mae: 14.9062\n",
      "Epoch 00022: val_custom_mae did not improve from 15.13170\n",
      "2500/2500 [==============================] - 864s 345ms/step - loss: 895.3510 - custom_mae: 14.9073 - val_loss: 998.1903 - val_custom_mae: 15.2296\n",
      "Epoch 23/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 877.4555 - custom_mae: 14.6297\n",
      "Epoch 00023: val_custom_mae did not improve from 15.13170\n",
      "2500/2500 [==============================] - 869s 348ms/step - loss: 877.3887 - custom_mae: 14.6289 - val_loss: 961.1819 - val_custom_mae: 15.2944\n",
      "Epoch 24/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 867.3120 - custom_mae: 14.4422\n",
      "Epoch 00024: val_custom_mae improved from 15.13170 to 14.67971, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_32_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 875s 350ms/step - loss: 867.5194 - custom_mae: 14.4435 - val_loss: 909.3460 - val_custom_mae: 14.6797\n",
      "Epoch 25/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 846.5493 - custom_mae: 14.2021"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    #for top_results_index in range(3):\n",
    "    #for top_results_index in [0, 1]:\n",
    "        top_results_index = 0\n",
    "        _MODEL_TO_LOAD_INDEX = df.iloc[top_results_index].name\n",
    "        _MODEL_TO_LOAD = 'Best_Weights_FC_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        _TMP_DIR = '..\\\\TMP_TALOS_{}'.format(_DEVICE)\n",
    "        _CSV_RESULTS = _LOG_DIR + 'Talos_Results_Fine_Idx{}.csv'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        startTime = datetime.now()\n",
    "        \n",
    "        parameters = get_params(top_results_index)\n",
    "\n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = grid_model_fine,\n",
    "            params = parameters,\n",
    "            experiment_name = _TMP_DIR,\n",
    "            #shuffle=False,\n",
    "            reduction_metric = parameters['reduction_metric'][0],\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = True\n",
    "        )\n",
    "\n",
    "        print(\"Time taken:\", datetime.now() - startTime)\n",
    "        \n",
    "        print('Writing Device File')\n",
    "        device_file.write('Trained Model: {}'.format(_MODEL_TO_LOAD))\n",
    "\n",
    "        df_experiment_results = pd.read_csv(_TMP_DIR + '\\\\' + os.listdir(_TMP_DIR)[0])\n",
    "        df_experiment_results['Base'] = None\n",
    "        for i in range(df_experiment_results.shape[0]):\n",
    "            df_experiment_results['Base'][i] = _MODEL_TO_LOAD_INDEX\n",
    "\n",
    "        if os.path.isfile(_CSV_RESULTS):\n",
    "            df_experiment_results.to_csv(_CSV_RESULTS, mode = 'a', index = False, header = False)\n",
    "        else:\n",
    "            df_experiment_results.to_csv(_CSV_RESULTS, index = False)\n",
    "\n",
    "        shutil.rmtree(_TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Results to NAS if SSD Directory was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_directory(src, dst, symlinks = False, ignore = None):\n",
    "    maxLen = 0\n",
    "    message = ''        \n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        \n",
    "        message = 'Creating Path: {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        os.makedirs(dst)\n",
    "        \n",
    "    for item in os.listdir(src):\n",
    "        \n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        \n",
    "        if os.path.isdir(s):\n",
    "            \n",
    "            message = 'Copying Directory: {}'.format(s)\n",
    "            maxLen = max(maxLen, len(message))\n",
    "            print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "            \n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if not os.path.exists(d): #or os.stat(s).st_mtime - os.stat(d).st_mtime > 1:\n",
    "                \n",
    "                message = 'Copying File: {}'.format(s)\n",
    "                maxLen = max(maxLen, len(message))\n",
    "                print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "                \n",
    "                shutil.copy2(s, d)\n",
    "        \n",
    "        time.sleep(.5)\n",
    "     \n",
    "    message = 'Coyping... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "\n",
    "def delete_directory(src, terminator = '\\n'):\n",
    "    message = ''\n",
    "    maxLen = 0\n",
    "    \n",
    "    try:\n",
    "        message = 'Deleting {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        shutil.rmtree(src)\n",
    "        \n",
    "    except OSError as e:\n",
    "        message = 'Error: {} : {}'.format(src, e.strerror)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "        return\n",
    "    \n",
    "    message = 'Deleting... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = terminator)\n",
    "\n",
    "    \n",
    "def copy_fine_training(src, dst):\n",
    "    copy_directory(src, dst)\n",
    "    delete_directory(src, terminator = '\\r')\n",
    "    delete_directory(src + '..\\\\', terminator = '\\r')\n",
    "    if not os.listdir(src + '..\\\\..\\\\'):\n",
    "        delete_directory(src + '..\\\\..\\\\', terminator = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(storage == OutputDirectory.SSD):\n",
    "    _COPY_DIR = '..\\\\output\\\\{}'.format(_NET_DIR)\n",
    "    copy_fine_training(_LOG_DIR, _COPY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name = \"CMSE.Mixed\"></a><a href = #Top>Up</a></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
