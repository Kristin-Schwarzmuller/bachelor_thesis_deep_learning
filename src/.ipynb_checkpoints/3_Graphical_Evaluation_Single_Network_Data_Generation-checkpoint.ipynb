{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation<a name = \"Top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links\n",
    "\n",
    "<ol>\n",
    "    <li><a href = #setup>Setup</a></li>\n",
    "    <li><a href = #store>Save File</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  2 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16354879290914898652\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9105744200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13616712144195203260\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9104897474\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5555813214875000103\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "#import ntpath\n",
    "\n",
    "import copy\n",
    "import re\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALEX Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(activation, leaky_alpha, dropout):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation=activation_layer, input_shape=(224,224,Global.num_image_channels)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(units = 2, activation=activation_layer)\n",
    "        #Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(activation, leaky_alpha):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape=(224,224,Global.num_image_channels), filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    #model.add(Dense(units = 2, activation = \"softmax\"))\n",
    "    #model.add(Dense(units = 2, activation=activation_layer))\n",
    "    model.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "\n",
    "    #opt = Adam(lr = 0.001)\n",
    "    #model.compile(optimizer = opt, loss= keras.losses.categorical_crossentropy, metrics = ['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolut_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenpipeline f√ºr Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data():\n",
    "    \n",
    "    df = pd.read_csv(Global.csv_file)\n",
    "    df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "    df_test = df_shuffled[df_shuffled.shape[0] - 10000 : df_shuffled.shape[0]]\n",
    "     \n",
    "    test_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "\n",
    "    test_generator = test_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_test,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        shuffle = False,\n",
    "        batch_size = 1\n",
    "    )\n",
    "    \n",
    "    return test_generator, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell f√ºr Inferenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(parameters, prediction_runs = 1):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    test_generator, df_test = setup_data()\n",
    "\n",
    "    if(Global.net_architecture == 'ALEX'):\n",
    "        model = alexnet(params['activation'], params['leaky_alpha'], params['drop'])\n",
    "    elif(Global.net_architecture == 'VGG16'):\n",
    "        model = vgg16(params['activation'], params['leaky_alpha'])\n",
    "    else:\n",
    "        print('Wrong net architecture!')\n",
    "\n",
    "    print('Using Optimizer: {} with Learning Rate: {}'.format(parameters['optimizer'][0], parameters['learning_rate']))\n",
    "    model.compile(\n",
    "        optimizer = parameters['optimizer'][0](lr = parameters['learning_rate']),\n",
    "        loss = Global.loss_function, \n",
    "        metrics = get_Reduction_Metric(Global.reduction_metric)\n",
    "    )\n",
    "\n",
    "    test_generator.reset()\n",
    "\n",
    "    print(\"Predicting using these values:\\nTest Data: {}\\nUsing Loss: {} on Dataset: {}\".format(parameters['dataset_to_use'], parameters['loss_function'], parameters['dataset_name']))\n",
    "    print('Net: {}'.format(Global.net_architecture))\n",
    "    \n",
    "    duration = 0    \n",
    "    \n",
    "    for n in range(prediction_runs):\n",
    "        startTime = datetime.now()\n",
    "        #predictions = model.predict_generator(generator = test_generator, steps = test_generator.n // test_generator.batch_size, verbose = 0)\n",
    "        predictions = model.predict(x = test_generator, steps = test_generator.n // test_generator.batch_size, verbose = 0)\n",
    "        duration = (datetime.now() - startTime).total_seconds()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    time_per_prediction = duration / prediction_runs\n",
    "    time_per_image = time_per_prediction / len(predictions)\n",
    "    print(\"Prediction repeated {} times at a total time of {}sec. \\nAverage Time per Prediction: {} sec. Average Time per Image: {} sec\".format(prediction_runs, duration, time_per_prediction, time_per_image))\n",
    "\n",
    "    del model\n",
    "    return predictions, df_test, duration, time_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct for global parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class global_parameter:\n",
    "    loss_function: str = 'mean_squared_error'\n",
    "    reduction_metric: str = 'mean_absolut_error'\n",
    "    monitor_value: str = 'val_mean_absolut_error'\n",
    "        \n",
    "    net_architecture = 'VGG16' # 'AlexNet'\n",
    "    \n",
    "    dataset: str = '201019_2253_final'\n",
    "    device: str = 'RTX_2080_Ti'\n",
    "    data_augmentation: bool = True\n",
    "    image_channels: str = 'rgba' # just change this, everything else will automaticlly adjusted\n",
    "    num_image_channels: int = 3\n",
    "    image_dir: str = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(dataset)\n",
    "    \n",
    "    csv_file_name: str = 'labels_ks_RGB.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(net_architecture, dataset, image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(net_architecture, dataset)\n",
    "    results_man: str = '\\\\..\\\\ALEX_201019_2253_final_Results_manual.csv'\n",
    "\n",
    "        \n",
    "Global = global_parameter\n",
    "\n",
    "if(Global.image_channels == 'rgba'):\n",
    "    Global.num_image_channels = 4\n",
    "    Global.csv_file_name: str = 'labels_ks_RGBD.csv'\n",
    "    Global.csv_file: str = Global.image_dir + Global.csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(Global.net_architecture, Global.dataset, Global.image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(Global.net_architecture, Global.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(dataframe, training_set, model_to_load, dataset):\n",
    "    params = copy.deepcopy(p)\n",
    "    params['dataset_to_use'] = training_set\n",
    "    params['model_to_load'] = model_to_load\n",
    "    params['dataset_name']: Global.dataset\n",
    "    # -------------------------------------------------------------\n",
    "    params['optimizer'] = [make_optimizer(dataframe.loc[0]['optimizer'])]\n",
    "    params['learning_rate'] = dataframe.iloc[0]['lr']\n",
    "    params['first_neuron'] = dataframe.iloc[0]['first_neuron']\n",
    "    params['dropout_rate'] = dataframe.iloc[0]['dropout']\n",
    "    params['activation_function'] = dataframe.iloc[0]['activation']\n",
    "    params['leaky_ReLU_alpha'] = dataframe.iloc[0]['leaky_alpha']\n",
    "    params['hidden_layers'] = dataframe.iloc[0]['hidden_layers']\n",
    "    # -------------------------------------------------------------\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radians $\\rightarrow$ Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_degree(angle_in_rad):\n",
    "    return angle_in_rad * 180 / np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree $\\rightarrow$ Radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_radians(angle_in_deg):\n",
    "    return angle_in_deg * np.pi / 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sph√§rische $\\rightarrow$ Karthesische Koordinaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spheric_cartesian_polar(phi_d, theta_d):\n",
    "    x = np.sin(np.radians(90.0 - theta_d)) * np.cos(np.radians(phi_d))\n",
    "    y = np.sin(np.radians(90.0 - theta_d)) * np.sin(np.radians(phi_d))\n",
    "    z = np.cos(np.radians(90.0 - theta_d))\n",
    "    return array([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spheric_cartesian_elevation(phi_d, theta_d):\n",
    "    x = np.cos(np.radians(theta_d)) * np.cos(np.radians(phi_d))\n",
    "    y = np.cos(np.radians(theta_d)) * np.sin(np.radians(phi_d))\n",
    "    z = np.sin(np.radians(theta_d))\n",
    "    return array([x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorlength(vector):\n",
    "    return np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculated Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angular_error(deg_e_phi, deg_e_theta):\n",
    "    return np.degrees(np.arccos(np.cos(np.radians(deg_e_phi)) * np.cos(np.radians(deg_e_theta))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalarprodukt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDot(a, b):\n",
    "    dot = 0;\n",
    "    it = np.nditer(a, flags=['f_index'])\n",
    "    for x in it:\n",
    "        dot = dot + (x * b[it.index])\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_angular_error_elevation(predicted_deg_vector, true_deg_vector):    \n",
    "    c_predicted = spheric_cartesian_elevation(predicted_deg_vector[0], predicted_deg_vector[1])\n",
    "    c_true = spheric_cartesian_elevation(true_deg_vector[0], true_deg_vector[1])\n",
    "    \n",
    "    len_prediction = vectorlength(c_predicted)\n",
    "    len_true = vectorlength(c_true)\n",
    "    \n",
    "    cos_angle = np.dot(c_true, c_predicted) / len_prediction / len_true\n",
    "    \n",
    "    return abs(np.degrees(np.arccos(cos_angle)))\n",
    "\n",
    "def dot_angular_error_polar(predicted_deg_vector, true_deg_vector):\n",
    "    c_predicted = spheric_cartesian_polar(predicted_deg_vector[0], predicted_deg_vector[1])\n",
    "    c_true = spheric_cartesian_polar(true_deg_vector[0], true_deg_vector[1])\n",
    "    \n",
    "    len_prediction = vectorlength(c_predicted)\n",
    "    len_true = vectorlength(c_true)\n",
    "    \n",
    "    cos_angle = np.dot(c_true, c_predicted) / len_prediction / len_true\n",
    "    \n",
    "    return abs(np.degrees(np.arccos(cos_angle)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduction_metric(metric):\n",
    "    \n",
    "    if metric == 'mean_absolut_error':\n",
    "        return [mean_absolut_error]\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normierte sph√§rische Koordinaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_df, eval_predictions, save_dir = None, save_to_file = False):\n",
    "    num_predictions = eval_df.shape[0]\n",
    "    \n",
    "    df_result = pd.DataFrame({\n",
    "        'Filename': eval_df['Filename'][0:num_predictions],\n",
    "        'Elevation_true': None if _STEREOGRAPHIC else eval_df['Elevation'][0:num_predictions],\n",
    "        'Elevation_pred': None,\n",
    "        'Elevation_err': None,\n",
    "        'Azimuth_true': None if _STEREOGRAPHIC else eval_df['Azimuth'][0:num_predictions],\n",
    "        'Azimuth_pred': None,\n",
    "        'Azimuth_err': None,\n",
    "        'Angular_MAE': None,\n",
    "        'dot_angular_err_elevation': None,\n",
    "        'dot_angular_err_polar': None,\n",
    "    })\n",
    "\n",
    "    angular_calculated = ''\n",
    "    stereo_calculated = '*'\n",
    "\n",
    "    df_result['Elevation_pred'] = eval_predictions[:, 0]\n",
    "    df_result['Azimuth_pred'] = eval_predictions[:, 1]\n",
    "\n",
    "    for index, row in df_result.iterrows():\n",
    "        #print('=======================================================================')\n",
    "        predicted_azimuth = row['Azimuth_pred']\n",
    "        predicted_elevation = row['Elevation_pred']\n",
    "\n",
    "        true_azimuth = row['Azimuth_true']\n",
    "        true_elevation = row['Elevation_true']\n",
    "\n",
    "        # azimuth and elevation error\n",
    "        error_elevation = abs((predicted_elevation - true_elevation))\n",
    "        df_result.at[index, 'Elevation_err'] = error_elevation\n",
    "\n",
    "        error_azimuth = min(abs(predicted_azimuth - true_azimuth), abs(360 - abs(predicted_azimuth - true_azimuth)))\n",
    "        df_result.at[index, 'Azimuth_err'] = error_azimuth\n",
    "\n",
    "        # dot angular error\n",
    "        predicted_direction = array([predicted_azimuth, predicted_elevation])\n",
    "        true_direction = array([true_azimuth, true_elevation])\n",
    "\n",
    "        error_direction_elevation = dot_angular_error_elevation(predicted_direction, true_direction)\n",
    "        df_result.at[index, 'dot_angular_err_elevation'] = error_direction_elevation\n",
    "        error_direction_polar = dot_angular_error_polar(predicted_direction, true_direction)\n",
    "        df_result.at[index, 'dot_angular_err_polar'] = error_direction_polar\n",
    "\n",
    "\n",
    "        # Angular Mean Absolute Error\n",
    "        y_pred = K.constant(predicted_direction)\n",
    "        y_true = K.constant (true_direction)\n",
    "        mae = mean_absolut_error(y_true, y_pred)\n",
    "        df_result.at[index, 'Angular_MAE'] = mae\n",
    "        #print('=======================================================================')\n",
    "\n",
    "    error_elevation_avg = np.mean(df_result['Elevation_err'], axis = 0)\n",
    "    error_azimuth_avg = np.mean(df_result['Azimuth_err'], axis = 0)\n",
    "    error_sx_avg = np.mean(df_result['S_x_err'], axis = 0)\n",
    "    error_sy_avg = np.mean(df_result['S_y_err'], axis = 0)\n",
    "    \n",
    "    \n",
    "    # Box'n Whiskers Diagram Data - Angular Error\n",
    "    e_angular_median = np.quantile(df_result['dot_angular_err_elevation'], 0.5)\n",
    "    e_angular_lower_q = np.quantile(df_result['dot_angular_err_elevation'], 0.25)\n",
    "    e_angular_upper_q = np.quantile(df_result['dot_angular_err_elevation'], 0.75)\n",
    "    e_angular_min = np.amin(df_result['dot_angular_err_elevation'])\n",
    "    e_angular_max = np.amax(df_result['dot_angular_err_elevation'])\n",
    "    \n",
    "    max_angular_error = df_result.loc[df_result['dot_angular_err_elevation'] == e_angular_max]\n",
    "    max_angular_error_file = max_angular_error.iloc[0]['Filename']\n",
    "    max_angular_error_theta_true = max_angular_error.iloc[0]['Elevation_true']\n",
    "    max_angular_error_phi_true = max_angular_error.iloc[0]['Azimuth_true']\n",
    "    max_angular_error_theta_pred = max_angular_error.iloc[0]['Elevation_pred']\n",
    "    max_angular_error_phi_pred = max_angular_error.iloc[0]['Azimuth_pred']\n",
    "    \n",
    "    \n",
    "    # Box'n Whiskers Diagram Data - Mean Absolute Error\n",
    "    e_mae_median = np.quantile(df_result['Angular_MAE'], 0.5)\n",
    "    e_mae_lower_q = np.quantile(df_result['Angular_MAE'], 0.25)\n",
    "    e_mae_upper_q = np.quantile(df_result['Angular_MAE'], 0.75)\n",
    "    e_mae_min = np.amin(df_result['Angular_MAE'])\n",
    "    e_mae_max = np.amax(df_result['Angular_MAE'])\n",
    "    \n",
    "    max_mae_error = df_result.loc[df_result['Angular_MAE'] == e_mae_max]\n",
    "    max_mae_error_file = max_mae_error.iloc[0]['Filename']\n",
    "    max_mae_error_theta_true = max_mae_error.iloc[0]['Elevation_true']\n",
    "    max_mae_error_phi_true = max_mae_error.iloc[0]['Azimuth_true']\n",
    "    max_mae_error_theta_pred = max_mae_error.iloc[0]['Elevation_pred']\n",
    "    max_mae_error_phi_pred = max_mae_error.iloc[0]['Azimuth_pred']\n",
    "    \n",
    "    # Avg Angular Error\n",
    "    error_dot_angular_elevation_avg = np.mean(df_result['dot_angular_err_elevation'], axis = 0)\n",
    "    angular_variance = np.var(df_result['dot_angular_err_elevation'], axis = 0)\n",
    "    avg_angular_lower_mean = np.mean(df_result['dot_angular_err_elevation'][df_result['dot_angular_err_elevation'] < error_dot_angular_elevation_avg])\n",
    "    avg_angular_upper_mean = np.mean(df_result['dot_angular_err_elevation'][df_result['dot_angular_err_elevation'] > error_dot_angular_elevation_avg])\n",
    "    \n",
    "    # Avg MAE Error\n",
    "    avg_mae_mean = np.mean(df_result['Angular_MAE'])\n",
    "    avg_mae_lower_mean = np.mean(df_result['Angular_MAE'][df_result['Angular_MAE'] < avg_mae_mean])\n",
    "    avg_mae_upper_mean = np.mean(df_result['Angular_MAE'][df_result['Angular_MAE'] > avg_mae_mean])\n",
    "    \n",
    "\n",
    "    print('Durchschnnittlicher Fehler Elevation{}: {:.1f}'.format(angular_calculated, error_elevation_avg))\n",
    "    print('Durchschnnittlicher Fehler Azimut{}: {:.1f}'.format(angular_calculated, error_azimuth_avg))\n",
    "    print()\n",
    "    print('Durchschnnittlicher Winkelfehler{} (Elevation): {:.1f}'.format(angular_calculated, error_dot_angular_elevation_avg))\n",
    "    print('Varianz Winkelfehler{}: {:.1f}'.format(angular_calculated, angular_variance))\n",
    "    print()\n",
    "    print('Box-Whisker Angular: max - {:.1f}, upper - {:.1f}, median - {:.1f}, lower - {:.1f}, min - {:.1f}'.format(e_angular_max, e_angular_upper_q, e_angular_median, e_angular_lower_q, e_angular_min))\n",
    "    print('Maxium Angular Error on: {}, True ({:.1f}, {:.1f}), Pred ({:.1f}, {:.1f})'.format(max_angular_error_file, max_angular_error_phi_true, max_angular_error_theta_true, max_angular_error_phi_pred, max_angular_error_theta_pred))\n",
    "    print('Box-Whisker MAE: max - {:.1f}, upper - {:.1f}, median - {:.1f}, lower - {:.1f}, min - {:.1f}'.format(e_mae_max, e_mae_upper_q, e_mae_median, e_mae_lower_q, e_mae_min))\n",
    "    print('Maxium MAE Error on: {}, True ({:.1f}, {:.1f}), Pred ({:.1f}, {:.1f})'.format(max_mae_error_file, max_mae_error_phi_true, max_mae_error_theta_true, max_mae_error_phi_pred, max_mae_error_theta_pred))\n",
    "    print()\n",
    "    print('Avg Angular Range: lower - {:.1f}, avg - {:.1f}, upper - {:.1f}'.format(avg_angular_lower_mean, error_dot_angular_elevation_avg, avg_angular_upper_mean))\n",
    "    print('Avg MAE Range: lower - {:.1f}, avg - {:.1f}, upper - {:.1f}'.format(avg_mae_lower_mean, avg_mae_mean, avg_mae_upper_mean))\n",
    "    \n",
    "        \n",
    "    df_avg = pd.DataFrame({\n",
    "        'Avg_Elevation_Err': [error_elevation_avg],\n",
    "        'Avg_Azimuth_Err': [error_azimuth_avg],\n",
    "        'Avg_Angular_Err': [error_dot_angular_elevation_avg],\n",
    "        'Angular_Variance': [angular_variance],\n",
    "        'Avg_Inference_Time': [image_time],\n",
    "        'box_angular_medium': [e_angular_median],\n",
    "        'box_angular_lower': [e_angular_lower_q],\n",
    "        'box_angular_upper': [e_angular_upper_q],\n",
    "        'box_angular_min': [e_angular_min],\n",
    "        'box_angular_max': [e_angular_max],\n",
    "        'box_mae_medium': [e_mae_median],\n",
    "        'box_mae_lower': [e_mae_lower_q],\n",
    "        'box_mae_upper': [e_mae_upper_q],\n",
    "        'box_mae_min': [e_mae_min],\n",
    "        'box_mae_max': [e_mae_max],\n",
    "        'avg_angular_lower_mean': [avg_angular_lower_mean],\n",
    "        'avg_angular_upper_mean': [avg_angular_upper_mean],\n",
    "        'avg_mae_mean': [avg_mae_mean],\n",
    "        'avg_mae_lower_mean': [avg_mae_lower_mean],\n",
    "        'avg_mae_lower_mean': [avg_mae_upper_mean],\n",
    "        'max_angular_error_file': [max_angular_error_file],\n",
    "        'max_angular_error_theta_true': [max_angular_error_theta_true],\n",
    "        'max_angular_error_phi_true': [max_angular_error_phi_true],\n",
    "        'max_angular_error_theta_pred': [max_angular_error_theta_pred],\n",
    "        'max_angular_error_phi_pred': [max_angular_error_phi_pred],\n",
    "        'max_mae_error_file': [max_mae_error_file],\n",
    "        'max_mae_error_theta_true': [max_mae_error_theta_true],\n",
    "        'max_mae_error_phi_true': [max_mae_error_phi_true],\n",
    "        'max_mae_error_theta_pred': [max_mae_error_theta_pred],\n",
    "        'max_mae_error_phi_pred': [max_mae_error_phi_pred]\n",
    "    })\n",
    "\n",
    "    if save_to_file:\n",
    "        df_result.to_csv(save_dir + 'Prognosen_ErrE_{}_ErrA_{}.csv'.format('%.2f'%error_elevation_avg, '%.2f'%error_azimuth_avg), index=False)\n",
    "    \n",
    "    model_path, current_model = ntpath.split(model)\n",
    "    evaluation_path = model_path + '\\\\Evaluation\\\\'\n",
    "    if(not os.path.exists(evaluation_path)):\n",
    "        os.makedirs(evaluation_path)\n",
    "    df_result.to_csv(evaluation_path + 'Model-{}_Testset-{}_Prediction_Results.csv'.format(net_index, trainingset_to_string(test_set)), index=False)\n",
    "    df_avg.to_csv(evaluation_path + 'Model-{}_Testset-{}_Average_Results.csv'.format(net_index, trainingset_to_string(test_set)), index=False)\n",
    "    \n",
    "    return df_result, df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model <a name = \"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Setup\n",
    "<p><a href = #Top>Top</a> \n",
    "<p><a href = #store>Save File</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required format of parameters parameter for _model_predict_ (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'dataset_to_use':[],\n",
    "    'model_to_load':[],\n",
    "    'dataset_name':'synth_dataset',\n",
    "    # --------------------------------\n",
    "    'optimizer':[],\n",
    "    'learning_rate':[],\n",
    "    'first_neuron':[],\n",
    "    'dropout_rate':[],\n",
    "    'activation_function':[],\n",
    "    'leaky_ReLU_alpha':[],\n",
    "    'hidden_layers':[],\n",
    "    # --------------------------------\n",
    "    'label_type':['Angular'],\n",
    "    'loss_function':[],\n",
    "    'reduction_metric':[],\n",
    "    'monitor_value':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct for global parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class global_parameter:\n",
    "    loss_function: str = 'mean_squared_error'\n",
    "    reduction_metric: str = 'mean_absolut_error'\n",
    "    monitor_value: str = 'val_mean_absolut_error'\n",
    "        \n",
    "    net_architecture = 'VGG16' # 'AlexNet'\n",
    "    \n",
    "    dataset: str = '201019_2253_final'\n",
    "    device: str = 'RTX_2080_Ti'\n",
    "    data_augmentation: bool = True\n",
    "    image_channels: str = 'rgba' # just change this, everything else will automaticlly adjusted\n",
    "    num_image_channels: int = 3\n",
    "    image_dir: str = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(dataset)\n",
    "    \n",
    "    csv_file_name: str = 'labels_ks_RGB.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(net_architecture, dataset, image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(net_architecture, dataset)\n",
    "    results_man: str = '\\\\..\\\\ALEX_201019_2253_final_Results_manual.csv'\n",
    "    \n",
    "    run =datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    print(run)\n",
    "    talos_results = 'Talos_Results_Fine_Idx{}_{}.csv'.format(Global.net_architecture, Global.image_channels)\n",
    "    network_path = '..\\\\output\\\\{}_{}_{}_Top_1_Evaluation\\\\'\n",
    "    syn_trained_model_dir = network_path.format(run, net_architecture, image_channels)\n",
    "\n",
    "        \n",
    "Global = global_parameter\n",
    "\n",
    "if(Global.image_channels == 'rgba'):\n",
    "    Global.num_image_channels = 4\n",
    "    Global.csv_file_name: str = 'labels_ks_RGBD.csv'\n",
    "    Global.csv_file: str = Global.image_dir + Global.csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(Global.net_architecture, Global.dataset, Global.image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(Global.net_architecture, Global.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Trainiert mit: Synthetische Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = syn_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_type = None\n",
    "loss_function = None\n",
    "reduction_metric = None\n",
    "monitor_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = syn_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, synth_samples)\n",
    "current_params = load_params(dataframe, test_set, model, Global.dataset, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainiert mit: Reale Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = real_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = real_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, real_samples)\n",
    "current_params = load_params(dataframe, test_set, model, Global.dataset, label_type, Global.loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainiert mit: Gemischte Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = mixed_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mixed_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, mixed_samples)\n",
    "current_params = load_params(dataframe, test_set, model, Global.dataset, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reale Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TrainingSet.REAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainiert mit: Synthetische Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = syn_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = syn_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, synth_samples)\n",
    "current_params = load_params(dataframe, test_set, model, Global.dataset, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainiert mit: Reale Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = real_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = real_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, real_samples)\n",
    "current_params = load_params(dataframe, test_set, model, Global.dataset, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Traininert mit: Gemischte Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file = mixed_trained_model_dir + talos_results\n",
    "dataframe = pd.read_csv(network_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mixed_trained_model_dir + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(net_index, mixed_samples)\n",
    "current_params = load_params(dataframe, test_set, model, Global.dataset, label_type, loss_function, reduction_metric, monitor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions, df_test, duration, image_time = model_predict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df, avg_df = evaluate(df_test, predictions)\n",
    "mse_results.append([network_file, result_df, current_params['dataset_to_use'], avg_df, label_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation <a name = \"store\">\n",
    "<p></a><a href = #Top>Top</a>\n",
    "<p><a href = #setup>Setup</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = '..\\\\output\\\\{}_Regression_{}\\\\Graphical_Evaluation\\\\'.format(run, loss)\n",
    "\n",
    "if(not os.path.exists(eval_dir)):\n",
    "    os.makedirs(eval_dir)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(eval_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(eval_dir + '{}_Net{}_{}{}_Results.pickle'.format(Global.dataset, net_index, p['label_type'][0], _note), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mse_results, fp) # uncomment if you are REALLY sure to overwrite the file\n",
    "\n",
    "with open(eval_dir + '{}_Net{}_{}{}_Results.pickle'.format(Global.dataset, net_index, p['label_type'][0], _note), \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_test]",
   "language": "python",
   "name": "conda-env-tf_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
