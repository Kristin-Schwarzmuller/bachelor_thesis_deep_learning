{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Base Training on GPU0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links <a name = \"Top\"></a>\n",
    "<ol>\n",
    "    <li><p><a href = #setup>Set Up</a></p></li>\n",
    "    <li><p><a href = #Base>Base Training</a></p></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "#print('Current Conda Environment: {}'.format(os.environ['CONDA_DEFAULT_ENV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  1 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16744123514524920838\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 20264236482\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4951676981344698482\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "\n",
    "* <i>SSD</i>, falls genug Speicher auf SSD im SymLink <i>fast_output</i> verfügbar ist\n",
    "* <i>HDD</i>, falls möglicherweise zu wenig SSD-Speicher verfügbar ist $\\rightarrow$ <i>output</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class OutputDirectory(IntEnum):\n",
    "    HDD = 0\n",
    "    SSD = 1\n",
    "    \n",
    "output_path = ['output', 'fast_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Label_Type into suitable label names.\n",
    "$\\Rightarrow$ Angular / Normalized $\\rightarrow$ ['Elevation', 'Azimuth']\n",
    "\n",
    "$\\Rightarrow$ Stereographic $\\rightarrow$ ['S_x', 'S_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Label_Names(label_type):\n",
    "    if label_type == 'Angular' or label_type == 'Normalized':\n",
    "        return ['Elevation', 'Azimuth']\n",
    "    elif label_type == 'Stereographic':\n",
    "        return ['S_x', 'S_y']\n",
    "    else:\n",
    "        assert(True, 'LabelType Invalid')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype = 'float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype = 'float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reduction_Metric(metric):\n",
    "    \n",
    "    if metric == 'custom_mae':\n",
    "        return [custom_mae]\n",
    "    elif metric == 'tf.keras.metrics.MeanAbsoluteError()':\n",
    "        return [tf.keras.metrics.MeanAbsoluteError()]\n",
    "    elif metric == 'circular_mae':\n",
    "        return [circular_mae]\n",
    "    elif metric == 'mean_squared_error':\n",
    "        return ['mean_squared_error']\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Bottleneck-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bottleneck_features(train_generator, valid_generator, bottleneck):\n",
    "        \n",
    "    print('Creating bottleneck features...')\n",
    "    if(_NET == 'VGG16'):\n",
    "        model = VGG16(include_top = False, weights = 'imagenet', input_shape = (224, 224, 3))\n",
    "    elif(_NET == 'RESNET'):\n",
    "        model = ResNet50(include_top = False, weights = 'imagenet', input_shape = (224, 224, 3))\n",
    "    else:\n",
    "        print('ERROR NET SPELLED WRONG')\n",
    "         \n",
    "    #bottleneck_features_train = model.predict_generator(train_generator, train_generator.n // train_generator.batch_size)\n",
    "    bottleneck_features_train = model.predict(train_generator)\n",
    "    np.save(open(_LOG_DIR + 'Train_' + bottleneck, 'wb'), bottleneck_features_train)\n",
    "        \n",
    "    #bottleneck_features_valid = model.predict_generator(valid_generator, valid_generator.n // valid_generator.batch_size)\n",
    "    bottleneck_features_valid = model.predict(valid_generator)\n",
    "    np.save(open(_LOG_DIR + 'Valid_'  + bottleneck, 'wb'), bottleneck_features_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch_size, num_samples, label_type):\n",
    "\n",
    "    df = pd.read_csv(_CSV_FILE)\n",
    "    df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "    df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]   \n",
    "    df_valid = df_shuffled.drop(df_shuffled.index[0: df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "    \n",
    "    train_labels = df_train.drop(['Filename'], axis = 1).values\n",
    "    valid_labels = df_valid.drop(['Filename'], axis = 1).values\n",
    "    \n",
    "    bottleneck = 'Bottleneck_Features_{}_{}.npy'.format(str(num_samples), str(batch_size))                                       \n",
    "    if not os.path.exists(_LOG_DIR + 'Train_' + bottleneck):\n",
    "        \n",
    "        if _USE_DATA_AUGMENTATION:\n",
    "            train_data_generator = ImageDataGenerator(\n",
    "                rescale = 1./255, \n",
    "                width_shift_range = 0.1,\n",
    "                height_shift_range = 0.1, \n",
    "                zoom_range = 0.1,\n",
    "                brightness_range = (0.25, 0.75),\n",
    "                fill_mode = 'nearest'\n",
    "            )\n",
    "        else:\n",
    "            train_data_generator = ImageDataGenerator(\n",
    "                rescale = 1./255\n",
    "            )\n",
    "            \n",
    "        valid_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "    \n",
    "        print('Y-Col: {}'.format(get_Label_Names(label_type)))\n",
    "        \n",
    "        train_generator = train_data_generator.flow_from_dataframe(\n",
    "            dataframe = df_train,\n",
    "            directory = _IMAGE_DIR,\n",
    "            x_col = 'Filename',\n",
    "            y_col = get_Label_Names(label_type),\n",
    "            class_mode = 'raw',\n",
    "            target_size = (224, 224),\n",
    "            color_mode = 'rgb',\n",
    "            shuffle = False,\n",
    "            seed = 1,\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "    \n",
    "        valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "            dataframe = df_valid,\n",
    "            directory = _IMAGE_DIR,\n",
    "            x_col = 'Filename',\n",
    "            y_col = get_Label_Names(label_type),\n",
    "            class_mode = 'raw',\n",
    "            target_size = (224, 224),\n",
    "            color_mode = 'rgb',\n",
    "            shuffle = False,\n",
    "            seed = 1,\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "                                                        \n",
    "        create_bottleneck_features(train_generator, valid_generator, bottleneck)\n",
    "        \n",
    "    train_features = np.load(open(_LOG_DIR + 'Train_' + bottleneck, 'rb'))\n",
    "    valid_features = np.load(open(_LOG_DIR + 'Valid_' + bottleneck, 'rb'))\n",
    "                                                        \n",
    "    return train_labels, valid_labels, train_features, valid_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Modell (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_model_base(x, y, x_val, y_val, params):\n",
    "    \n",
    "    global _COUNTER    \n",
    "    K.clear_session()\n",
    "\n",
    "    train_labels, valid_labels, train_features, valid_features = prepare_data(params['batch_size'], params['samples'], params['label_type'])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = train_features.shape[1 :])) # (7, 7, 512)\n",
    "    \n",
    "    dropout_rate = params['dropout']\n",
    "    first_neuron = params['first_neuron']\n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = 0.1)\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()  \n",
    "    \n",
    "    model.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    model.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        model.add(Dropout(rate = dropout_rate))\n",
    "        \n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(params['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        model.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        model.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            model.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    model.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "\n",
    "    print('Using Loss: {} \\nand Reduction Metric: {}'.format(\n",
    "        params['loss_function'], \n",
    "        get_Reduction_Metric(params['reduction_metric'])))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer'])), \n",
    "        loss = params['loss_function'], \n",
    "        metrics = get_Reduction_Metric(params['reduction_metric'])\n",
    "    )\n",
    "    \n",
    "    print('Monitor: {}'.format(params['monitor_value']))\n",
    "    \n",
    "    #checkpointer = callbacks.ModelCheckpoint(\n",
    "    #    filepath = _LOG_DIR + 'Best_Weights_FC_{}.hdf5'.format(_COUNTER),\n",
    "    #    monitor = params['monitor_value'],\n",
    "    #    verbose = 1,\n",
    "    #    save_best_only = True,\n",
    "    #    mode = 'min'\n",
    "    #)\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    out = model.fit(\n",
    "        x = train_features,\n",
    "        y = train_labels,\n",
    "        epochs = params['epochs'],\n",
    "        validation_data = (valid_features, valid_labels),\n",
    "        steps_per_epoch = int(params['samples'] * 0.8 // params['batch_size']),\n",
    "        validation_steps = int(params['samples'] * 0.2 // params['batch_size']),\n",
    "        #callbacks = [checkpointer]\n",
    "    )\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "    \n",
    "    _COUNTER = _COUNTER + 1\n",
    "     \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter <a name = \"setup\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "hyper_parameter = {\n",
    "    'samples': [20000],\n",
    "    'epochs': [1],\n",
    "    'batch_size': [32, 64],\n",
    "    'optimizer': [Adam],\n",
    "    'lr': [1, 2, 5],\n",
    "    'first_neuron': [1024, 2048, 4096],\n",
    "    'dropout': [0.25, 0.50],\n",
    "    'activation': ['leakyrelu', 'relu'],\n",
    "    'hidden_layers': [0, 1, 2, 3, 4],\n",
    "    # beginning from here, Values should only contain a single entry:\n",
    "    # ===============================================================\n",
    "    'label_type': ['Angular'], # Stereographic, Angular, Normalized\n",
    "    'loss_function': ['mean_squared_error'], # circular_mse\n",
    "    'reduction_metric': ['custom_mae'], # tf.keras.metrics.MeanAbsoluteError(), circular_mae, custom_mae, mean_squared_error\n",
    "    'monitor_value': ['val_custom_mae'] # val_custom_mae, val_mean_absolute_error, val_circular_mae, val_loss\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():  \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dateisystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RUN = 'SYNTH'\n",
    "_LOSS = 'MSE'\n",
    "_DATASET_NAME = '2020-05-28'\n",
    "_DEVICE = 'TITAN_GPU0'\n",
    "\n",
    "storage = OutputDirectory.SSD # 'fast_output' if ssd storage may suffice, 'output' otherwise\n",
    "\n",
    "#APPENDIX = 'Stereographic'\n",
    "\n",
    "#FUNCTION_OVERRIDE = ['mean_squared_error', [custom_mae], 'val_custom_mae'] # None, or e. g. ['mean_squared_error', [circular_mae], 'val_circular_mae']\n",
    "\n",
    "if hyper_parameter['label_type'][0] == 'Stereographic':\n",
    "    _CSV_FILE_NAME = 'images_synthetisch_stereographic.csv'\n",
    "    _STEREOGRAPHIC = True\n",
    "elif hyper_parameter['label_type'][0] == 'Angular':\n",
    "    _CSV_FILE_NAME = 'images_synthetisch.csv'\n",
    "    _STEREOGRAPHIC = False\n",
    "elif hyper_parameter['label_type'][0] == 'Normalized':\n",
    "    _CSV_FILE_NAME = 'images_synthetisch_normalized.csv'\n",
    "    _STEREOGRAPHIC = False\n",
    "else:\n",
    "    assert(True, 'LabelType Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_USE_DATA_AUGMENTATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\fast_output\\SYNTH_Regression_MSE_RESNET\\2020-05-28_Angular_Base_Custom-MAE\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n",
      "..\\fast_output\\SYNTH_Regression_MSE_RESNET\\2020-05-28_Angular_Base_Custom-MAE\\\\..\\2020-05-28_Angular_Base_Custom-MAE_Results.csv\n"
     ]
    }
   ],
   "source": [
    "_MODEL_NAME = '{}_Optimierung_Hyperparameter_v{}'.format(_DATASET_NAME, _RUN)\n",
    "_IMAGE_DIR = '..\\\\dataset_mm\\\\{}\\\\'.format(_DATASET_NAME)\n",
    "_CSV_FILE = _IMAGE_DIR + _CSV_FILE_NAME\n",
    "\n",
    "_COUNTER = 0\n",
    "\n",
    "_NET = 'RESNET' # RESNET vs VGG16\n",
    "\n",
    "_note = '_Custom-MAE'\n",
    "\n",
    "_NET_DIR = '{}_Regression_{}_{}\\\\{}_{}_Base{}\\\\'.format(_RUN, _LOSS, _NET, _DATASET_NAME, hyper_parameter['label_type'][0], _note)\n",
    "_LOG_DIR = '..\\\\{}\\\\{}'.format(output_path[storage], _NET_DIR)\n",
    "\n",
    "_RESULTS_FILE = '\\\\..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, hyper_parameter['label_type'][0], _note)\n",
    "\n",
    "if(not os.path.exists(_LOG_DIR)):\n",
    "    os.makedirs(_LOG_DIR)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(_LOG_DIR))\n",
    "\n",
    "device_file = open(_LOG_DIR + '{}'.format(_DEVICE), \"a+\")\n",
    "device_file.close()\n",
    "\n",
    "print(_LOG_DIR + _RESULTS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ausführung GridSearch mit Talos <a name = \"Base\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/360 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tf_ks\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1897s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:31:42.385672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                           | 1/360 [31:47<190:10:39, 1907.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1818s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:21.433936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                         | 2/360 [1:02:13<187:13:57, 1882.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1816s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:19.327892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                         | 3/360 [1:32:37<184:57:35, 1865.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1818s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:21.594670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                         | 4/360 [2:03:03<183:17:20, 1853.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1825s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:29.150758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█                                                                         | 5/360 [2:33:37<182:11:33, 1847.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1812s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:15.587971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▏                                                                        | 6/360 [3:03:57<180:52:22, 1839.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1804s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:08.346204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▍                                                                        | 7/360 [3:34:10<179:35:12, 1831.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1820s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:23.369334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▋                                                                        | 8/360 [4:04:38<178:58:44, 1830.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1822s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:26.124261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▊                                                                        | 9/360 [4:35:09<178:28:53, 1830.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1811s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:14.891627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██                                                                       | 10/360 [5:05:29<177:39:08, 1827.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1817s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:20.450592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▏                                                                      | 11/360 [5:35:54<177:05:01, 1826.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1816s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:20.250284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▍                                                                      | 12/360 [6:06:19<176:31:42, 1826.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1822s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:25.448097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██▋                                                                      | 13/360 [6:36:49<176:08:22, 1827.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1808s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:11.615779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██▊                                                                      | 14/360 [7:07:05<175:18:57, 1824.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 1831s 4s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:30:35.399397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███                                                                      | 15/360 [7:37:46<175:16:22, 1828.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 2926s 6s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:48:52.739566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▏                                                                     | 16/360 [8:26:43<206:32:27, 2161.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 2932s 6s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:48:58.187602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▍                                                                     | 17/360 [9:15:46<228:16:22, 2395.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 2922s 6s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:48:48.666587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▌                                                                    | 18/360 [10:04:39<242:55:27, 2557.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "500/500 [==============================] - 2969s 6s/step - loss: nan - custom_mae: nan - val_loss: nan - val_custom_mae: nan\n",
      "Time taken: 0:49:36.267316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▊                                                                    | 19/360 [10:54:20<254:15:30, 2684.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x0000025C9927E168>]\n",
      "Monitor: val_custom_mae\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 500 samples, validate on 4000 samples\n",
      "499/500 [============================>.] - ETA: 5s - loss: nan - custom_mae: nan "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    \n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = grid_model_base,\n",
    "            params = hyper_parameter,\n",
    "            experiment_name = '{}'.format(_DATASET_NAME),\n",
    "            #shuffle=False,\n",
    "            reduction_metric = hyper_parameter['reduction_metric'][0],\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = True,\n",
    "            save_weights = False\n",
    "        )\n",
    "        \n",
    "\n",
    "#t.data.to_csv(_LOG_DIR + _RESULTS_FILE, index = True)\n",
    "t.data.to_csv(_LOG_DIR + _RESULTS_FILE, index = True, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy best 10 Results to NAS if SSD Directory was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = _LOG_DIR + '..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, hyper_parameter['label_type'][0], _note)\n",
    "df = pd.read_csv(best_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "df = df.sort_values(hyper_parameter['monitor_value'][0], axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(best_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_base_directory(src, dst, data, symlinks = False, ignore = None):\n",
    "    maxLen = 0\n",
    "    message = ''\n",
    "    \n",
    "    networks_to_copy = data.head(10).index\n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        \n",
    "        message = 'Creating Path: {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        os.makedirs(dst)\n",
    "        \n",
    "    for item in os.listdir(src):\n",
    "        \n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        \n",
    "        if os.path.isdir(s):\n",
    "            \n",
    "            message = 'Copying Directory: {}'.format(s)\n",
    "            maxLen = max(maxLen, len(message))\n",
    "            print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "            \n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if not os.path.exists(d): #or os.stat(s).st_mtime - os.stat(d).st_mtime > 1:\n",
    "                for idx in networks_to_copy:\n",
    "                    net = '_FC_{}.hdf5'.format(idx)\n",
    "                    if net in item or '_Bottleneck_' in item or '_GPU' in item:\n",
    "                        message = 'Copying File: {}'.format(s)\n",
    "                        maxLen = max(maxLen, len(message))\n",
    "                        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "                \n",
    "                        shutil.copy2(s, d)\n",
    "        \n",
    "        time.sleep(.1)    \n",
    "    \n",
    "    src_csv = src + _RESULTS_FILE\n",
    "    dst_csv = dst + _RESULTS_FILE\n",
    "    if not os.path.exists(dst_csv):\n",
    "        message = 'Copying File: {}'.format(src_csv)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        shutil.copy2(src_csv, dst_csv)\n",
    "     \n",
    "    message = 'Coyping... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "\n",
    "    \n",
    "def delete_directory(src, terminator = '\\n'):\n",
    "    message = ''\n",
    "    maxLen = 0\n",
    "    \n",
    "    try:\n",
    "        message = 'Deleting {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        if os.path.isdir(src):\n",
    "            shutil.rmtree(src)\n",
    "            \n",
    "        elif os.path.isfile(src):\n",
    "            os.remove(src)\n",
    "        \n",
    "    except OSError as e:\n",
    "        message = 'Error: {} : {}'.format(src, e.strerror)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "        return\n",
    "    \n",
    "    message = 'Deleting... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = terminator)\n",
    "\n",
    "    \n",
    "def copy_base_training(src, dst):\n",
    "    copy_base_directory(src, dst, df)\n",
    "    \n",
    "    delete_directory(src, terminator = '\\r')\n",
    "    delete_directory(src + _RESULTS_FILE, terminator = '\\r')\n",
    "    if not os.listdir(src + '..\\\\'):\n",
    "        delete_directory(src + '..\\\\', terminator = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(storage == OutputDirectory.SSD):\n",
    "    _COPY_DIR = '..\\\\output\\\\{}'.format(_NET_DIR)\n",
    "    copy_base_training(_LOG_DIR, _COPY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOG_DIR = _LOG_DIR.replace('fast_', '')\n",
    "best_results = _LOG_DIR + '..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, hyper_parameter['label_type'][0], _note)\n",
    "print(best_results)\n",
    "df = pd.read_csv(best_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "df = df.sort_values(hyper_parameter['monitor_value'][0], axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(best_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'keras.optimizers.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Nadam'>\":\n",
    "        return Nadam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Adagard'>\":\n",
    "        return Adagard\n",
    "    elif optimizer == \"<class 'keras.optimizers.RMSprop'>\":\n",
    "        return RMSprop\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results_index = 0\n",
    "\n",
    "hyper_parameter['samples'] = [20000]\n",
    "hyper_parameter['epochs'] = [1]\n",
    "hyper_parameter['batch_size'] = [df.iloc[top_results_index]['batch_size']]\n",
    "hyper_parameter['optimizer'] = [make_optimizer(df.loc[top_results_index]['optimizer'])]\n",
    "hyper_parameter['lr'] = [df.iloc[top_results_index]['lr']]\n",
    "hyper_parameter['first_neuron'] = [df.iloc[top_results_index]['first_neuron']]\n",
    "hyper_parameter['dropout'] = [df.iloc[top_results_index]['dropout']]\n",
    "hyper_parameter['activation'] = [df.iloc[top_results_index]['activation']]\n",
    "hyper_parameter['leaky_alpha'] = [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "hyper_parameter['hidden_layers'] = [df.iloc[top_results_index]['hidden_layers']]\n",
    "\n",
    "hyper_parameter['loss_function'] = [df.iloc[top_results_index]['loss_function']]\n",
    "hyper_parameter['reduction_metric'] = [df.iloc[top_results_index]['reduction_metric']]\n",
    "hyper_parameter['monitor_value'] = [df.iloc[top_results_index]['monitor_value']]\n",
    "\n",
    "print(hyper_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_model_base_with_checkpoint(x, y, x_val, y_val, params):\n",
    "    \n",
    "    global _COUNTER    \n",
    "    K.clear_session()\n",
    "\n",
    "    train_labels, valid_labels, train_features, valid_features = prepare_data(params['batch_size'], params['samples'], params['label_type'])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = train_features.shape[1 :])) # (7, 7, 512)\n",
    "    \n",
    "    dropout_rate = params['dropout']\n",
    "    first_neuron = params['first_neuron']\n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = 0.1)\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()  \n",
    "    \n",
    "    model.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    model.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        model.add(Dropout(rate = dropout_rate))\n",
    "        \n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(params['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        model.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        model.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            model.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    model.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "\n",
    "    print('Using Loss: {} \\nand Reduction Metric: {}'.format(\n",
    "        params['loss_function'], \n",
    "        get_Reduction_Metric(params['reduction_metric'])))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer'])), \n",
    "        loss = params['loss_function'], \n",
    "        metrics = get_Reduction_Metric(params['reduction_metric'])\n",
    "    )\n",
    "    \n",
    "    print('Monitor: {}'.format(params['monitor_value']))\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = _LOG_DIR + 'Best_Weights_FC_{}.hdf5'.format(_COUNTER),\n",
    "        monitor = params['monitor_value'],\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    out = model.fit(\n",
    "        x = train_features,\n",
    "        y = train_labels,\n",
    "        epochs = params['epochs'],\n",
    "        validation_data = (valid_features, valid_labels),\n",
    "        steps_per_epoch = int(params['samples'] * 0.8 // params['batch_size']),\n",
    "        validation_steps = int(params['samples'] * 0.2 // params['batch_size']),\n",
    "        callbacks = [checkpointer]\n",
    "    )\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "    \n",
    "    _COUNTER = _COUNTER + 1\n",
    "     \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    \n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = grid_model_base_with_checkpoint,\n",
    "            params = hyper_parameter,\n",
    "            experiment_name = '{}'.format(_DATASET_NAME),\n",
    "            #shuffle=False,\n",
    "            reduction_metric = hyper_parameter['reduction_metric'][0],\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = True,\n",
    "            save_weights = False\n",
    "        )\n",
    "        \n",
    "_RESULTS_FILE = _RESULTS_FILE.replace('.csv', '_Top_1.csv')\n",
    "t.data.to_csv(_LOG_DIR + _RESULTS_FILE, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
