{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Training on GPU 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links <a name = \"Top\"></a>\n",
    "\n",
    "<ol>\n",
    "<li><a href = #setup>Begin Training</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "#print('Current Conda Environment: {}'.format(os.environ['CONDA_DEFAULT_ENV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.3.1 does not include GPU support.\n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4121312485433687498\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11800305277889147288\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12820639740191829897\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum für Training-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TrainingSet(Enum):\n",
    "    SYNTHETIC = 1\n",
    "    REAL = 2\n",
    "    MIXED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "\n",
    "* <i>SSD</i>, falls genug Speicher auf SSD im SymLink <i>fast_output</i> verfügbar ist\n",
    "* <i>HDD</i>, falls möglicherweise zu wenig SSD-Speicher verfügbar ist $\\rightarrow$ <i>output</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class OutputDirectory(IntEnum):\n",
    "    HDD = 0\n",
    "    SSD = 1\n",
    "    \n",
    "output_path = ['output', 'fast_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Label_Type into suitable label names.\n",
    "$\\Rightarrow$ Angular / Normalized $\\rightarrow$ ['Elevation', 'Azimuth']\n",
    "\n",
    "$\\Rightarrow$ Stereographic $\\rightarrow$ ['S_x', 'S_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Label_Names(label_type):\n",
    "    if label_type == 'Angular' or label_type == 'Normalized':\n",
    "        return ['Elevation', 'Azimuth']\n",
    "    elif label_type == 'Stereographic':\n",
    "        return ['S_x', 'S_y']\n",
    "    else:\n",
    "        assert(True, 'LabelType Invalid')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reduction_Metric(metric):\n",
    "    \n",
    "    if metric == 'custom_mae':\n",
    "        return [custom_mae]\n",
    "    elif metric == 'tf.keras.metrics.MeanAbsoluteError()':\n",
    "        return [tf.keras.metrics.MeanAbsoluteError()]\n",
    "    elif metric == 'circular_mae':\n",
    "        return [circular_mae]\n",
    "    elif metric == 'mean_squared_error':\n",
    "        return ['mean_squared_error']\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'keras.optimizers.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Nadam'>\":\n",
    "        return Nadam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Adagard'>\":\n",
    "        return Adagard\n",
    "    elif optimizer == \"<class 'keras.optimizers.RMSprop'>\":\n",
    "        return RMSprop\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsset-Typ nach String Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_to_string(ts):\n",
    "    if ts == TrainingSet.SYNTHETIC:\n",
    "        return 'Synth'\n",
    "    elif ts == TrainingSet.REAL:\n",
    "        return 'Real'\n",
    "    elif ts == TrainingSet.MIXED:\n",
    "        return 'Mixed'\n",
    "    else:\n",
    "        print('Unknown TrainingSet')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(batch_size, num_samples, label_type):\n",
    "    # if Block für synthetische Daten, um nur auf realen Daten zu trainieren _USE_SYNTHETIC_TRAIN_DATA\n",
    "    # 1. lege df_train und df_valid als leere Liste an\n",
    "    # 2. If-block um Zeile df = ... bis df_valid\n",
    "    \n",
    "    if trainingset == TrainingSet.SYNTHETIC:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "    elif trainingset == TrainingSet.MIXED:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train_real = df_shuffled_real[0: int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid_real = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train_real.shape[0]])\n",
    "        df_train = df_train.drop(df_train.index[df_train.shape[0] - df_train_real.shape[0] : df_train.shape[0]])\n",
    "        df_valid = df_valid.drop(df_valid.index[df_valid.shape[0] - df_valid_real.shape[0] : df_valid.shape[0]])\n",
    "        df_train = df_train.append(df_train_real)\n",
    "        df_valid= df_valid.append(df_valid_real)\n",
    "    \n",
    "    elif trainingset == TrainingSet.REAL: # Add check for num_samples, once the real dataset increases\n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train = df_shuffled_real[0 : int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train.shape[0]])\n",
    "        \n",
    "    else:\n",
    "        print('Create_Data :: should not have reached here')\n",
    "        \n",
    "\n",
    "        \n",
    "    if _USE_DATA_AUGMENTATION:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.25, 0.75),\n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    print('Y-Col: {}'.format(get_Label_Names(label_type)))\n",
    "    print('Train Data Generator: ', end = '')\n",
    "    \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    print('Validation Data Generator: ', end = '')\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Modell (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_model_fine(x, y, x_val, y_val, params):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data(params['batch_size'], params['samples'], params['label_type'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    "    dropout_rate = params['dropout']\n",
    "    first_neuron = params['first_neuron']\n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = params['leaky_alpha'])\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    if(_NET == 'VGG16'):\n",
    "        cnn = VGG16(include_top = False, weights = 'imagenet', input_shape = (224, 224, 3))\n",
    "    elif(_NET == 'RESNET'):\n",
    "        cnn = ResNet50(include_top = False, weights = 'imagenet', input_shape = (224, 224, 3))\n",
    "    else:\n",
    "        print('ERROR NET SPELLED WRONG')\n",
    "        \n",
    "    \n",
    "    for layer in cnn.layers[:15]:\n",
    "        layer.trainable = False\n",
    "        #print(layer.name, layer.trainable)\n",
    "        \n",
    "    print('_________________________________________________________________')\n",
    "    print('{:>16} {:>16}'.format('Network Layer', 'Trainable'))\n",
    "    print('=================================================================')\n",
    "    for layer in cnn.layers:\n",
    "        print('{:>16} {:>16}'.format(layer.name, layer.trainable))\n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    model.add(cnn)\n",
    "    \n",
    "    fc = Sequential()\n",
    "    fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512)\n",
    "    \n",
    "    fc.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    print('Number Hidden Layers {}'.format(params['hidden_layers']))\n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(params['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.load_weights(_MODEL_DIR + _MODEL_TO_LOAD)\n",
    "    model.add(fc)\n",
    "    print('Fully Connected Layers added to Base Network')\n",
    "    \n",
    "    print('Using Loss: {} \\nand Reduction Metric: {}'.format(\n",
    "        params['loss_function'], \n",
    "        get_Reduction_Metric(params['reduction_metric'])))\n",
    "    \n",
    "    model.compile(\n",
    "        #optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])*1e-2),\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer']) * 1e-3),\n",
    "        loss = params['loss_function'],\n",
    "        metrics = get_Reduction_Metric(params['reduction_metric'])\n",
    "    )\n",
    "    print('Model was compiled')\n",
    "    print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = _LOG_DIR + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        monitor =  params['monitor_value'],\n",
    "        verbose = 1,\n",
    "        save_weights_only = False,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    print('Checkpointer was created')\n",
    "    \n",
    "    csv_logger = callbacks.CSVLogger(\n",
    "        filename = _LOG_DIR + 'CNN_Base_{}_Logger_{}.csv'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        separator = ',',\n",
    "        append = False\n",
    "    )\n",
    "    print('CSV Logger was created')\n",
    "\n",
    "    lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 13,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        min_delta = 0.0001\n",
    "    )\n",
    "    print('Learning Rate Reducer was created')\n",
    "    \n",
    "    early_stopper = callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0,\n",
    "        #patience = 15,\n",
    "        patience = 20,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    print('Early Stopper was created')\n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_data = valid_generator,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        callbacks = [checkpointer, csv_logger, lr_reducer, early_stopper],\n",
    "        epochs = params['epochs'],\n",
    "        workers = 8\n",
    "    )\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feinoptimierung <a name = \"setup\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "global_hyper_parameter = {\n",
    "    'samples': None,\n",
    "    'epochs': None,\n",
    "    'batch_size': None,\n",
    "    'optimizer': None,\n",
    "    'lr': None,\n",
    "    'first_neuron': None,\n",
    "    'dropout': None,\n",
    "    'activation': None,\n",
    "    'leaky_alpha': None,\n",
    "    'hidden_layers': None,\n",
    "    # beginning from here, Values should only contain one single entry:\n",
    "    # ===============================================================\n",
    "    'label_type': ['Angular'], # Stereographic, Angular, Normalized\n",
    "    'loss_function': None,\n",
    "    'reduction_metric': None,\n",
    "    'monitor_value': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RUN = 'SYNTH'\n",
    "_LOSS = 'MSE'\n",
    "_DATASET_NAME = '2020-05-28'\n",
    "_DEVICE = 'TITAN_GPU0'\n",
    "\n",
    "storage = OutputDirectory.SSD # 'fast_output' if ssd storage may suffice, 'output' otherwise\n",
    "\n",
    "if global_hyper_parameter['label_type'][0] == 'Stereographic':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_stereographic.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_stereographic.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Angular':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Normalized':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_normalized.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_normalized.csv'\n",
    "    \n",
    "else:\n",
    "    assert(True, 'Label Type Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = TrainingSet.SYNTHETIC\n",
    "_USE_DATA_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\fast_output\\SYNTH_Regression_MSE_RESNET\\2020-05-28_Angular_Base_Custom-MAE\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n"
     ]
    }
   ],
   "source": [
    "_IMAGE_DIR = '..\\\\dataset_mm\\\\{}\\\\'.format(_DATASET_NAME)\n",
    "_CSV_FILE = _IMAGE_DIR + _CSV_SYNTH_FILE_NAME\n",
    "_CSV_FILE_REAL = _IMAGE_DIR + _CSV_REAL_FILE_NAME\n",
    "\n",
    "_note = '_Custom-MAE'\n",
    "_NET = 'RESNET' # RESNET vs VGG16\n",
    "\n",
    "_MODEL_DIR = '..\\\\output\\\\{}_Regression_{}_{}\\\\{}_{}_Base{}\\\\'.format(_RUN, _LOSS, _NET, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "_NET_DIR = '{}_Regression_{}_{}\\\\{}_{}_Base{}\\\\'.format(_RUN, _LOSS, _NET, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "\n",
    "_LOG_DIR = '..\\\\{}\\\\{}'.format(output_path[storage], _NET_DIR)\n",
    "\n",
    "if(not os.path.exists(_LOG_DIR)):\n",
    "    os.makedirs(_LOG_DIR)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(_LOG_DIR))\n",
    "\n",
    "device_file = open(_LOG_DIR + '{}.txt'.format(_DEVICE), \"a+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 FC-Gewichte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: ..\\output\\SYNTH_Regression_MSE_RESNET\\2020-05-28_Angular_Base_Custom-MAE\\..\\2020-05-28_Angular_Base_Custom-MAE_Results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>loss</th>\n",
       "      <th>custom_mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_custom_mae</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>label_type</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>lr</th>\n",
       "      <th>monitor_value</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>reduction_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>12/12/20-081648</td>\n",
       "      <td>12/12/20-081937</td>\n",
       "      <td>168.191500</td>\n",
       "      <td>4647.978516</td>\n",
       "      <td>46.865284</td>\n",
       "      <td>3724.097412</td>\n",
       "      <td>39.188641</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>286</td>\n",
       "      <td>12/12/20-081115</td>\n",
       "      <td>12/12/20-081401</td>\n",
       "      <td>165.589999</td>\n",
       "      <td>4627.437988</td>\n",
       "      <td>45.692707</td>\n",
       "      <td>3793.402100</td>\n",
       "      <td>39.276451</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>12/12/20-014413</td>\n",
       "      <td>12/12/20-015416</td>\n",
       "      <td>603.198499</td>\n",
       "      <td>4908.233398</td>\n",
       "      <td>47.927132</td>\n",
       "      <td>3702.308105</td>\n",
       "      <td>39.294670</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>12/11/20-225310</td>\n",
       "      <td>12/11/20-225551</td>\n",
       "      <td>160.720944</td>\n",
       "      <td>4373.488281</td>\n",
       "      <td>44.243889</td>\n",
       "      <td>3819.536865</td>\n",
       "      <td>39.377956</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>12/11/20-184830</td>\n",
       "      <td>12/11/20-185104</td>\n",
       "      <td>154.061001</td>\n",
       "      <td>4945.546387</td>\n",
       "      <td>48.627647</td>\n",
       "      <td>3764.579590</td>\n",
       "      <td>39.679089</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>12/11/20-194935</td>\n",
       "      <td>12/11/20-195441</td>\n",
       "      <td>305.826001</td>\n",
       "      <td>5659.679199</td>\n",
       "      <td>50.415417</td>\n",
       "      <td>3736.920410</td>\n",
       "      <td>39.830891</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "      <td>12/12/20-081938</td>\n",
       "      <td>12/12/20-082226</td>\n",
       "      <td>168.551000</td>\n",
       "      <td>5438.338379</td>\n",
       "      <td>49.753960</td>\n",
       "      <td>3782.959961</td>\n",
       "      <td>39.910362</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>12/11/20-230048</td>\n",
       "      <td>12/11/20-230317</td>\n",
       "      <td>148.510997</td>\n",
       "      <td>4613.883301</td>\n",
       "      <td>46.454300</td>\n",
       "      <td>3823.207520</td>\n",
       "      <td>39.912418</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>12/11/20-114104</td>\n",
       "      <td>12/11/20-115036</td>\n",
       "      <td>572.545962</td>\n",
       "      <td>11078.558594</td>\n",
       "      <td>56.172352</td>\n",
       "      <td>3819.126709</td>\n",
       "      <td>39.920837</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>5</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>12/11/20-235514</td>\n",
       "      <td>12/12/20-000010</td>\n",
       "      <td>295.947999</td>\n",
       "      <td>14867.835938</td>\n",
       "      <td>53.200043</td>\n",
       "      <td>3759.782715</td>\n",
       "      <td>40.192142</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>5</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0            start              end    duration          loss  \\\n",
       "288         288  12/12/20-081648  12/12/20-081937  168.191500   4647.978516   \n",
       "286         286  12/12/20-081115  12/12/20-081401  165.589999   4627.437988   \n",
       "216         216  12/12/20-014413  12/12/20-015416  603.198499   4908.233398   \n",
       "180         180  12/11/20-225310  12/11/20-225551  160.720944   4373.488281   \n",
       "108         108  12/11/20-184830  12/11/20-185104  154.061001   4945.546387   \n",
       "126         126  12/11/20-194935  12/11/20-195441  305.826001   5659.679199   \n",
       "289         289  12/12/20-081938  12/12/20-082226  168.551000   5438.338379   \n",
       "183         183  12/11/20-230048  12/11/20-230317  148.510997   4613.883301   \n",
       "32           32  12/11/20-114104  12/11/20-115036  572.545962  11078.558594   \n",
       "200         200  12/11/20-235514  12/12/20-000010  295.947999  14867.835938   \n",
       "\n",
       "     custom_mae     val_loss  val_custom_mae activation  batch_size  dropout  \\\n",
       "288   46.865284  3724.097412       39.188641       relu          64     0.25   \n",
       "286   45.692707  3793.402100       39.276451       relu          64     0.25   \n",
       "216   47.927132  3702.308105       39.294670       relu          32     0.25   \n",
       "180   44.243889  3819.536865       39.377956       relu          32     0.25   \n",
       "108   48.627647  3764.579590       39.679089  leakyrelu          64     0.25   \n",
       "126   50.415417  3736.920410       39.830891  leakyrelu          64     0.25   \n",
       "289   49.753960  3782.959961       39.910362       relu          64     0.25   \n",
       "183   46.454300  3823.207520       39.912418       relu          32     0.25   \n",
       "32    56.172352  3819.126709       39.920837  leakyrelu          32     0.25   \n",
       "200   53.200043  3759.782715       40.192142       relu          32     0.25   \n",
       "\n",
       "     first_neuron  hidden_layers label_type       loss_function  lr  \\\n",
       "288          2048              1    Angular  mean_squared_error   1   \n",
       "286          2048              0    Angular  mean_squared_error   2   \n",
       "216          4096              2    Angular  mean_squared_error   1   \n",
       "180          1024              0    Angular  mean_squared_error   1   \n",
       "108          2048              1    Angular  mean_squared_error   1   \n",
       "126          4096              2    Angular  mean_squared_error   1   \n",
       "289          2048              1    Angular  mean_squared_error   2   \n",
       "183          1024              1    Angular  mean_squared_error   1   \n",
       "32           4096              0    Angular  mean_squared_error   5   \n",
       "200          2048              1    Angular  mean_squared_error   5   \n",
       "\n",
       "      monitor_value                                          optimizer  \\\n",
       "288  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "286  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "216  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "180  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "108  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "126  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "289  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "183  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "32   val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "200  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "\n",
       "    reduction_metric  \n",
       "288       custom_mae  \n",
       "286       custom_mae  \n",
       "216       custom_mae  \n",
       "180       custom_mae  \n",
       "108       custom_mae  \n",
       "126       custom_mae  \n",
       "289       custom_mae  \n",
       "183       custom_mae  \n",
       "32        custom_mae  \n",
       "200       custom_mae  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_results = _MODEL_DIR + '..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "df = pd.read_csv(base_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "sort_value = df['monitor_value'][0]\n",
    "df = df.sort_values(sort_value, axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(base_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(top_results_index):\n",
    "    \n",
    "    #     Adam = RMSprop + Momentum (lr=0.001)\n",
    "    #     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "    #     RMSprop = (lr=0.001)\n",
    "    #     SGD = (lr=0.01)\n",
    "    #     Adagrad\n",
    "\n",
    "    hyper_parameter = global_hyper_parameter\n",
    "\n",
    "    hyper_parameter['samples'] = [100000]\n",
    "    hyper_parameter['epochs'] = [400]\n",
    "    hyper_parameter['batch_size'] = [df.iloc[top_results_index]['batch_size']]\n",
    "    hyper_parameter['optimizer'] = [make_optimizer(df.loc[top_results_index]['optimizer'])]\n",
    "    hyper_parameter['lr'] = [df.iloc[top_results_index]['lr']]\n",
    "    hyper_parameter['first_neuron'] = [df.iloc[top_results_index]['first_neuron']]\n",
    "    hyper_parameter['dropout'] = [df.iloc[top_results_index]['dropout']]\n",
    "    hyper_parameter['activation'] = [df.iloc[top_results_index]['activation']]\n",
    "    hyper_parameter['leaky_alpha'] = [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "    hyper_parameter['hidden_layers'] = [df.iloc[top_results_index]['hidden_layers']]\n",
    "    \n",
    "    hyper_parameter['loss_function'] = [df.iloc[top_results_index]['loss_function']]\n",
    "    hyper_parameter['reduction_metric'] = [df.iloc[top_results_index]['reduction_metric']]\n",
    "    hyper_parameter['monitor_value'] = [df.iloc[top_results_index]['monitor_value']]\n",
    "\n",
    "    return hyper_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 64, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 2048, 'hidden_layers': 1, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "Y-Col: ['Elevation', 'Azimuth']\n",
      "Train Data Generator: Found 80000 validated image filenames.\n",
      "Validation Data Generator: Found 19968 validated image filenames.\n",
      "Steps per Epoch: 1250, Validation Steps: 312\n",
      "_________________________________________________________________\n",
      "   Network Layer        Trainable\n",
      "=================================================================\n",
      "         input_1                0\n",
      "       conv1_pad                0\n",
      "      conv1_conv                0\n",
      "        conv1_bn                0\n",
      "      conv1_relu                0\n",
      "       pool1_pad                0\n",
      "      pool1_pool                0\n",
      "conv2_block1_1_conv                0\n",
      "conv2_block1_1_bn                0\n",
      "conv2_block1_1_relu                0\n",
      "conv2_block1_2_conv                0\n",
      "conv2_block1_2_bn                0\n",
      "conv2_block1_2_relu                0\n",
      "conv2_block1_0_conv                0\n",
      "conv2_block1_3_conv                0\n",
      "conv2_block1_0_bn                1\n",
      "conv2_block1_3_bn                1\n",
      "conv2_block1_add                1\n",
      "conv2_block1_out                1\n",
      "conv2_block2_1_conv                1\n",
      "conv2_block2_1_bn                1\n",
      "conv2_block2_1_relu                1\n",
      "conv2_block2_2_conv                1\n",
      "conv2_block2_2_bn                1\n",
      "conv2_block2_2_relu                1\n",
      "conv2_block2_3_conv                1\n",
      "conv2_block2_3_bn                1\n",
      "conv2_block2_add                1\n",
      "conv2_block2_out                1\n",
      "conv2_block3_1_conv                1\n",
      "conv2_block3_1_bn                1\n",
      "conv2_block3_1_relu                1\n",
      "conv2_block3_2_conv                1\n",
      "conv2_block3_2_bn                1\n",
      "conv2_block3_2_relu                1\n",
      "conv2_block3_3_conv                1\n",
      "conv2_block3_3_bn                1\n",
      "conv2_block3_add                1\n",
      "conv2_block3_out                1\n",
      "conv3_block1_1_conv                1\n",
      "conv3_block1_1_bn                1\n",
      "conv3_block1_1_relu                1\n",
      "conv3_block1_2_conv                1\n",
      "conv3_block1_2_bn                1\n",
      "conv3_block1_2_relu                1\n",
      "conv3_block1_0_conv                1\n",
      "conv3_block1_3_conv                1\n",
      "conv3_block1_0_bn                1\n",
      "conv3_block1_3_bn                1\n",
      "conv3_block1_add                1\n",
      "conv3_block1_out                1\n",
      "conv3_block2_1_conv                1\n",
      "conv3_block2_1_bn                1\n",
      "conv3_block2_1_relu                1\n",
      "conv3_block2_2_conv                1\n",
      "conv3_block2_2_bn                1\n",
      "conv3_block2_2_relu                1\n",
      "conv3_block2_3_conv                1\n",
      "conv3_block2_3_bn                1\n",
      "conv3_block2_add                1\n",
      "conv3_block2_out                1\n",
      "conv3_block3_1_conv                1\n",
      "conv3_block3_1_bn                1\n",
      "conv3_block3_1_relu                1\n",
      "conv3_block3_2_conv                1\n",
      "conv3_block3_2_bn                1\n",
      "conv3_block3_2_relu                1\n",
      "conv3_block3_3_conv                1\n",
      "conv3_block3_3_bn                1\n",
      "conv3_block3_add                1\n",
      "conv3_block3_out                1\n",
      "conv3_block4_1_conv                1\n",
      "conv3_block4_1_bn                1\n",
      "conv3_block4_1_relu                1\n",
      "conv3_block4_2_conv                1\n",
      "conv3_block4_2_bn                1\n",
      "conv3_block4_2_relu                1\n",
      "conv3_block4_3_conv                1\n",
      "conv3_block4_3_bn                1\n",
      "conv3_block4_add                1\n",
      "conv3_block4_out                1\n",
      "conv4_block1_1_conv                1\n",
      "conv4_block1_1_bn                1\n",
      "conv4_block1_1_relu                1\n",
      "conv4_block1_2_conv                1\n",
      "conv4_block1_2_bn                1\n",
      "conv4_block1_2_relu                1\n",
      "conv4_block1_0_conv                1\n",
      "conv4_block1_3_conv                1\n",
      "conv4_block1_0_bn                1\n",
      "conv4_block1_3_bn                1\n",
      "conv4_block1_add                1\n",
      "conv4_block1_out                1\n",
      "conv4_block2_1_conv                1\n",
      "conv4_block2_1_bn                1\n",
      "conv4_block2_1_relu                1\n",
      "conv4_block2_2_conv                1\n",
      "conv4_block2_2_bn                1\n",
      "conv4_block2_2_relu                1\n",
      "conv4_block2_3_conv                1\n",
      "conv4_block2_3_bn                1\n",
      "conv4_block2_add                1\n",
      "conv4_block2_out                1\n",
      "conv4_block3_1_conv                1\n",
      "conv4_block3_1_bn                1\n",
      "conv4_block3_1_relu                1\n",
      "conv4_block3_2_conv                1\n",
      "conv4_block3_2_bn                1\n",
      "conv4_block3_2_relu                1\n",
      "conv4_block3_3_conv                1\n",
      "conv4_block3_3_bn                1\n",
      "conv4_block3_add                1\n",
      "conv4_block3_out                1\n",
      "conv4_block4_1_conv                1\n",
      "conv4_block4_1_bn                1\n",
      "conv4_block4_1_relu                1\n",
      "conv4_block4_2_conv                1\n",
      "conv4_block4_2_bn                1\n",
      "conv4_block4_2_relu                1\n",
      "conv4_block4_3_conv                1\n",
      "conv4_block4_3_bn                1\n",
      "conv4_block4_add                1\n",
      "conv4_block4_out                1\n",
      "conv4_block5_1_conv                1\n",
      "conv4_block5_1_bn                1\n",
      "conv4_block5_1_relu                1\n",
      "conv4_block5_2_conv                1\n",
      "conv4_block5_2_bn                1\n",
      "conv4_block5_2_relu                1\n",
      "conv4_block5_3_conv                1\n",
      "conv4_block5_3_bn                1\n",
      "conv4_block5_add                1\n",
      "conv4_block5_out                1\n",
      "conv4_block6_1_conv                1\n",
      "conv4_block6_1_bn                1\n",
      "conv4_block6_1_relu                1\n",
      "conv4_block6_2_conv                1\n",
      "conv4_block6_2_bn                1\n",
      "conv4_block6_2_relu                1\n",
      "conv4_block6_3_conv                1\n",
      "conv4_block6_3_bn                1\n",
      "conv4_block6_add                1\n",
      "conv4_block6_out                1\n",
      "conv5_block1_1_conv                1\n",
      "conv5_block1_1_bn                1\n",
      "conv5_block1_1_relu                1\n",
      "conv5_block1_2_conv                1\n",
      "conv5_block1_2_bn                1\n",
      "conv5_block1_2_relu                1\n",
      "conv5_block1_0_conv                1\n",
      "conv5_block1_3_conv                1\n",
      "conv5_block1_0_bn                1\n",
      "conv5_block1_3_bn                1\n",
      "conv5_block1_add                1\n",
      "conv5_block1_out                1\n",
      "conv5_block2_1_conv                1\n",
      "conv5_block2_1_bn                1\n",
      "conv5_block2_1_relu                1\n",
      "conv5_block2_2_conv                1\n",
      "conv5_block2_2_bn                1\n",
      "conv5_block2_2_relu                1\n",
      "conv5_block2_3_conv                1\n",
      "conv5_block2_3_bn                1\n",
      "conv5_block2_add                1\n",
      "conv5_block2_out                1\n",
      "conv5_block3_1_conv                1\n",
      "conv5_block3_1_bn                1\n",
      "conv5_block3_1_relu                1\n",
      "conv5_block3_2_conv                1\n",
      "conv5_block3_2_bn                1\n",
      "conv5_block3_2_relu                1\n",
      "conv5_block3_3_conv                1\n",
      "conv5_block3_3_bn                1\n",
      "conv5_block3_add                1\n",
      "conv5_block3_out                1\n",
      "_________________________________________________________________\n",
      "\n",
      "Number Hidden Layers 1\n",
      "Fully Connected Layers added to Base Network\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000200B698B5E8>]\n",
      "Model was compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 2)                 207623170 \n",
      "=================================================================\n",
      "Total params: 231,210,882\n",
      "Trainable params: 231,073,538\n",
      "Non-trainable params: 137,344\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Checkpointer was created\n",
      "CSV Logger was created\n",
      "Learning Rate Reducer was created\n",
      "Early Stopper was created\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 146/1250 [==>...........................] - ETA: 0s - loss: 25902.6406 - custom_mae: 116.73 - ETA: 2:29:56 - loss: 23711.3926 - custom_mae: 113.522 - ETA: 3:18:35 - loss: 24736.6250 - custom_mae: 115.876 - ETA: 3:43:05 - loss: 24939.0156 - custom_mae: 116.754 - ETA: 3:57:30 - loss: 24461.8711 - custom_mae: 115.096 - ETA: 4:07:33 - loss: 24171.3262 - custom_mae: 114.224 - ETA: 4:14:15 - loss: 24365.7949 - custom_mae: 115.172 - ETA: 4:19:36 - loss: 24207.3184 - custom_mae: 114.685 - ETA: 4:23:41 - loss: 24293.0098 - custom_mae: 114.871 - ETA: 4:27:05 - loss: 24115.3086 - custom_mae: 114.497 - ETA: 4:29:43 - loss: 24167.1543 - custom_mae: 114.520 - ETA: 4:32:04 - loss: 24107.9395 - custom_mae: 113.943 - ETA: 4:33:55 - loss: 24232.0195 - custom_mae: 114.002 - ETA: 4:35:16 - loss: 24384.0117 - custom_mae: 114.349 - ETA: 4:36:36 - loss: 24553.7910 - custom_mae: 114.687 - ETA: 4:37:36 - loss: 24676.8086 - custom_mae: 115.130 - ETA: 4:38:35 - loss: 24581.5039 - custom_mae: 114.800 - ETA: 4:39:13 - loss: 24611.8711 - custom_mae: 114.718 - ETA: 4:40:00 - loss: 24566.3242 - custom_mae: 114.525 - ETA: 4:40:34 - loss: 24380.6836 - custom_mae: 113.863 - ETA: 4:41:14 - loss: 24224.9062 - custom_mae: 113.257 - ETA: 4:41:36 - loss: 24201.9863 - custom_mae: 113.210 - ETA: 4:41:52 - loss: 24204.8281 - custom_mae: 113.269 - ETA: 4:42:15 - loss: 24101.7363 - custom_mae: 112.922 - ETA: 4:42:29 - loss: 24136.9180 - custom_mae: 112.981 - ETA: 4:42:43 - loss: 24015.6016 - custom_mae: 112.656 - ETA: 4:42:50 - loss: 24047.7871 - custom_mae: 112.588 - ETA: 4:43:06 - loss: 24044.4277 - custom_mae: 112.505 - ETA: 4:43:09 - loss: 24077.1465 - custom_mae: 112.492 - ETA: 4:43:17 - loss: 24111.4980 - custom_mae: 112.546 - ETA: 4:43:24 - loss: 24101.4922 - custom_mae: 112.483 - ETA: 4:43:35 - loss: 23920.2754 - custom_mae: 111.895 - ETA: 4:43:37 - loss: 23765.6016 - custom_mae: 111.430 - ETA: 4:43:38 - loss: 23856.6094 - custom_mae: 111.509 - ETA: 4:43:43 - loss: 23758.4316 - custom_mae: 111.225 - ETA: 4:43:39 - loss: 23718.4590 - custom_mae: 111.103 - ETA: 4:43:39 - loss: 23726.6699 - custom_mae: 111.115 - ETA: 4:43:36 - loss: 23625.0059 - custom_mae: 110.792 - ETA: 4:43:40 - loss: 23605.7363 - custom_mae: 110.706 - ETA: 4:43:35 - loss: 23524.4648 - custom_mae: 110.487 - ETA: 4:43:34 - loss: 23485.4609 - custom_mae: 110.288 - ETA: 4:43:30 - loss: 23484.1934 - custom_mae: 110.222 - ETA: 4:43:26 - loss: 23438.4805 - custom_mae: 110.027 - ETA: 4:43:18 - loss: 23413.3555 - custom_mae: 109.874 - ETA: 4:43:15 - loss: 23392.5918 - custom_mae: 109.775 - ETA: 4:43:09 - loss: 23303.0117 - custom_mae: 109.524 - ETA: 4:43:02 - loss: 23308.3516 - custom_mae: 109.506 - ETA: 4:42:59 - loss: 23258.3379 - custom_mae: 109.360 - ETA: 4:42:49 - loss: 23192.6094 - custom_mae: 109.166 - ETA: 4:42:41 - loss: 23174.0723 - custom_mae: 109.116 - ETA: 4:42:36 - loss: 23169.5977 - custom_mae: 109.097 - ETA: 4:42:28 - loss: 23179.8516 - custom_mae: 109.107 - ETA: 4:42:22 - loss: 23215.4453 - custom_mae: 109.046 - ETA: 4:42:17 - loss: 23218.3164 - custom_mae: 109.006 - ETA: 4:42:09 - loss: 23214.0273 - custom_mae: 108.946 - ETA: 4:42:05 - loss: 23117.3398 - custom_mae: 108.640 - ETA: 4:41:56 - loss: 23115.8535 - custom_mae: 108.613 - ETA: 4:41:52 - loss: 23098.1367 - custom_mae: 108.457 - ETA: 4:41:42 - loss: 23077.5176 - custom_mae: 108.318 - ETA: 4:41:35 - loss: 23024.5039 - custom_mae: 108.168 - ETA: 4:41:24 - loss: 22992.9082 - custom_mae: 108.076 - ETA: 4:41:17 - loss: 22955.8066 - custom_mae: 107.942 - ETA: 4:41:05 - loss: 22902.8398 - custom_mae: 107.813 - ETA: 4:41:00 - loss: 22872.3770 - custom_mae: 107.669 - ETA: 4:40:51 - loss: 22927.9590 - custom_mae: 107.837 - ETA: 4:40:44 - loss: 22882.3320 - custom_mae: 107.695 - ETA: 4:40:35 - loss: 22880.6875 - custom_mae: 107.667 - ETA: 4:40:24 - loss: 22777.4023 - custom_mae: 107.335 - ETA: 4:40:14 - loss: 22793.7637 - custom_mae: 107.333 - ETA: 4:40:02 - loss: 22789.6816 - custom_mae: 107.307 - ETA: 4:39:53 - loss: 22781.3691 - custom_mae: 107.270 - ETA: 4:39:42 - loss: 22726.8320 - custom_mae: 107.133 - ETA: 4:39:33 - loss: 22668.1621 - custom_mae: 106.947 - ETA: 4:39:21 - loss: 22600.9727 - custom_mae: 106.742 - ETA: 4:39:12 - loss: 22579.7188 - custom_mae: 106.689 - ETA: 4:39:00 - loss: 22536.0547 - custom_mae: 106.549 - ETA: 4:38:48 - loss: 22570.5625 - custom_mae: 106.604 - ETA: 4:38:39 - loss: 22550.9238 - custom_mae: 106.507 - ETA: 4:38:28 - loss: 22565.9453 - custom_mae: 106.518 - ETA: 4:38:19 - loss: 22581.9980 - custom_mae: 106.572 - ETA: 4:38:07 - loss: 22531.8008 - custom_mae: 106.352 - ETA: 4:37:56 - loss: 22601.5859 - custom_mae: 106.491 - ETA: 4:37:45 - loss: 22560.4512 - custom_mae: 106.340 - ETA: 4:37:34 - loss: 22556.8789 - custom_mae: 106.320 - ETA: 4:37:21 - loss: 22506.9375 - custom_mae: 106.152 - ETA: 4:37:09 - loss: 22486.3145 - custom_mae: 106.093 - ETA: 4:36:57 - loss: 22495.5703 - custom_mae: 106.075 - ETA: 4:36:45 - loss: 22469.0859 - custom_mae: 105.951 - ETA: 4:36:34 - loss: 22429.0645 - custom_mae: 105.830 - ETA: 4:36:21 - loss: 22406.8867 - custom_mae: 105.715 - ETA: 4:36:09 - loss: 22378.5742 - custom_mae: 105.587 - ETA: 4:35:57 - loss: 22387.5801 - custom_mae: 105.597 - ETA: 4:35:46 - loss: 22366.8984 - custom_mae: 105.539 - ETA: 4:35:33 - loss: 22335.7305 - custom_mae: 105.415 - ETA: 4:35:23 - loss: 22374.3535 - custom_mae: 105.493 - ETA: 4:35:10 - loss: 22351.2109 - custom_mae: 105.409 - ETA: 4:34:58 - loss: 22319.7715 - custom_mae: 105.330 - ETA: 4:34:45 - loss: 22270.9531 - custom_mae: 105.155 - ETA: 4:34:33 - loss: 22268.3438 - custom_mae: 105.106 - ETA: 4:34:20 - loss: 22265.5078 - custom_mae: 105.083 - ETA: 4:34:08 - loss: 22277.1094 - custom_mae: 105.148 - ETA: 4:33:55 - loss: 22294.5645 - custom_mae: 105.179 - ETA: 4:33:43 - loss: 22279.3438 - custom_mae: 105.146 - ETA: 4:33:32 - loss: 22231.1543 - custom_mae: 104.998 - ETA: 4:33:18 - loss: 22250.7090 - custom_mae: 105.074 - ETA: 4:33:05 - loss: 22193.3652 - custom_mae: 104.903 - ETA: 4:32:52 - loss: 22227.1074 - custom_mae: 105.005 - ETA: 4:32:39 - loss: 22223.0391 - custom_mae: 104.978 - ETA: 4:32:27 - loss: 22211.4023 - custom_mae: 104.959 - ETA: 4:32:14 - loss: 22221.7891 - custom_mae: 104.979 - ETA: 4:32:01 - loss: 22194.6875 - custom_mae: 104.879 - ETA: 4:31:47 - loss: 22148.1094 - custom_mae: 104.718 - ETA: 4:31:36 - loss: 22124.6133 - custom_mae: 104.600 - ETA: 4:31:22 - loss: 22131.1855 - custom_mae: 104.611 - ETA: 4:31:10 - loss: 22134.9062 - custom_mae: 104.619 - ETA: 4:30:57 - loss: 22077.3125 - custom_mae: 104.434 - ETA: 4:30:44 - loss: 22039.7305 - custom_mae: 104.329 - ETA: 4:30:32 - loss: 22022.8066 - custom_mae: 104.264 - ETA: 4:30:18 - loss: 21993.0625 - custom_mae: 104.178 - ETA: 4:30:06 - loss: 21967.1113 - custom_mae: 104.114 - ETA: 4:29:53 - loss: 21934.3887 - custom_mae: 104.019 - ETA: 4:29:40 - loss: 21903.6152 - custom_mae: 103.949 - ETA: 4:29:27 - loss: 21868.9902 - custom_mae: 103.843 - ETA: 4:29:14 - loss: 21844.9238 - custom_mae: 103.759 - ETA: 4:29:01 - loss: 21823.0391 - custom_mae: 103.688 - ETA: 4:28:48 - loss: 21827.3105 - custom_mae: 103.697 - ETA: 4:28:36 - loss: 21803.9102 - custom_mae: 103.622 - ETA: 4:28:21 - loss: 21777.3301 - custom_mae: 103.545 - ETA: 4:28:07 - loss: 21752.7246 - custom_mae: 103.470 - ETA: 4:27:54 - loss: 21722.7480 - custom_mae: 103.396 - ETA: 4:27:40 - loss: 21691.2891 - custom_mae: 103.275 - ETA: 4:27:28 - loss: 21688.2305 - custom_mae: 103.269 - ETA: 4:27:14 - loss: 21641.9629 - custom_mae: 103.141 - ETA: 4:27:02 - loss: 21633.2832 - custom_mae: 103.108 - ETA: 4:26:49 - loss: 21637.3809 - custom_mae: 103.099 - ETA: 4:26:36 - loss: 21595.1309 - custom_mae: 102.964 - ETA: 4:26:24 - loss: 21579.8887 - custom_mae: 102.898 - ETA: 4:26:09 - loss: 21549.9922 - custom_mae: 102.804 - ETA: 4:25:56 - loss: 21524.8027 - custom_mae: 102.720 - ETA: 4:25:42 - loss: 21492.9062 - custom_mae: 102.627 - ETA: 4:25:30 - loss: 21484.3633 - custom_mae: 102.589 - ETA: 4:25:17 - loss: 21451.6465 - custom_mae: 102.488 - ETA: 4:25:03 - loss: 21418.0039 - custom_mae: 102.402 - ETA: 4:24:50 - loss: 21410.3320 - custom_mae: 102.373 - ETA: 4:24:35 - loss: 21401.2754 - custom_mae: 102.344 - ETA: 4:24:24 - loss: 21364.1250 - custom_mae: 102.2495"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 296/1250 [======>.......................] - ETA: 4:24:10 - loss: 21350.4648 - custom_mae: 102.201 - ETA: 4:23:58 - loss: 21315.7344 - custom_mae: 102.074 - ETA: 4:23:43 - loss: 21279.3164 - custom_mae: 101.953 - ETA: 4:23:30 - loss: 21248.5312 - custom_mae: 101.859 - ETA: 4:23:16 - loss: 21207.3848 - custom_mae: 101.745 - ETA: 4:23:02 - loss: 21170.6172 - custom_mae: 101.604 - ETA: 4:22:48 - loss: 21137.8535 - custom_mae: 101.509 - ETA: 4:22:34 - loss: 21113.8672 - custom_mae: 101.427 - ETA: 4:22:21 - loss: 21090.0312 - custom_mae: 101.360 - ETA: 4:22:07 - loss: 21073.2285 - custom_mae: 101.313 - ETA: 4:21:54 - loss: 21031.0137 - custom_mae: 101.196 - ETA: 4:21:41 - loss: 21035.6992 - custom_mae: 101.205 - ETA: 4:21:27 - loss: 21008.3984 - custom_mae: 101.127 - ETA: 4:21:13 - loss: 21010.0898 - custom_mae: 101.117 - ETA: 4:20:59 - loss: 20972.9395 - custom_mae: 100.996 - ETA: 4:20:46 - loss: 20956.0469 - custom_mae: 100.937 - ETA: 4:20:32 - loss: 20934.7793 - custom_mae: 100.901 - ETA: 4:20:18 - loss: 20919.6953 - custom_mae: 100.855 - ETA: 4:20:03 - loss: 20906.2090 - custom_mae: 100.792 - ETA: 4:19:49 - loss: 20889.6094 - custom_mae: 100.729 - ETA: 4:19:36 - loss: 20864.2363 - custom_mae: 100.670 - ETA: 4:19:21 - loss: 20836.7070 - custom_mae: 100.600 - ETA: 4:19:07 - loss: 20812.1250 - custom_mae: 100.520 - ETA: 4:18:52 - loss: 20785.6270 - custom_mae: 100.418 - ETA: 4:18:38 - loss: 20742.5312 - custom_mae: 100.287 - ETA: 4:18:25 - loss: 20731.5742 - custom_mae: 100.247 - ETA: 4:18:11 - loss: 20704.5938 - custom_mae: 100.150 - ETA: 4:17:58 - loss: 20684.9980 - custom_mae: 100.081 - ETA: 4:17:44 - loss: 20658.8555 - custom_mae: 99.993 - ETA: 4:17:30 - loss: 20654.3770 - custom_mae: 99.96 - ETA: 4:17:16 - loss: 20632.5566 - custom_mae: 99.87 - ETA: 4:17:03 - loss: 20608.7793 - custom_mae: 99.80 - ETA: 4:16:49 - loss: 20588.4277 - custom_mae: 99.75 - ETA: 4:16:34 - loss: 20559.4824 - custom_mae: 99.65 - ETA: 4:16:21 - loss: 20543.9746 - custom_mae: 99.62 - ETA: 4:16:07 - loss: 20548.4004 - custom_mae: 99.64 - ETA: 4:15:53 - loss: 20516.4609 - custom_mae: 99.55 - ETA: 4:15:39 - loss: 20489.4453 - custom_mae: 99.47 - ETA: 4:15:24 - loss: 20468.2031 - custom_mae: 99.40 - ETA: 4:15:11 - loss: 20460.2129 - custom_mae: 99.38 - ETA: 4:14:57 - loss: 20442.6543 - custom_mae: 99.31 - ETA: 4:14:42 - loss: 20409.0371 - custom_mae: 99.21 - ETA: 4:14:29 - loss: 20381.4395 - custom_mae: 99.13 - ETA: 4:14:15 - loss: 20362.6230 - custom_mae: 99.07 - ETA: 4:14:01 - loss: 20339.6914 - custom_mae: 99.00 - ETA: 4:13:47 - loss: 20333.4355 - custom_mae: 98.99 - ETA: 4:13:34 - loss: 20321.8828 - custom_mae: 98.95 - ETA: 4:13:21 - loss: 20314.9062 - custom_mae: 98.94 - ETA: 4:13:06 - loss: 20292.8652 - custom_mae: 98.88 - ETA: 4:12:53 - loss: 20293.4336 - custom_mae: 98.89 - ETA: 4:12:38 - loss: 20262.5801 - custom_mae: 98.80 - ETA: 4:12:25 - loss: 20241.6953 - custom_mae: 98.73 - ETA: 4:12:10 - loss: 20218.6270 - custom_mae: 98.66 - ETA: 4:11:57 - loss: 20201.3906 - custom_mae: 98.61 - ETA: 4:11:43 - loss: 20181.6152 - custom_mae: 98.56 - ETA: 4:11:29 - loss: 20158.2695 - custom_mae: 98.48 - ETA: 4:11:14 - loss: 20143.7109 - custom_mae: 98.43 - ETA: 4:11:00 - loss: 20114.3340 - custom_mae: 98.34 - ETA: 4:10:47 - loss: 20092.4648 - custom_mae: 98.27 - ETA: 4:10:32 - loss: 20072.3164 - custom_mae: 98.21 - ETA: 4:10:18 - loss: 20073.7988 - custom_mae: 98.21 - ETA: 4:10:04 - loss: 20044.2520 - custom_mae: 98.11 - ETA: 4:09:50 - loss: 20031.5820 - custom_mae: 98.08 - ETA: 4:09:36 - loss: 20005.4980 - custom_mae: 97.99 - ETA: 4:09:22 - loss: 19983.3184 - custom_mae: 97.92 - ETA: 4:09:08 - loss: 19944.3594 - custom_mae: 97.79 - ETA: 4:08:54 - loss: 19925.2891 - custom_mae: 97.73 - ETA: 4:08:39 - loss: 19905.0039 - custom_mae: 97.66 - ETA: 4:08:26 - loss: 19882.5000 - custom_mae: 97.60 - ETA: 4:08:12 - loss: 19853.9414 - custom_mae: 97.52 - ETA: 4:07:58 - loss: 19831.9473 - custom_mae: 97.45 - ETA: 4:07:44 - loss: 19796.9258 - custom_mae: 97.36 - ETA: 4:07:31 - loss: 19775.8789 - custom_mae: 97.31 - ETA: 4:07:17 - loss: 19754.6699 - custom_mae: 97.23 - ETA: 4:07:03 - loss: 19732.0605 - custom_mae: 97.17 - ETA: 4:06:49 - loss: 19711.1113 - custom_mae: 97.11 - ETA: 4:06:34 - loss: 19696.5469 - custom_mae: 97.08 - ETA: 4:06:20 - loss: 19669.8281 - custom_mae: 97.00 - ETA: 4:06:06 - loss: 19636.4746 - custom_mae: 96.88 - ETA: 4:05:52 - loss: 19605.5273 - custom_mae: 96.79 - ETA: 4:05:38 - loss: 19579.6699 - custom_mae: 96.70 - ETA: 4:05:24 - loss: 19552.1406 - custom_mae: 96.61 - ETA: 4:05:10 - loss: 19540.7656 - custom_mae: 96.58 - ETA: 4:04:55 - loss: 19513.1230 - custom_mae: 96.48 - ETA: 4:04:42 - loss: 19494.5195 - custom_mae: 96.42 - ETA: 4:04:27 - loss: 19477.1523 - custom_mae: 96.38 - ETA: 4:04:13 - loss: 19458.1055 - custom_mae: 96.34 - ETA: 4:03:58 - loss: 19450.0020 - custom_mae: 96.32 - ETA: 4:03:44 - loss: 19429.6934 - custom_mae: 96.26 - ETA: 4:03:30 - loss: 19398.9590 - custom_mae: 96.17 - ETA: 4:03:16 - loss: 19365.5801 - custom_mae: 96.05 - ETA: 4:03:02 - loss: 19348.9570 - custom_mae: 95.99 - ETA: 4:02:47 - loss: 19321.9062 - custom_mae: 95.91 - ETA: 4:02:33 - loss: 19296.6660 - custom_mae: 95.83 - ETA: 4:02:19 - loss: 19272.1895 - custom_mae: 95.77 - ETA: 4:02:04 - loss: 19254.9609 - custom_mae: 95.72 - ETA: 4:01:51 - loss: 19228.8457 - custom_mae: 95.64 - ETA: 4:01:36 - loss: 19211.2402 - custom_mae: 95.58 - ETA: 4:01:22 - loss: 19174.1387 - custom_mae: 95.46 - ETA: 4:01:08 - loss: 19142.7949 - custom_mae: 95.36 - ETA: 4:00:54 - loss: 19136.3379 - custom_mae: 95.33 - ETA: 4:00:39 - loss: 19108.4883 - custom_mae: 95.24 - ETA: 4:00:25 - loss: 19095.8770 - custom_mae: 95.21 - ETA: 4:00:11 - loss: 19082.4961 - custom_mae: 95.17 - ETA: 3:59:57 - loss: 19058.9688 - custom_mae: 95.10 - ETA: 3:59:43 - loss: 19030.8574 - custom_mae: 95.01 - ETA: 3:59:29 - loss: 19025.6777 - custom_mae: 95.00 - ETA: 3:59:15 - loss: 19001.2949 - custom_mae: 94.93 - ETA: 3:59:00 - loss: 18981.7129 - custom_mae: 94.87 - ETA: 3:58:46 - loss: 18955.5488 - custom_mae: 94.79 - ETA: 3:58:32 - loss: 18933.3750 - custom_mae: 94.72 - ETA: 3:58:18 - loss: 18899.9219 - custom_mae: 94.61 - ETA: 3:58:04 - loss: 18873.0664 - custom_mae: 94.53 - ETA: 3:57:50 - loss: 18844.0977 - custom_mae: 94.44 - ETA: 3:57:36 - loss: 18810.1172 - custom_mae: 94.34 - ETA: 3:57:22 - loss: 18790.5742 - custom_mae: 94.28 - ETA: 3:57:08 - loss: 18757.0820 - custom_mae: 94.17 - ETA: 3:56:54 - loss: 18721.7227 - custom_mae: 94.06 - ETA: 3:56:40 - loss: 18700.3203 - custom_mae: 94.00 - ETA: 3:56:26 - loss: 18681.7227 - custom_mae: 93.95 - ETA: 3:56:11 - loss: 18669.0332 - custom_mae: 93.91 - ETA: 3:55:57 - loss: 18638.0684 - custom_mae: 93.82 - ETA: 3:55:42 - loss: 18613.3945 - custom_mae: 93.75 - ETA: 3:55:28 - loss: 18585.7344 - custom_mae: 93.66 - ETA: 3:55:14 - loss: 18558.9258 - custom_mae: 93.57 - ETA: 3:54:59 - loss: 18542.5176 - custom_mae: 93.52 - ETA: 3:54:45 - loss: 18514.7090 - custom_mae: 93.44 - ETA: 3:54:31 - loss: 18495.6797 - custom_mae: 93.38 - ETA: 3:54:17 - loss: 18477.6875 - custom_mae: 93.32 - ETA: 3:54:02 - loss: 18452.7285 - custom_mae: 93.25 - ETA: 3:53:48 - loss: 18426.3281 - custom_mae: 93.17 - ETA: 3:53:34 - loss: 18397.8379 - custom_mae: 93.09 - ETA: 3:53:19 - loss: 18375.7520 - custom_mae: 93.02 - ETA: 3:53:06 - loss: 18345.5391 - custom_mae: 92.93 - ETA: 3:52:51 - loss: 18327.8926 - custom_mae: 92.87 - ETA: 3:52:38 - loss: 18311.5293 - custom_mae: 92.84 - ETA: 3:52:23 - loss: 18283.2285 - custom_mae: 92.75 - ETA: 3:52:09 - loss: 18262.8301 - custom_mae: 92.69 - ETA: 3:51:55 - loss: 18239.0117 - custom_mae: 92.62 - ETA: 3:51:40 - loss: 18223.8926 - custom_mae: 92.58 - ETA: 3:51:26 - loss: 18202.6836 - custom_mae: 92.52 - ETA: 3:51:11 - loss: 18172.4824 - custom_mae: 92.42 - ETA: 3:50:57 - loss: 18147.6172 - custom_mae: 92.35 - ETA: 3:50:43 - loss: 18125.0996 - custom_mae: 92.28 - ETA: 3:50:29 - loss: 18104.5723 - custom_mae: 92.22 - ETA: 3:50:15 - loss: 18080.5820 - custom_mae: 92.14 - ETA: 3:50:01 - loss: 18057.3574 - custom_mae: 92.08 - ETA: 3:49:46 - loss: 18029.7422 - custom_mae: 91.99 - ETA: 3:49:32 - loss: 18001.7637 - custom_mae: 91.90 - ETA: 3:49:17 - loss: 17976.7383 - custom_mae: 91.8293"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 447/1250 [=========>....................] - ETA: 3:49:03 - loss: 17948.1934 - custom_mae: 91.74 - ETA: 3:48:48 - loss: 17923.8184 - custom_mae: 91.66 - ETA: 3:48:34 - loss: 17904.0996 - custom_mae: 91.61 - ETA: 3:48:20 - loss: 17879.1230 - custom_mae: 91.54 - ETA: 3:48:05 - loss: 17857.5703 - custom_mae: 91.48 - ETA: 3:47:51 - loss: 17825.8867 - custom_mae: 91.39 - ETA: 3:47:37 - loss: 17807.3848 - custom_mae: 91.33 - ETA: 3:47:23 - loss: 17783.0430 - custom_mae: 91.26 - ETA: 3:47:08 - loss: 17760.5273 - custom_mae: 91.20 - ETA: 3:46:54 - loss: 17740.1895 - custom_mae: 91.13 - ETA: 3:46:40 - loss: 17713.6328 - custom_mae: 91.05 - ETA: 3:46:26 - loss: 17690.1797 - custom_mae: 90.99 - ETA: 3:46:11 - loss: 17661.0938 - custom_mae: 90.90 - ETA: 3:45:57 - loss: 17644.1191 - custom_mae: 90.85 - ETA: 3:45:42 - loss: 17621.4863 - custom_mae: 90.79 - ETA: 3:45:28 - loss: 17592.3789 - custom_mae: 90.70 - ETA: 3:45:13 - loss: 17564.6113 - custom_mae: 90.60 - ETA: 3:44:59 - loss: 17536.8711 - custom_mae: 90.51 - ETA: 3:44:45 - loss: 17512.3516 - custom_mae: 90.43 - ETA: 3:44:30 - loss: 17489.7871 - custom_mae: 90.37 - ETA: 3:44:16 - loss: 17463.2754 - custom_mae: 90.29 - ETA: 3:44:01 - loss: 17445.4551 - custom_mae: 90.24 - ETA: 3:43:47 - loss: 17425.6289 - custom_mae: 90.19 - ETA: 3:43:33 - loss: 17403.6133 - custom_mae: 90.12 - ETA: 3:43:18 - loss: 17383.1270 - custom_mae: 90.06 - ETA: 3:43:04 - loss: 17354.2676 - custom_mae: 89.97 - ETA: 3:42:50 - loss: 17332.1172 - custom_mae: 89.91 - ETA: 3:42:36 - loss: 17309.4160 - custom_mae: 89.84 - ETA: 3:42:22 - loss: 17288.0977 - custom_mae: 89.78 - ETA: 3:42:08 - loss: 17270.5039 - custom_mae: 89.74 - ETA: 3:41:53 - loss: 17243.1309 - custom_mae: 89.66 - ETA: 3:41:39 - loss: 17221.1016 - custom_mae: 89.59 - ETA: 3:41:25 - loss: 17195.4414 - custom_mae: 89.51 - ETA: 3:41:10 - loss: 17168.9746 - custom_mae: 89.43 - ETA: 3:40:56 - loss: 17138.4961 - custom_mae: 89.33 - ETA: 3:40:42 - loss: 17114.5352 - custom_mae: 89.26 - ETA: 3:40:27 - loss: 17094.9766 - custom_mae: 89.22 - ETA: 3:40:13 - loss: 17073.9707 - custom_mae: 89.16 - ETA: 3:39:58 - loss: 17048.6074 - custom_mae: 89.08 - ETA: 3:39:44 - loss: 17026.2148 - custom_mae: 89.02 - ETA: 3:39:29 - loss: 16997.7695 - custom_mae: 88.94 - ETA: 3:39:15 - loss: 16978.1055 - custom_mae: 88.88 - ETA: 3:39:01 - loss: 16951.5820 - custom_mae: 88.80 - ETA: 3:38:46 - loss: 16931.9941 - custom_mae: 88.75 - ETA: 3:38:32 - loss: 16910.2441 - custom_mae: 88.68 - ETA: 3:38:18 - loss: 16886.2305 - custom_mae: 88.61 - ETA: 3:38:04 - loss: 16859.3535 - custom_mae: 88.52 - ETA: 3:37:49 - loss: 16829.5156 - custom_mae: 88.43 - ETA: 3:37:35 - loss: 16805.7383 - custom_mae: 88.37 - ETA: 3:37:20 - loss: 16779.5410 - custom_mae: 88.29 - ETA: 3:37:06 - loss: 16752.0781 - custom_mae: 88.20 - ETA: 3:36:52 - loss: 16729.9980 - custom_mae: 88.14 - ETA: 3:36:38 - loss: 16708.3633 - custom_mae: 88.08 - ETA: 3:36:23 - loss: 16682.1426 - custom_mae: 88.00 - ETA: 3:36:09 - loss: 16661.2871 - custom_mae: 87.94 - ETA: 3:35:55 - loss: 16635.5625 - custom_mae: 87.86 - ETA: 3:35:40 - loss: 16614.5430 - custom_mae: 87.81 - ETA: 3:35:26 - loss: 16589.3809 - custom_mae: 87.72 - ETA: 3:35:12 - loss: 16568.8047 - custom_mae: 87.67 - ETA: 3:34:58 - loss: 16544.7773 - custom_mae: 87.60 - ETA: 3:34:43 - loss: 16518.6543 - custom_mae: 87.53 - ETA: 3:34:29 - loss: 16493.3125 - custom_mae: 87.45 - ETA: 3:34:15 - loss: 16466.1602 - custom_mae: 87.37 - ETA: 3:34:00 - loss: 16437.9512 - custom_mae: 87.28 - ETA: 3:33:46 - loss: 16412.6426 - custom_mae: 87.21 - ETA: 3:33:32 - loss: 16388.6035 - custom_mae: 87.14 - ETA: 3:33:18 - loss: 16361.6104 - custom_mae: 87.05 - ETA: 3:33:04 - loss: 16336.0957 - custom_mae: 86.97 - ETA: 3:32:49 - loss: 16313.8496 - custom_mae: 86.90 - ETA: 3:32:35 - loss: 16290.8252 - custom_mae: 86.84 - ETA: 3:32:20 - loss: 16266.3682 - custom_mae: 86.76 - ETA: 3:32:06 - loss: 16242.5039 - custom_mae: 86.69 - ETA: 3:31:51 - loss: 16215.0830 - custom_mae: 86.60 - ETA: 3:31:37 - loss: 16191.1201 - custom_mae: 86.53 - ETA: 3:31:23 - loss: 16169.0713 - custom_mae: 86.47 - ETA: 3:31:08 - loss: 16147.7754 - custom_mae: 86.41 - ETA: 3:30:54 - loss: 16121.5996 - custom_mae: 86.33 - ETA: 3:30:39 - loss: 16096.6445 - custom_mae: 86.26 - ETA: 3:30:25 - loss: 16073.4189 - custom_mae: 86.19 - ETA: 3:30:11 - loss: 16050.5430 - custom_mae: 86.12 - ETA: 3:29:56 - loss: 16029.2480 - custom_mae: 86.06 - ETA: 3:29:42 - loss: 16004.1934 - custom_mae: 85.99 - ETA: 3:29:28 - loss: 15981.8945 - custom_mae: 85.93 - ETA: 3:29:14 - loss: 15959.7314 - custom_mae: 85.87 - ETA: 3:28:59 - loss: 15935.1211 - custom_mae: 85.79 - ETA: 3:28:45 - loss: 15918.0527 - custom_mae: 85.76 - ETA: 3:28:30 - loss: 15895.1816 - custom_mae: 85.69 - ETA: 3:28:16 - loss: 15872.6846 - custom_mae: 85.62 - ETA: 3:28:02 - loss: 15856.8281 - custom_mae: 85.58 - ETA: 3:27:47 - loss: 15829.9961 - custom_mae: 85.50 - ETA: 3:27:33 - loss: 15805.8008 - custom_mae: 85.42 - ETA: 3:27:19 - loss: 15781.4961 - custom_mae: 85.34 - ETA: 3:27:04 - loss: 15758.0078 - custom_mae: 85.27 - ETA: 3:26:50 - loss: 15737.9229 - custom_mae: 85.22 - ETA: 3:26:36 - loss: 15713.3672 - custom_mae: 85.15 - ETA: 3:26:21 - loss: 15694.6250 - custom_mae: 85.10 - ETA: 3:26:07 - loss: 15670.9990 - custom_mae: 85.02 - ETA: 3:25:52 - loss: 15647.7637 - custom_mae: 84.95 - ETA: 3:25:38 - loss: 15620.4609 - custom_mae: 84.87 - ETA: 3:25:24 - loss: 15597.5732 - custom_mae: 84.80 - ETA: 3:25:10 - loss: 15572.6904 - custom_mae: 84.72 - ETA: 3:24:55 - loss: 15550.6787 - custom_mae: 84.66 - ETA: 3:24:41 - loss: 15528.0605 - custom_mae: 84.60 - ETA: 3:24:26 - loss: 15507.1504 - custom_mae: 84.53 - ETA: 3:24:12 - loss: 15486.5664 - custom_mae: 84.47 - ETA: 3:23:57 - loss: 15466.4355 - custom_mae: 84.42 - ETA: 3:23:43 - loss: 15446.4863 - custom_mae: 84.36 - ETA: 3:23:28 - loss: 15425.3623 - custom_mae: 84.30 - ETA: 3:23:14 - loss: 15404.4170 - custom_mae: 84.24 - ETA: 3:23:00 - loss: 15383.6143 - custom_mae: 84.18 - ETA: 3:22:45 - loss: 15360.0811 - custom_mae: 84.11 - ETA: 3:22:31 - loss: 15339.6973 - custom_mae: 84.05 - ETA: 3:22:16 - loss: 15318.4336 - custom_mae: 83.99 - ETA: 3:22:02 - loss: 15296.0752 - custom_mae: 83.92 - ETA: 3:21:48 - loss: 15275.2314 - custom_mae: 83.86 - ETA: 3:21:33 - loss: 15256.6309 - custom_mae: 83.82 - ETA: 3:21:19 - loss: 15235.3555 - custom_mae: 83.76 - ETA: 3:21:04 - loss: 15214.0205 - custom_mae: 83.69 - ETA: 3:20:50 - loss: 15190.8486 - custom_mae: 83.62 - ETA: 3:20:35 - loss: 15175.9629 - custom_mae: 83.59 - ETA: 3:20:21 - loss: 15151.7227 - custom_mae: 83.51 - ETA: 3:20:06 - loss: 15129.9297 - custom_mae: 83.45 - ETA: 3:19:53 - loss: 15108.7734 - custom_mae: 83.39 - ETA: 3:19:38 - loss: 15085.5537 - custom_mae: 83.31 - ETA: 3:19:25 - loss: 15064.0439 - custom_mae: 83.24 - ETA: 3:19:11 - loss: 15041.7617 - custom_mae: 83.17 - ETA: 3:18:57 - loss: 15021.1689 - custom_mae: 83.12 - ETA: 3:18:43 - loss: 14997.6709 - custom_mae: 83.04 - ETA: 3:18:29 - loss: 14974.0176 - custom_mae: 82.96 - ETA: 3:18:15 - loss: 14954.1553 - custom_mae: 82.90 - ETA: 3:18:00 - loss: 14932.6865 - custom_mae: 82.83 - ETA: 3:17:45 - loss: 14911.5791 - custom_mae: 82.77 - ETA: 3:17:31 - loss: 14892.0107 - custom_mae: 82.71 - ETA: 3:17:16 - loss: 14870.4502 - custom_mae: 82.65 - ETA: 3:17:01 - loss: 14850.5244 - custom_mae: 82.59 - ETA: 3:16:47 - loss: 14829.3145 - custom_mae: 82.52 - ETA: 3:16:33 - loss: 14808.6133 - custom_mae: 82.46 - ETA: 3:16:18 - loss: 14787.8955 - custom_mae: 82.40 - ETA: 3:16:04 - loss: 14767.4141 - custom_mae: 82.34 - ETA: 3:15:49 - loss: 14747.6816 - custom_mae: 82.28 - ETA: 3:15:34 - loss: 14727.9756 - custom_mae: 82.22 - ETA: 3:15:20 - loss: 14707.9404 - custom_mae: 82.16 - ETA: 3:15:06 - loss: 14690.6045 - custom_mae: 82.11 - ETA: 3:14:51 - loss: 14668.3701 - custom_mae: 82.03 - ETA: 3:14:36 - loss: 14648.4434 - custom_mae: 81.97 - ETA: 3:14:22 - loss: 14627.8359 - custom_mae: 81.91 - ETA: 3:14:07 - loss: 14606.3447 - custom_mae: 81.84 - ETA: 3:13:53 - loss: 14589.6191 - custom_mae: 81.80 - ETA: 3:13:38 - loss: 14569.8682 - custom_mae: 81.74 - ETA: 3:13:24 - loss: 14551.8457 - custom_mae: 81.68 - ETA: 3:13:10 - loss: 14530.4756 - custom_mae: 81.6218"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 598/1250 [=============>................] - ETA: 3:12:55 - loss: 14511.4990 - custom_mae: 81.56 - ETA: 3:12:40 - loss: 14491.3193 - custom_mae: 81.50 - ETA: 3:12:26 - loss: 14472.7432 - custom_mae: 81.44 - ETA: 3:12:12 - loss: 14452.4785 - custom_mae: 81.38 - ETA: 3:11:57 - loss: 14430.1348 - custom_mae: 81.31 - ETA: 3:11:43 - loss: 14412.2422 - custom_mae: 81.25 - ETA: 3:11:28 - loss: 14394.0332 - custom_mae: 81.20 - ETA: 3:11:14 - loss: 14376.9180 - custom_mae: 81.15 - ETA: 3:10:59 - loss: 14359.4053 - custom_mae: 81.10 - ETA: 3:10:45 - loss: 14342.6904 - custom_mae: 81.05 - ETA: 3:10:30 - loss: 14322.7988 - custom_mae: 80.98 - ETA: 3:10:16 - loss: 14305.5674 - custom_mae: 80.93 - ETA: 3:10:01 - loss: 14286.9199 - custom_mae: 80.88 - ETA: 3:09:47 - loss: 14268.9336 - custom_mae: 80.83 - ETA: 3:09:32 - loss: 14248.5098 - custom_mae: 80.76 - ETA: 3:09:18 - loss: 14230.2217 - custom_mae: 80.71 - ETA: 3:09:03 - loss: 14214.6934 - custom_mae: 80.67 - ETA: 3:08:49 - loss: 14195.5342 - custom_mae: 80.61 - ETA: 3:08:34 - loss: 14178.9707 - custom_mae: 80.55 - ETA: 3:08:20 - loss: 14159.5762 - custom_mae: 80.50 - ETA: 3:08:06 - loss: 14141.0762 - custom_mae: 80.44 - ETA: 3:07:51 - loss: 14121.1436 - custom_mae: 80.38 - ETA: 3:07:36 - loss: 14100.7627 - custom_mae: 80.31 - ETA: 3:07:22 - loss: 14083.7646 - custom_mae: 80.26 - ETA: 3:07:08 - loss: 14065.2217 - custom_mae: 80.21 - ETA: 3:06:53 - loss: 14046.4648 - custom_mae: 80.15 - ETA: 3:06:39 - loss: 14027.5312 - custom_mae: 80.09 - ETA: 3:06:24 - loss: 14012.4463 - custom_mae: 80.04 - ETA: 3:06:10 - loss: 13996.5889 - custom_mae: 80.00 - ETA: 3:05:55 - loss: 13980.4629 - custom_mae: 79.95 - ETA: 3:05:41 - loss: 13963.0146 - custom_mae: 79.90 - ETA: 3:05:26 - loss: 13945.7305 - custom_mae: 79.85 - ETA: 3:05:12 - loss: 13926.3174 - custom_mae: 79.79 - ETA: 3:04:57 - loss: 13906.9678 - custom_mae: 79.72 - ETA: 3:04:43 - loss: 13888.6475 - custom_mae: 79.66 - ETA: 3:04:28 - loss: 13870.4922 - custom_mae: 79.60 - ETA: 3:04:14 - loss: 13853.1982 - custom_mae: 79.55 - ETA: 3:03:59 - loss: 13834.5596 - custom_mae: 79.49 - ETA: 3:03:45 - loss: 13816.4111 - custom_mae: 79.43 - ETA: 3:03:30 - loss: 13799.8799 - custom_mae: 79.38 - ETA: 3:03:16 - loss: 13784.1904 - custom_mae: 79.33 - ETA: 3:03:01 - loss: 13767.5752 - custom_mae: 79.28 - ETA: 3:02:47 - loss: 13749.9482 - custom_mae: 79.22 - ETA: 3:02:33 - loss: 13732.4082 - custom_mae: 79.17 - ETA: 3:02:18 - loss: 13715.3701 - custom_mae: 79.12 - ETA: 3:02:03 - loss: 13699.0029 - custom_mae: 79.07 - ETA: 3:01:49 - loss: 13684.3926 - custom_mae: 79.03 - ETA: 3:01:34 - loss: 13665.2871 - custom_mae: 78.96 - ETA: 3:01:20 - loss: 13649.9736 - custom_mae: 78.91 - ETA: 3:01:06 - loss: 13633.4883 - custom_mae: 78.86 - ETA: 3:00:51 - loss: 13614.5615 - custom_mae: 78.79 - ETA: 3:00:37 - loss: 13594.3711 - custom_mae: 78.72 - ETA: 3:00:22 - loss: 13577.6787 - custom_mae: 78.67 - ETA: 3:00:08 - loss: 13560.3037 - custom_mae: 78.62 - ETA: 2:59:53 - loss: 13543.7793 - custom_mae: 78.56 - ETA: 2:59:38 - loss: 13529.8145 - custom_mae: 78.52 - ETA: 2:59:24 - loss: 13513.5469 - custom_mae: 78.48 - ETA: 2:59:10 - loss: 13496.6738 - custom_mae: 78.42 - ETA: 2:58:55 - loss: 13480.2334 - custom_mae: 78.37 - ETA: 2:58:41 - loss: 13463.4023 - custom_mae: 78.31 - ETA: 2:58:26 - loss: 13448.0615 - custom_mae: 78.26 - ETA: 2:58:12 - loss: 13432.7578 - custom_mae: 78.22 - ETA: 2:57:58 - loss: 13418.6465 - custom_mae: 78.18 - ETA: 2:57:43 - loss: 13402.1465 - custom_mae: 78.12 - ETA: 2:57:29 - loss: 13386.2432 - custom_mae: 78.07 - ETA: 2:57:14 - loss: 13369.1836 - custom_mae: 78.01 - ETA: 2:57:00 - loss: 13351.8965 - custom_mae: 77.96 - ETA: 2:56:45 - loss: 13337.8145 - custom_mae: 77.91 - ETA: 2:56:31 - loss: 13323.4893 - custom_mae: 77.87 - ETA: 2:56:16 - loss: 13308.9893 - custom_mae: 77.82 - ETA: 2:56:02 - loss: 13291.7402 - custom_mae: 77.76 - ETA: 2:55:48 - loss: 13277.0352 - custom_mae: 77.72 - ETA: 2:55:33 - loss: 13262.9922 - custom_mae: 77.68 - ETA: 2:55:18 - loss: 13247.5234 - custom_mae: 77.63 - ETA: 2:55:04 - loss: 13231.0273 - custom_mae: 77.57 - ETA: 2:54:50 - loss: 13215.4668 - custom_mae: 77.53 - ETA: 2:54:35 - loss: 13202.9629 - custom_mae: 77.49 - ETA: 2:54:21 - loss: 13189.0420 - custom_mae: 77.45 - ETA: 2:54:06 - loss: 13174.9170 - custom_mae: 77.40 - ETA: 2:53:52 - loss: 13159.0508 - custom_mae: 77.35 - ETA: 2:53:38 - loss: 13143.0010 - custom_mae: 77.30 - ETA: 2:53:23 - loss: 13128.1582 - custom_mae: 77.26 - ETA: 2:53:09 - loss: 13111.7607 - custom_mae: 77.20 - ETA: 2:52:54 - loss: 13097.1309 - custom_mae: 77.16 - ETA: 2:52:40 - loss: 13081.7832 - custom_mae: 77.11 - ETA: 2:52:26 - loss: 13065.6162 - custom_mae: 77.06 - ETA: 2:52:11 - loss: 13049.9785 - custom_mae: 77.01 - ETA: 2:51:57 - loss: 13037.2422 - custom_mae: 76.97 - ETA: 2:51:42 - loss: 13021.6172 - custom_mae: 76.92 - ETA: 2:51:28 - loss: 13007.7246 - custom_mae: 76.88 - ETA: 2:51:13 - loss: 12993.6064 - custom_mae: 76.83 - ETA: 2:50:59 - loss: 12979.8125 - custom_mae: 76.79 - ETA: 2:50:44 - loss: 12964.0107 - custom_mae: 76.73 - ETA: 2:50:30 - loss: 12947.9023 - custom_mae: 76.68 - ETA: 2:50:15 - loss: 12932.8418 - custom_mae: 76.63 - ETA: 2:50:01 - loss: 12918.0693 - custom_mae: 76.58 - ETA: 2:49:46 - loss: 12904.8027 - custom_mae: 76.54 - ETA: 2:49:32 - loss: 12889.8369 - custom_mae: 76.49 - ETA: 2:49:18 - loss: 12876.4473 - custom_mae: 76.45 - ETA: 2:49:03 - loss: 12861.1357 - custom_mae: 76.40 - ETA: 2:48:49 - loss: 12846.8389 - custom_mae: 76.35 - ETA: 2:48:34 - loss: 12832.9434 - custom_mae: 76.30 - ETA: 2:48:20 - loss: 12818.4863 - custom_mae: 76.25 - ETA: 2:48:05 - loss: 12804.2559 - custom_mae: 76.20 - ETA: 2:47:51 - loss: 12789.5635 - custom_mae: 76.15 - ETA: 2:47:36 - loss: 12776.6680 - custom_mae: 76.11 - ETA: 2:47:22 - loss: 12763.1309 - custom_mae: 76.07 - ETA: 2:47:08 - loss: 12748.5537 - custom_mae: 76.02 - ETA: 2:46:53 - loss: 12733.9775 - custom_mae: 75.97 - ETA: 2:46:39 - loss: 12719.4971 - custom_mae: 75.92 - ETA: 2:46:24 - loss: 12704.8906 - custom_mae: 75.87 - ETA: 2:46:10 - loss: 12692.5322 - custom_mae: 75.83 - ETA: 2:45:55 - loss: 12679.2676 - custom_mae: 75.79 - ETA: 2:45:41 - loss: 12664.5127 - custom_mae: 75.74 - ETA: 2:45:26 - loss: 12649.9795 - custom_mae: 75.69 - ETA: 2:45:12 - loss: 12634.1240 - custom_mae: 75.63 - ETA: 2:44:58 - loss: 12619.8320 - custom_mae: 75.58 - ETA: 2:44:43 - loss: 12608.1270 - custom_mae: 75.55 - ETA: 2:44:29 - loss: 12595.0732 - custom_mae: 75.51 - ETA: 2:44:14 - loss: 12580.8320 - custom_mae: 75.46 - ETA: 2:44:00 - loss: 12567.4766 - custom_mae: 75.42 - ETA: 2:43:45 - loss: 12551.9805 - custom_mae: 75.37 - ETA: 2:43:31 - loss: 12539.6650 - custom_mae: 75.33 - ETA: 2:43:16 - loss: 12526.6123 - custom_mae: 75.29 - ETA: 2:43:02 - loss: 12512.8242 - custom_mae: 75.24 - ETA: 2:42:48 - loss: 12498.2744 - custom_mae: 75.19 - ETA: 2:42:33 - loss: 12486.1074 - custom_mae: 75.16 - ETA: 2:42:19 - loss: 12472.5479 - custom_mae: 75.11 - ETA: 2:42:04 - loss: 12460.2861 - custom_mae: 75.07 - ETA: 2:41:50 - loss: 12447.5342 - custom_mae: 75.03 - ETA: 2:41:35 - loss: 12435.6748 - custom_mae: 74.99 - ETA: 2:41:21 - loss: 12423.7461 - custom_mae: 74.96 - ETA: 2:41:07 - loss: 12411.5732 - custom_mae: 74.92 - ETA: 2:40:52 - loss: 12396.8633 - custom_mae: 74.86 - ETA: 2:40:38 - loss: 12382.6084 - custom_mae: 74.81 - ETA: 2:40:24 - loss: 12369.5186 - custom_mae: 74.77 - ETA: 2:40:09 - loss: 12356.2402 - custom_mae: 74.72 - ETA: 2:39:54 - loss: 12344.1484 - custom_mae: 74.69 - ETA: 2:39:40 - loss: 12330.7852 - custom_mae: 74.64 - ETA: 2:39:26 - loss: 12317.9756 - custom_mae: 74.60 - ETA: 2:39:11 - loss: 12304.6338 - custom_mae: 74.55 - ETA: 2:38:57 - loss: 12291.9639 - custom_mae: 74.51 - ETA: 2:38:43 - loss: 12280.8965 - custom_mae: 74.48 - ETA: 2:38:28 - loss: 12269.1465 - custom_mae: 74.44 - ETA: 2:38:14 - loss: 12254.8701 - custom_mae: 74.39 - ETA: 2:37:59 - loss: 12243.8115 - custom_mae: 74.35 - ETA: 2:37:45 - loss: 12230.9111 - custom_mae: 74.31 - ETA: 2:37:31 - loss: 12218.5107 - custom_mae: 74.27 - ETA: 2:37:16 - loss: 12209.0225 - custom_mae: 74.24 - ETA: 2:37:02 - loss: 12197.2559 - custom_mae: 74.21 - ETA: 2:36:48 - loss: 12183.6396 - custom_mae: 74.1662"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 749/1250 [================>.............] - ETA: 2:36:33 - loss: 12171.4092 - custom_mae: 74.12 - ETA: 2:36:19 - loss: 12159.1621 - custom_mae: 74.08 - ETA: 2:36:04 - loss: 12145.2021 - custom_mae: 74.03 - ETA: 2:35:50 - loss: 12133.3975 - custom_mae: 73.99 - ETA: 2:35:35 - loss: 12121.4648 - custom_mae: 73.95 - ETA: 2:35:21 - loss: 12108.0635 - custom_mae: 73.90 - ETA: 2:35:07 - loss: 12094.9980 - custom_mae: 73.86 - ETA: 2:34:52 - loss: 12082.1260 - custom_mae: 73.81 - ETA: 2:34:38 - loss: 12069.9512 - custom_mae: 73.77 - ETA: 2:34:23 - loss: 12059.0283 - custom_mae: 73.74 - ETA: 2:34:09 - loss: 12045.7656 - custom_mae: 73.69 - ETA: 2:33:54 - loss: 12031.9062 - custom_mae: 73.65 - ETA: 2:33:40 - loss: 12019.6943 - custom_mae: 73.60 - ETA: 2:33:25 - loss: 12006.8320 - custom_mae: 73.55 - ETA: 2:33:11 - loss: 11995.7275 - custom_mae: 73.52 - ETA: 2:32:57 - loss: 11983.7949 - custom_mae: 73.48 - ETA: 2:32:42 - loss: 11971.3916 - custom_mae: 73.43 - ETA: 2:32:28 - loss: 11959.5479 - custom_mae: 73.39 - ETA: 2:32:13 - loss: 11946.3848 - custom_mae: 73.35 - ETA: 2:31:59 - loss: 11933.1807 - custom_mae: 73.30 - ETA: 2:31:44 - loss: 11921.7295 - custom_mae: 73.26 - ETA: 2:31:30 - loss: 11912.4248 - custom_mae: 73.23 - ETA: 2:31:16 - loss: 11900.3838 - custom_mae: 73.19 - ETA: 2:31:01 - loss: 11890.3623 - custom_mae: 73.16 - ETA: 2:30:47 - loss: 11879.0498 - custom_mae: 73.12 - ETA: 2:30:33 - loss: 11867.9053 - custom_mae: 73.08 - ETA: 2:30:18 - loss: 11857.0947 - custom_mae: 73.05 - ETA: 2:30:04 - loss: 11847.1807 - custom_mae: 73.01 - ETA: 2:29:49 - loss: 11834.1699 - custom_mae: 72.96 - ETA: 2:29:35 - loss: 11823.3701 - custom_mae: 72.93 - ETA: 2:29:20 - loss: 11812.0908 - custom_mae: 72.89 - ETA: 2:29:06 - loss: 11798.7578 - custom_mae: 72.84 - ETA: 2:28:52 - loss: 11786.1699 - custom_mae: 72.79 - ETA: 2:28:37 - loss: 11774.4229 - custom_mae: 72.76 - ETA: 2:28:23 - loss: 11763.9277 - custom_mae: 72.72 - ETA: 2:28:08 - loss: 11752.5400 - custom_mae: 72.68 - ETA: 2:27:54 - loss: 11741.2158 - custom_mae: 72.64 - ETA: 2:27:40 - loss: 11728.4336 - custom_mae: 72.59 - ETA: 2:27:25 - loss: 11718.7588 - custom_mae: 72.56 - ETA: 2:27:11 - loss: 11707.1484 - custom_mae: 72.52 - ETA: 2:26:57 - loss: 11695.7480 - custom_mae: 72.48 - ETA: 2:26:42 - loss: 11684.9316 - custom_mae: 72.44 - ETA: 2:26:28 - loss: 11673.4658 - custom_mae: 72.40 - ETA: 2:26:13 - loss: 11663.4775 - custom_mae: 72.37 - ETA: 2:25:59 - loss: 11653.4521 - custom_mae: 72.34 - ETA: 2:25:45 - loss: 11640.8662 - custom_mae: 72.29 - ETA: 2:25:30 - loss: 11631.7754 - custom_mae: 72.27 - ETA: 2:25:16 - loss: 11619.8750 - custom_mae: 72.22 - ETA: 2:25:02 - loss: 11608.3604 - custom_mae: 72.18 - ETA: 2:24:47 - loss: 11596.6221 - custom_mae: 72.15 - ETA: 2:24:33 - loss: 11585.1318 - custom_mae: 72.10 - ETA: 2:24:18 - loss: 11574.2959 - custom_mae: 72.07 - ETA: 2:24:04 - loss: 11563.8721 - custom_mae: 72.03 - ETA: 2:23:50 - loss: 11552.7607 - custom_mae: 72.00 - ETA: 2:23:35 - loss: 11543.5010 - custom_mae: 71.97 - ETA: 2:23:21 - loss: 11532.0000 - custom_mae: 71.92 - ETA: 2:23:07 - loss: 11522.1162 - custom_mae: 71.89 - ETA: 2:22:52 - loss: 11511.9082 - custom_mae: 71.86 - ETA: 2:22:38 - loss: 11501.2148 - custom_mae: 71.82 - ETA: 2:22:23 - loss: 11490.0498 - custom_mae: 71.78 - ETA: 2:22:09 - loss: 11479.5469 - custom_mae: 71.74 - ETA: 2:21:54 - loss: 11471.1494 - custom_mae: 71.72 - ETA: 2:21:40 - loss: 11460.4375 - custom_mae: 71.68 - ETA: 2:21:26 - loss: 11451.2734 - custom_mae: 71.65 - ETA: 2:21:11 - loss: 11440.9434 - custom_mae: 71.61 - ETA: 2:20:57 - loss: 11430.7383 - custom_mae: 71.58 - ETA: 2:20:42 - loss: 11420.5029 - custom_mae: 71.54 - ETA: 2:20:28 - loss: 11410.7607 - custom_mae: 71.50 - ETA: 2:20:14 - loss: 11401.4072 - custom_mae: 71.47 - ETA: 2:19:59 - loss: 11391.3135 - custom_mae: 71.44 - ETA: 2:19:45 - loss: 11381.8164 - custom_mae: 71.41 - ETA: 2:19:30 - loss: 11372.0322 - custom_mae: 71.38 - ETA: 2:19:16 - loss: 11363.1250 - custom_mae: 71.35 - ETA: 2:19:02 - loss: 11353.9502 - custom_mae: 71.32 - ETA: 2:18:47 - loss: 11344.5850 - custom_mae: 71.29 - ETA: 2:18:33 - loss: 11336.2891 - custom_mae: 71.26 - ETA: 2:18:19 - loss: 11327.3330 - custom_mae: 71.23 - ETA: 2:18:04 - loss: 11316.9609 - custom_mae: 71.20 - ETA: 2:17:50 - loss: 11306.0732 - custom_mae: 71.16 - ETA: 2:17:35 - loss: 11295.3486 - custom_mae: 71.12 - ETA: 2:17:21 - loss: 11286.9160 - custom_mae: 71.09 - ETA: 2:17:07 - loss: 11275.1035 - custom_mae: 71.05 - ETA: 2:16:52 - loss: 11264.7402 - custom_mae: 71.01 - ETA: 2:16:38 - loss: 11255.5879 - custom_mae: 70.98 - ETA: 2:16:23 - loss: 11245.6934 - custom_mae: 70.94 - ETA: 2:16:09 - loss: 11235.7314 - custom_mae: 70.90 - ETA: 2:15:54 - loss: 11225.7559 - custom_mae: 70.87 - ETA: 2:15:40 - loss: 11215.7588 - custom_mae: 70.84 - ETA: 2:15:26 - loss: 11205.8076 - custom_mae: 70.80 - ETA: 2:15:11 - loss: 11197.1543 - custom_mae: 70.77 - ETA: 2:14:57 - loss: 11187.3691 - custom_mae: 70.74 - ETA: 2:14:42 - loss: 11179.8838 - custom_mae: 70.72 - ETA: 2:14:28 - loss: 11171.8340 - custom_mae: 70.69 - ETA: 2:14:13 - loss: 11163.8164 - custom_mae: 70.66 - ETA: 2:13:59 - loss: 11153.9180 - custom_mae: 70.63 - ETA: 2:13:45 - loss: 11144.6172 - custom_mae: 70.59 - ETA: 2:13:30 - loss: 11135.6328 - custom_mae: 70.56 - ETA: 2:13:16 - loss: 11124.1895 - custom_mae: 70.52 - ETA: 2:13:01 - loss: 11114.0215 - custom_mae: 70.48 - ETA: 2:12:47 - loss: 11104.0469 - custom_mae: 70.45 - ETA: 2:12:33 - loss: 11096.3672 - custom_mae: 70.43 - ETA: 2:12:18 - loss: 11088.4111 - custom_mae: 70.40 - ETA: 2:12:04 - loss: 11080.5166 - custom_mae: 70.37 - ETA: 2:11:49 - loss: 11070.1738 - custom_mae: 70.33 - ETA: 2:11:35 - loss: 11060.5342 - custom_mae: 70.30 - ETA: 2:11:20 - loss: 11050.7686 - custom_mae: 70.26 - ETA: 2:11:06 - loss: 11041.1904 - custom_mae: 70.23 - ETA: 2:10:51 - loss: 11031.0449 - custom_mae: 70.19 - ETA: 2:10:37 - loss: 11023.9854 - custom_mae: 70.17 - ETA: 2:10:22 - loss: 11016.5596 - custom_mae: 70.15 - ETA: 2:10:08 - loss: 11006.5391 - custom_mae: 70.11 - ETA: 2:09:54 - loss: 10997.6943 - custom_mae: 70.08 - ETA: 2:09:39 - loss: 10990.2607 - custom_mae: 70.06 - ETA: 2:09:25 - loss: 10981.9912 - custom_mae: 70.03 - ETA: 2:09:10 - loss: 10972.4170 - custom_mae: 70.00 - ETA: 2:08:56 - loss: 10964.1855 - custom_mae: 69.97 - ETA: 2:08:41 - loss: 10956.2773 - custom_mae: 69.94 - ETA: 2:08:27 - loss: 10947.6201 - custom_mae: 69.91 - ETA: 2:08:12 - loss: 10938.0625 - custom_mae: 69.88 - ETA: 2:07:58 - loss: 10928.8936 - custom_mae: 69.85 - ETA: 2:07:43 - loss: 10919.6777 - custom_mae: 69.81 - ETA: 2:07:29 - loss: 10910.0020 - custom_mae: 69.78 - ETA: 2:07:15 - loss: 10900.1992 - custom_mae: 69.74 - ETA: 2:07:00 - loss: 10892.0996 - custom_mae: 69.71 - ETA: 2:06:46 - loss: 10884.1250 - custom_mae: 69.68 - ETA: 2:06:31 - loss: 10877.7031 - custom_mae: 69.66 - ETA: 2:06:17 - loss: 10869.9775 - custom_mae: 69.64 - ETA: 2:06:02 - loss: 10860.9238 - custom_mae: 69.60 - ETA: 2:05:48 - loss: 10851.9678 - custom_mae: 69.57 - ETA: 2:05:33 - loss: 10842.7178 - custom_mae: 69.54 - ETA: 2:05:19 - loss: 10834.1377 - custom_mae: 69.51 - ETA: 2:05:05 - loss: 10824.1914 - custom_mae: 69.47 - ETA: 2:04:50 - loss: 10815.3574 - custom_mae: 69.44 - ETA: 2:04:36 - loss: 10806.6270 - custom_mae: 69.41 - ETA: 2:04:21 - loss: 10797.5625 - custom_mae: 69.38 - ETA: 2:04:07 - loss: 10790.1826 - custom_mae: 69.35 - ETA: 2:03:52 - loss: 10781.1387 - custom_mae: 69.32 - ETA: 2:03:38 - loss: 10772.1006 - custom_mae: 69.29 - ETA: 2:03:23 - loss: 10761.8975 - custom_mae: 69.25 - ETA: 2:03:09 - loss: 10753.1328 - custom_mae: 69.22 - ETA: 2:02:54 - loss: 10745.1465 - custom_mae: 69.19 - ETA: 2:02:40 - loss: 10736.6484 - custom_mae: 69.16 - ETA: 2:02:26 - loss: 10726.9004 - custom_mae: 69.12 - ETA: 2:02:11 - loss: 10718.0938 - custom_mae: 69.09 - ETA: 2:01:57 - loss: 10709.0713 - custom_mae: 69.05 - ETA: 2:01:42 - loss: 10701.2148 - custom_mae: 69.03 - ETA: 2:01:28 - loss: 10691.4131 - custom_mae: 68.99 - ETA: 2:01:13 - loss: 10682.9756 - custom_mae: 68.96 - ETA: 2:00:59 - loss: 10673.8867 - custom_mae: 68.92 - ETA: 2:00:45 - loss: 10665.5967 - custom_mae: 68.89 - ETA: 2:00:30 - loss: 10656.8145 - custom_mae: 68.8632"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900/1250 [====================>.........] - ETA: 2:00:16 - loss: 10647.0811 - custom_mae: 68.82 - ETA: 2:00:01 - loss: 10639.0645 - custom_mae: 68.79 - ETA: 1:59:47 - loss: 10629.7070 - custom_mae: 68.75 - ETA: 1:59:33 - loss: 10621.5283 - custom_mae: 68.72 - ETA: 1:59:18 - loss: 10613.8643 - custom_mae: 68.70 - ETA: 1:59:04 - loss: 10606.7344 - custom_mae: 68.67 - ETA: 1:58:49 - loss: 10598.9980 - custom_mae: 68.65 - ETA: 1:58:35 - loss: 10591.8691 - custom_mae: 68.62 - ETA: 1:58:20 - loss: 10583.0469 - custom_mae: 68.59 - ETA: 1:58:06 - loss: 10574.5400 - custom_mae: 68.56 - ETA: 1:57:52 - loss: 10566.3271 - custom_mae: 68.53 - ETA: 1:57:37 - loss: 10557.9297 - custom_mae: 68.50 - ETA: 1:57:23 - loss: 10548.9717 - custom_mae: 68.47 - ETA: 1:57:08 - loss: 10542.5850 - custom_mae: 68.44 - ETA: 1:56:54 - loss: 10535.2148 - custom_mae: 68.42 - ETA: 1:56:39 - loss: 10526.6982 - custom_mae: 68.39 - ETA: 1:56:25 - loss: 10518.2939 - custom_mae: 68.35 - ETA: 1:56:11 - loss: 10510.7246 - custom_mae: 68.33 - ETA: 1:55:56 - loss: 10501.8467 - custom_mae: 68.29 - ETA: 1:55:42 - loss: 10493.8828 - custom_mae: 68.26 - ETA: 1:55:27 - loss: 10486.2891 - custom_mae: 68.24 - ETA: 1:55:13 - loss: 10476.6582 - custom_mae: 68.20 - ETA: 1:54:58 - loss: 10468.0449 - custom_mae: 68.17 - ETA: 1:54:44 - loss: 10461.0645 - custom_mae: 68.14 - ETA: 1:54:29 - loss: 10453.1006 - custom_mae: 68.12 - ETA: 1:54:15 - loss: 10444.9180 - custom_mae: 68.09 - ETA: 1:54:01 - loss: 10437.3516 - custom_mae: 68.06 - ETA: 1:53:46 - loss: 10429.1523 - custom_mae: 68.03 - ETA: 1:53:32 - loss: 10419.5654 - custom_mae: 67.99 - ETA: 1:53:17 - loss: 10411.6289 - custom_mae: 67.96 - ETA: 1:53:03 - loss: 10404.0303 - custom_mae: 67.94 - ETA: 1:52:48 - loss: 10396.6250 - custom_mae: 67.90 - ETA: 1:52:34 - loss: 10389.5928 - custom_mae: 67.88 - ETA: 1:52:19 - loss: 10382.0469 - custom_mae: 67.85 - ETA: 1:52:05 - loss: 10374.5957 - custom_mae: 67.83 - ETA: 1:51:50 - loss: 10366.0088 - custom_mae: 67.79 - ETA: 1:51:36 - loss: 10356.9385 - custom_mae: 67.76 - ETA: 1:51:22 - loss: 10349.8291 - custom_mae: 67.73 - ETA: 1:51:07 - loss: 10341.3008 - custom_mae: 67.70 - ETA: 1:50:53 - loss: 10334.2139 - custom_mae: 67.67 - ETA: 1:50:38 - loss: 10327.0010 - custom_mae: 67.65 - ETA: 1:50:24 - loss: 10320.4736 - custom_mae: 67.63 - ETA: 1:50:09 - loss: 10314.7227 - custom_mae: 67.61 - ETA: 1:49:55 - loss: 10307.6738 - custom_mae: 67.59 - ETA: 1:49:41 - loss: 10300.8994 - custom_mae: 67.56 - ETA: 1:49:26 - loss: 10294.8203 - custom_mae: 67.54 - ETA: 1:49:12 - loss: 10287.5449 - custom_mae: 67.52 - ETA: 1:48:57 - loss: 10281.4170 - custom_mae: 67.50 - ETA: 1:48:43 - loss: 10273.1582 - custom_mae: 67.46 - ETA: 1:48:29 - loss: 10265.9629 - custom_mae: 67.44 - ETA: 1:48:14 - loss: 10256.2334 - custom_mae: 67.40 - ETA: 1:48:00 - loss: 10248.4111 - custom_mae: 67.37 - ETA: 1:47:45 - loss: 10240.8252 - custom_mae: 67.34 - ETA: 1:47:31 - loss: 10231.5293 - custom_mae: 67.30 - ETA: 1:47:16 - loss: 10223.8623 - custom_mae: 67.27 - ETA: 1:47:02 - loss: 10216.6709 - custom_mae: 67.25 - ETA: 1:46:47 - loss: 10208.6250 - custom_mae: 67.22 - ETA: 1:46:33 - loss: 10199.6094 - custom_mae: 67.18 - ETA: 1:46:18 - loss: 10192.0547 - custom_mae: 67.16 - ETA: 1:46:04 - loss: 10182.9326 - custom_mae: 67.12 - ETA: 1:45:50 - loss: 10175.5332 - custom_mae: 67.10 - ETA: 1:45:35 - loss: 10167.9092 - custom_mae: 67.07 - ETA: 1:45:21 - loss: 10159.8018 - custom_mae: 67.04 - ETA: 1:45:06 - loss: 10152.5029 - custom_mae: 67.01 - ETA: 1:44:52 - loss: 10144.8535 - custom_mae: 66.98 - ETA: 1:44:37 - loss: 10136.6211 - custom_mae: 66.95 - ETA: 1:44:23 - loss: 10129.4053 - custom_mae: 66.92 - ETA: 1:44:08 - loss: 10121.5029 - custom_mae: 66.89 - ETA: 1:43:54 - loss: 10115.8633 - custom_mae: 66.88 - ETA: 1:43:39 - loss: 10109.6025 - custom_mae: 66.85 - ETA: 1:43:25 - loss: 10102.7734 - custom_mae: 66.83 - ETA: 1:43:11 - loss: 10095.5889 - custom_mae: 66.80 - ETA: 1:42:56 - loss: 10088.9609 - custom_mae: 66.78 - ETA: 1:42:42 - loss: 10081.5850 - custom_mae: 66.75 - ETA: 1:42:27 - loss: 10073.1670 - custom_mae: 66.72 - ETA: 1:42:13 - loss: 10064.3760 - custom_mae: 66.68 - ETA: 1:41:58 - loss: 10057.6982 - custom_mae: 66.66 - ETA: 1:41:44 - loss: 10050.7402 - custom_mae: 66.63 - ETA: 1:41:30 - loss: 10043.4385 - custom_mae: 66.60 - ETA: 1:41:15 - loss: 10035.0137 - custom_mae: 66.57 - ETA: 1:41:01 - loss: 10026.2930 - custom_mae: 66.54 - ETA: 1:40:46 - loss: 10018.5576 - custom_mae: 66.50 - ETA: 1:40:32 - loss: 10011.6279 - custom_mae: 66.48 - ETA: 1:40:17 - loss: 10005.9248 - custom_mae: 66.46 - ETA: 1:40:03 - loss: 9998.3535 - custom_mae: 66.4320 - ETA: 1:39:48 - loss: 9990.1250 - custom_mae: 66.399 - ETA: 1:39:34 - loss: 9983.3418 - custom_mae: 66.373 - ETA: 1:39:19 - loss: 9975.3857 - custom_mae: 66.341 - ETA: 1:39:05 - loss: 9968.8936 - custom_mae: 66.315 - ETA: 1:38:50 - loss: 9963.4209 - custom_mae: 66.294 - ETA: 1:38:36 - loss: 9956.5342 - custom_mae: 66.267 - ETA: 1:38:22 - loss: 9948.6055 - custom_mae: 66.235 - ETA: 1:38:07 - loss: 9940.9180 - custom_mae: 66.202 - ETA: 1:37:53 - loss: 9932.8750 - custom_mae: 66.171 - ETA: 1:37:38 - loss: 9925.2246 - custom_mae: 66.140 - ETA: 1:37:24 - loss: 9918.4658 - custom_mae: 66.117 - ETA: 1:37:09 - loss: 9912.3203 - custom_mae: 66.096 - ETA: 1:36:55 - loss: 9904.3408 - custom_mae: 66.065 - ETA: 1:36:40 - loss: 9897.0010 - custom_mae: 66.034 - ETA: 1:36:26 - loss: 9891.2324 - custom_mae: 66.016 - ETA: 1:36:12 - loss: 9884.3525 - custom_mae: 65.989 - ETA: 1:35:57 - loss: 9878.3662 - custom_mae: 65.968 - ETA: 1:35:43 - loss: 9871.7188 - custom_mae: 65.945 - ETA: 1:35:28 - loss: 9864.9600 - custom_mae: 65.918 - ETA: 1:35:14 - loss: 9858.7178 - custom_mae: 65.897 - ETA: 1:34:59 - loss: 9852.3711 - custom_mae: 65.875 - ETA: 1:34:45 - loss: 9845.3438 - custom_mae: 65.852 - ETA: 1:34:31 - loss: 9838.2432 - custom_mae: 65.825 - ETA: 1:34:16 - loss: 9831.1982 - custom_mae: 65.799 - ETA: 1:34:02 - loss: 9823.7646 - custom_mae: 65.771 - ETA: 1:33:47 - loss: 9816.3984 - custom_mae: 65.740 - ETA: 1:33:33 - loss: 9810.4102 - custom_mae: 65.720 - ETA: 1:33:18 - loss: 9803.8467 - custom_mae: 65.696 - ETA: 1:33:04 - loss: 9796.6191 - custom_mae: 65.667 - ETA: 1:32:49 - loss: 9789.4072 - custom_mae: 65.639 - ETA: 1:32:35 - loss: 9782.5420 - custom_mae: 65.613 - ETA: 1:32:21 - loss: 9775.2324 - custom_mae: 65.584 - ETA: 1:32:06 - loss: 9768.9795 - custom_mae: 65.561 - ETA: 1:31:52 - loss: 9763.0557 - custom_mae: 65.536 - ETA: 1:31:37 - loss: 9756.2930 - custom_mae: 65.508 - ETA: 1:31:23 - loss: 9749.1572 - custom_mae: 65.483 - ETA: 1:31:08 - loss: 9742.0449 - custom_mae: 65.456 - ETA: 1:30:54 - loss: 9735.0928 - custom_mae: 65.426 - ETA: 1:30:39 - loss: 9728.4170 - custom_mae: 65.401 - ETA: 1:30:25 - loss: 9722.7598 - custom_mae: 65.381 - ETA: 1:30:11 - loss: 9715.3467 - custom_mae: 65.352 - ETA: 1:29:56 - loss: 9707.0430 - custom_mae: 65.317 - ETA: 1:29:42 - loss: 9701.7598 - custom_mae: 65.298 - ETA: 1:29:27 - loss: 9694.2383 - custom_mae: 65.270 - ETA: 1:29:13 - loss: 9686.6816 - custom_mae: 65.236 - ETA: 1:28:58 - loss: 9681.7637 - custom_mae: 65.220 - ETA: 1:28:44 - loss: 9675.0439 - custom_mae: 65.195 - ETA: 1:28:30 - loss: 9669.2666 - custom_mae: 65.173 - ETA: 1:28:15 - loss: 9662.6035 - custom_mae: 65.148 - ETA: 1:28:01 - loss: 9657.8555 - custom_mae: 65.132 - ETA: 1:27:46 - loss: 9650.7959 - custom_mae: 65.104 - ETA: 1:27:32 - loss: 9646.3408 - custom_mae: 65.088 - ETA: 1:27:17 - loss: 9638.1514 - custom_mae: 65.055 - ETA: 1:27:03 - loss: 9631.9512 - custom_mae: 65.034 - ETA: 1:26:49 - loss: 9626.7012 - custom_mae: 65.015 - ETA: 1:26:34 - loss: 9620.4326 - custom_mae: 64.991 - ETA: 1:26:20 - loss: 9613.9248 - custom_mae: 64.965 - ETA: 1:26:05 - loss: 9607.5664 - custom_mae: 64.938 - ETA: 1:25:51 - loss: 9600.7236 - custom_mae: 64.911 - ETA: 1:25:36 - loss: 9594.5840 - custom_mae: 64.887 - ETA: 1:25:22 - loss: 9588.1367 - custom_mae: 64.861 - ETA: 1:25:07 - loss: 9582.1357 - custom_mae: 64.837 - ETA: 1:24:53 - loss: 9575.4141 - custom_mae: 64.810 - ETA: 1:24:38 - loss: 9568.4121 - custom_mae: 64.783 - ETA: 1:24:24 - loss: 9562.3887 - custom_mae: 64.761 - ETA: 1:24:10 - loss: 9557.5938 - custom_mae: 64.7405"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1250 [========================>.....] - ETA: 1:23:55 - loss: 9550.8398 - custom_mae: 64.714 - ETA: 1:23:41 - loss: 9544.0596 - custom_mae: 64.687 - ETA: 1:23:26 - loss: 9537.1504 - custom_mae: 64.660 - ETA: 1:23:12 - loss: 9530.4189 - custom_mae: 64.633 - ETA: 1:22:57 - loss: 9524.1426 - custom_mae: 64.608 - ETA: 1:22:43 - loss: 9517.0996 - custom_mae: 64.579 - ETA: 1:22:29 - loss: 9511.2334 - custom_mae: 64.557 - ETA: 1:22:14 - loss: 9504.6562 - custom_mae: 64.530 - ETA: 1:22:00 - loss: 9498.4619 - custom_mae: 64.507 - ETA: 1:21:45 - loss: 9492.6982 - custom_mae: 64.487 - ETA: 1:21:31 - loss: 9486.4160 - custom_mae: 64.459 - ETA: 1:21:16 - loss: 9480.1533 - custom_mae: 64.432 - ETA: 1:21:02 - loss: 9474.4199 - custom_mae: 64.411 - ETA: 1:20:47 - loss: 9467.0068 - custom_mae: 64.381 - ETA: 1:20:33 - loss: 9459.7041 - custom_mae: 64.351 - ETA: 1:20:19 - loss: 9454.1680 - custom_mae: 64.335 - ETA: 1:20:04 - loss: 9447.4658 - custom_mae: 64.310 - ETA: 1:19:50 - loss: 9441.1318 - custom_mae: 64.284 - ETA: 1:19:35 - loss: 9434.1162 - custom_mae: 64.255 - ETA: 1:19:21 - loss: 9428.4248 - custom_mae: 64.235 - ETA: 1:19:06 - loss: 9422.3330 - custom_mae: 64.210 - ETA: 1:18:52 - loss: 9415.8682 - custom_mae: 64.183 - ETA: 1:18:37 - loss: 9409.8486 - custom_mae: 64.160 - ETA: 1:18:23 - loss: 9404.0977 - custom_mae: 64.137 - ETA: 1:18:09 - loss: 9398.2520 - custom_mae: 64.115 - ETA: 1:17:54 - loss: 9391.3945 - custom_mae: 64.086 - ETA: 1:17:40 - loss: 9384.8594 - custom_mae: 64.059 - ETA: 1:17:25 - loss: 9379.0586 - custom_mae: 64.033 - ETA: 1:17:11 - loss: 9373.9053 - custom_mae: 64.013 - ETA: 1:16:56 - loss: 9367.8311 - custom_mae: 63.988 - ETA: 1:16:42 - loss: 9361.4424 - custom_mae: 63.961 - ETA: 1:16:28 - loss: 9357.2588 - custom_mae: 63.945 - ETA: 1:16:13 - loss: 9352.1807 - custom_mae: 63.929 - ETA: 1:15:59 - loss: 9345.2100 - custom_mae: 63.901 - ETA: 1:15:44 - loss: 9339.5391 - custom_mae: 63.877 - ETA: 1:15:30 - loss: 9334.6553 - custom_mae: 63.858 - ETA: 1:15:15 - loss: 9328.9092 - custom_mae: 63.836 - ETA: 1:15:01 - loss: 9322.6250 - custom_mae: 63.811 - ETA: 1:14:47 - loss: 9316.1318 - custom_mae: 63.784 - ETA: 1:14:32 - loss: 9310.4629 - custom_mae: 63.760 - ETA: 1:14:18 - loss: 9304.3770 - custom_mae: 63.737 - ETA: 1:14:03 - loss: 9298.8828 - custom_mae: 63.715 - ETA: 1:13:49 - loss: 9292.5674 - custom_mae: 63.689 - ETA: 1:13:34 - loss: 9287.2080 - custom_mae: 63.669 - ETA: 1:13:20 - loss: 9282.3223 - custom_mae: 63.653 - ETA: 1:13:06 - loss: 9275.6064 - custom_mae: 63.626 - ETA: 1:12:51 - loss: 9268.5088 - custom_mae: 63.595 - ETA: 1:12:37 - loss: 9262.9453 - custom_mae: 63.577 - ETA: 1:12:22 - loss: 9257.9697 - custom_mae: 63.558 - ETA: 1:12:08 - loss: 9251.9443 - custom_mae: 63.536 - ETA: 1:11:53 - loss: 9247.0400 - custom_mae: 63.516 - ETA: 1:11:39 - loss: 9242.1758 - custom_mae: 63.499 - ETA: 1:11:25 - loss: 9237.0762 - custom_mae: 63.478 - ETA: 1:11:10 - loss: 9231.1172 - custom_mae: 63.455 - ETA: 1:10:56 - loss: 9224.6406 - custom_mae: 63.430 - ETA: 1:10:41 - loss: 9218.9600 - custom_mae: 63.409 - ETA: 1:10:27 - loss: 9211.9980 - custom_mae: 63.378 - ETA: 1:10:12 - loss: 9205.6318 - custom_mae: 63.352 - ETA: 1:09:58 - loss: 9199.9854 - custom_mae: 63.330 - ETA: 1:09:44 - loss: 9194.1992 - custom_mae: 63.307 - ETA: 1:09:29 - loss: 9188.5781 - custom_mae: 63.286 - ETA: 1:09:15 - loss: 9184.0342 - custom_mae: 63.269 - ETA: 1:09:00 - loss: 9178.5625 - custom_mae: 63.247 - ETA: 1:08:46 - loss: 9173.3105 - custom_mae: 63.224 - ETA: 1:08:32 - loss: 9169.3369 - custom_mae: 63.208 - ETA: 1:08:17 - loss: 9163.5537 - custom_mae: 63.184 - ETA: 1:08:03 - loss: 9157.8057 - custom_mae: 63.163 - ETA: 1:07:48 - loss: 9152.7393 - custom_mae: 63.144 - ETA: 1:07:34 - loss: 9146.5586 - custom_mae: 63.118 - ETA: 1:07:20 - loss: 9140.0801 - custom_mae: 63.091 - ETA: 1:07:05 - loss: 9133.8027 - custom_mae: 63.065 - ETA: 1:06:51 - loss: 9127.2900 - custom_mae: 63.036 - ETA: 1:06:36 - loss: 9121.8945 - custom_mae: 63.015 - ETA: 1:06:22 - loss: 9116.7559 - custom_mae: 62.995 - ETA: 1:06:08 - loss: 9111.1631 - custom_mae: 62.972 - ETA: 1:05:53 - loss: 9105.6943 - custom_mae: 62.951 - ETA: 1:05:39 - loss: 9098.9111 - custom_mae: 62.921 - ETA: 1:05:24 - loss: 9093.3613 - custom_mae: 62.898 - ETA: 1:05:10 - loss: 9088.9844 - custom_mae: 62.883 - ETA: 1:04:55 - loss: 9082.5049 - custom_mae: 62.857 - ETA: 1:04:41 - loss: 9076.5391 - custom_mae: 62.834 - ETA: 1:04:27 - loss: 9071.5703 - custom_mae: 62.816 - ETA: 1:04:12 - loss: 9066.5830 - custom_mae: 62.798 - ETA: 1:03:58 - loss: 9060.4707 - custom_mae: 62.774 - ETA: 1:03:43 - loss: 9055.3330 - custom_mae: 62.753 - ETA: 1:03:29 - loss: 9049.3359 - custom_mae: 62.730 - ETA: 1:03:15 - loss: 9044.3828 - custom_mae: 62.711 - ETA: 1:03:00 - loss: 9038.3857 - custom_mae: 62.686 - ETA: 1:02:46 - loss: 9032.2773 - custom_mae: 62.661 - ETA: 1:02:31 - loss: 9026.6152 - custom_mae: 62.639 - ETA: 1:02:17 - loss: 9020.4424 - custom_mae: 62.612 - ETA: 1:02:02 - loss: 9015.1309 - custom_mae: 62.590 - ETA: 1:01:48 - loss: 9010.0635 - custom_mae: 62.572 - ETA: 1:01:34 - loss: 9004.6660 - custom_mae: 62.549 - ETA: 1:01:19 - loss: 9000.8535 - custom_mae: 62.536 - ETA: 1:01:05 - loss: 8995.3027 - custom_mae: 62.512 - ETA: 1:00:50 - loss: 8989.9482 - custom_mae: 62.489 - ETA: 1:00:36 - loss: 8984.7197 - custom_mae: 62.467 - ETA: 1:00:22 - loss: 8979.4502 - custom_mae: 62.448 - ETA: 1:00:07 - loss: 8974.4834 - custom_mae: 62.428 - ETA: 59:53 - loss: 8969.5078 - custom_mae: 62.4103  - ETA: 59:38 - loss: 8965.2900 - custom_mae: 62.394 - ETA: 59:24 - loss: 8960.2510 - custom_mae: 62.375 - ETA: 59:10 - loss: 8954.7275 - custom_mae: 62.353 - ETA: 58:55 - loss: 8948.8262 - custom_mae: 62.329 - ETA: 58:41 - loss: 8943.4961 - custom_mae: 62.309 - ETA: 58:26 - loss: 8937.7783 - custom_mae: 62.285 - ETA: 58:12 - loss: 8932.4814 - custom_mae: 62.266 - ETA: 57:58 - loss: 8927.2529 - custom_mae: 62.246 - ETA: 57:43 - loss: 8921.9971 - custom_mae: 62.227 - ETA: 57:29 - loss: 8917.0566 - custom_mae: 62.208 - ETA: 57:14 - loss: 8911.8037 - custom_mae: 62.186 - ETA: 57:00 - loss: 8907.0498 - custom_mae: 62.167 - ETA: 56:45 - loss: 8901.5293 - custom_mae: 62.148 - ETA: 56:31 - loss: 8897.1123 - custom_mae: 62.131 - ETA: 56:17 - loss: 8891.2861 - custom_mae: 62.106 - ETA: 56:02 - loss: 8886.5791 - custom_mae: 62.086 - ETA: 55:48 - loss: 8881.3096 - custom_mae: 62.064 - ETA: 55:33 - loss: 8874.7812 - custom_mae: 62.036 - ETA: 55:19 - loss: 8869.9932 - custom_mae: 62.016 - ETA: 55:05 - loss: 8865.4355 - custom_mae: 61.998 - ETA: 54:50 - loss: 8860.5869 - custom_mae: 61.977 - ETA: 54:36 - loss: 8854.9756 - custom_mae: 61.955 - ETA: 54:21 - loss: 8849.9600 - custom_mae: 61.935 - ETA: 54:07 - loss: 8845.0244 - custom_mae: 61.915 - ETA: 53:52 - loss: 8839.9990 - custom_mae: 61.894 - ETA: 53:38 - loss: 8834.3252 - custom_mae: 61.870 - ETA: 53:24 - loss: 8829.8018 - custom_mae: 61.853 - ETA: 53:09 - loss: 8825.6367 - custom_mae: 61.837 - ETA: 52:55 - loss: 8820.0576 - custom_mae: 61.814 - ETA: 52:40 - loss: 8815.5000 - custom_mae: 61.796 - ETA: 52:26 - loss: 8809.7773 - custom_mae: 61.772 - ETA: 52:11 - loss: 8803.5557 - custom_mae: 61.745 - ETA: 51:57 - loss: 8798.5771 - custom_mae: 61.725 - ETA: 51:43 - loss: 8794.2422 - custom_mae: 61.706 - ETA: 51:28 - loss: 8789.1934 - custom_mae: 61.686 - ETA: 51:14 - loss: 8784.4609 - custom_mae: 61.668 - ETA: 50:59 - loss: 8779.6309 - custom_mae: 61.647 - ETA: 50:45 - loss: 8774.2959 - custom_mae: 61.625 - ETA: 50:30 - loss: 8769.6416 - custom_mae: 61.605 - ETA: 50:16 - loss: 8764.9482 - custom_mae: 61.585 - ETA: 50:01 - loss: 8759.9697 - custom_mae: 61.566 - ETA: 49:47 - loss: 8754.3457 - custom_mae: 61.544 - ETA: 49:33 - loss: 8749.9199 - custom_mae: 61.527 - ETA: 49:18 - loss: 8744.2070 - custom_mae: 61.503 - ETA: 49:04 - loss: 8739.3467 - custom_mae: 61.485 - ETA: 48:49 - loss: 8733.4072 - custom_mae: 61.460 - ETA: 48:35 - loss: 8729.2803 - custom_mae: 61.441 - ETA: 48:20 - loss: 8724.9688 - custom_mae: 61.423 - ETA: 48:06 - loss: 8720.5449 - custom_mae: 61.404 - ETA: 47:52 - loss: 8716.9043 - custom_mae: 61.390 - ETA: 47:37 - loss: 8711.4180 - custom_mae: 61.365 - ETA: 47:23 - loss: 8706.0293 - custom_mae: 61.3419"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1210/1250 [============================>.] - ETA: 47:08 - loss: 8701.1289 - custom_mae: 61.321 - ETA: 46:54 - loss: 8695.8496 - custom_mae: 61.299 - ETA: 46:39 - loss: 8690.6250 - custom_mae: 61.278 - ETA: 46:25 - loss: 8686.1875 - custom_mae: 61.256 - ETA: 46:10 - loss: 8680.5596 - custom_mae: 61.233 - ETA: 45:56 - loss: 8675.7080 - custom_mae: 61.211 - ETA: 45:42 - loss: 8670.9424 - custom_mae: 61.192 - ETA: 45:27 - loss: 8666.4893 - custom_mae: 61.174 - ETA: 45:13 - loss: 8662.0156 - custom_mae: 61.157 - ETA: 44:58 - loss: 8657.4629 - custom_mae: 61.140 - ETA: 44:44 - loss: 8653.8066 - custom_mae: 61.127 - ETA: 44:29 - loss: 8649.6719 - custom_mae: 61.111 - ETA: 44:15 - loss: 8644.8662 - custom_mae: 61.092 - ETA: 44:01 - loss: 8640.3604 - custom_mae: 61.074 - ETA: 43:46 - loss: 8634.7568 - custom_mae: 61.051 - ETA: 43:32 - loss: 8629.8916 - custom_mae: 61.033 - ETA: 43:17 - loss: 8625.2061 - custom_mae: 61.016 - ETA: 43:03 - loss: 8620.5273 - custom_mae: 60.998 - ETA: 42:49 - loss: 8615.6426 - custom_mae: 60.978 - ETA: 42:34 - loss: 8611.5225 - custom_mae: 60.962 - ETA: 42:20 - loss: 8606.5361 - custom_mae: 60.939 - ETA: 42:05 - loss: 8602.5977 - custom_mae: 60.924 - ETA: 41:51 - loss: 8597.9395 - custom_mae: 60.906 - ETA: 41:36 - loss: 8593.2549 - custom_mae: 60.887 - ETA: 41:22 - loss: 8588.0049 - custom_mae: 60.865 - ETA: 41:08 - loss: 8583.4521 - custom_mae: 60.848 - ETA: 40:53 - loss: 8578.5195 - custom_mae: 60.828 - ETA: 40:39 - loss: 8573.9326 - custom_mae: 60.810 - ETA: 40:24 - loss: 8568.8955 - custom_mae: 60.788 - ETA: 40:10 - loss: 8564.1045 - custom_mae: 60.769 - ETA: 39:55 - loss: 8560.5029 - custom_mae: 60.757 - ETA: 39:41 - loss: 8555.5762 - custom_mae: 60.734 - ETA: 39:27 - loss: 8550.5068 - custom_mae: 60.712 - ETA: 39:12 - loss: 8545.9688 - custom_mae: 60.693 - ETA: 38:58 - loss: 8540.7432 - custom_mae: 60.669 - ETA: 38:43 - loss: 8536.7021 - custom_mae: 60.652 - ETA: 38:29 - loss: 8533.0342 - custom_mae: 60.637 - ETA: 38:14 - loss: 8528.0264 - custom_mae: 60.617 - ETA: 38:00 - loss: 8523.0840 - custom_mae: 60.595 - ETA: 37:46 - loss: 8518.5303 - custom_mae: 60.577 - ETA: 37:31 - loss: 8513.8525 - custom_mae: 60.557 - ETA: 37:17 - loss: 8508.5771 - custom_mae: 60.534 - ETA: 37:02 - loss: 8503.5107 - custom_mae: 60.515 - ETA: 36:48 - loss: 8499.0352 - custom_mae: 60.499 - ETA: 36:33 - loss: 8494.1357 - custom_mae: 60.479 - ETA: 36:19 - loss: 8490.6387 - custom_mae: 60.464 - ETA: 36:05 - loss: 8487.4814 - custom_mae: 60.450 - ETA: 35:50 - loss: 8482.6865 - custom_mae: 60.430 - ETA: 35:36 - loss: 8477.9678 - custom_mae: 60.412 - ETA: 35:21 - loss: 8473.1914 - custom_mae: 60.394 - ETA: 35:07 - loss: 8468.4668 - custom_mae: 60.374 - ETA: 34:52 - loss: 8463.3779 - custom_mae: 60.350 - ETA: 34:38 - loss: 8458.0342 - custom_mae: 60.325 - ETA: 34:24 - loss: 8453.8496 - custom_mae: 60.308 - ETA: 34:09 - loss: 8449.7441 - custom_mae: 60.293 - ETA: 33:55 - loss: 8445.4922 - custom_mae: 60.275 - ETA: 33:40 - loss: 8440.5391 - custom_mae: 60.253 - ETA: 33:26 - loss: 8435.9141 - custom_mae: 60.234 - ETA: 33:11 - loss: 8431.1562 - custom_mae: 60.216 - ETA: 32:57 - loss: 8426.3799 - custom_mae: 60.197 - ETA: 32:43 - loss: 8421.8994 - custom_mae: 60.178 - ETA: 32:28 - loss: 8417.5869 - custom_mae: 60.157 - ETA: 32:14 - loss: 8413.1172 - custom_mae: 60.140 - ETA: 31:59 - loss: 8409.1221 - custom_mae: 60.124 - ETA: 31:45 - loss: 8404.9990 - custom_mae: 60.108 - ETA: 31:30 - loss: 8400.2197 - custom_mae: 60.089 - ETA: 31:16 - loss: 8395.5576 - custom_mae: 60.069 - ETA: 31:02 - loss: 8390.9658 - custom_mae: 60.050 - ETA: 30:47 - loss: 8386.8643 - custom_mae: 60.034 - ETA: 30:33 - loss: 8381.7998 - custom_mae: 60.014 - ETA: 30:18 - loss: 8376.9004 - custom_mae: 59.991 - ETA: 30:04 - loss: 8372.5078 - custom_mae: 59.975 - ETA: 29:49 - loss: 8367.5527 - custom_mae: 59.954 - ETA: 29:35 - loss: 8363.5947 - custom_mae: 59.937 - ETA: 29:21 - loss: 8359.8535 - custom_mae: 59.923 - ETA: 29:06 - loss: 8355.7256 - custom_mae: 59.906 - ETA: 28:52 - loss: 8351.6846 - custom_mae: 59.890 - ETA: 28:37 - loss: 8346.7314 - custom_mae: 59.869 - ETA: 28:23 - loss: 8341.8525 - custom_mae: 59.848 - ETA: 28:08 - loss: 8337.0967 - custom_mae: 59.826 - ETA: 27:54 - loss: 8333.4502 - custom_mae: 59.811 - ETA: 27:40 - loss: 8328.8506 - custom_mae: 59.791 - ETA: 27:25 - loss: 8323.7188 - custom_mae: 59.770 - ETA: 27:11 - loss: 8319.8789 - custom_mae: 59.752 - ETA: 26:56 - loss: 8315.4873 - custom_mae: 59.734 - ETA: 26:42 - loss: 8311.1113 - custom_mae: 59.716 - ETA: 26:27 - loss: 8307.3750 - custom_mae: 59.701 - ETA: 26:13 - loss: 8302.3408 - custom_mae: 59.679 - ETA: 25:59 - loss: 8298.3477 - custom_mae: 59.661 - ETA: 25:44 - loss: 8293.7803 - custom_mae: 59.642 - ETA: 25:30 - loss: 8288.6582 - custom_mae: 59.620 - ETA: 25:15 - loss: 8284.3467 - custom_mae: 59.602 - ETA: 25:01 - loss: 8280.0566 - custom_mae: 59.583 - ETA: 24:46 - loss: 8274.9062 - custom_mae: 59.560 - ETA: 24:32 - loss: 8270.8086 - custom_mae: 59.544 - ETA: 24:17 - loss: 8266.6914 - custom_mae: 59.529 - ETA: 24:03 - loss: 8262.5713 - custom_mae: 59.511 - ETA: 23:49 - loss: 8258.5039 - custom_mae: 59.494 - ETA: 23:34 - loss: 8253.4756 - custom_mae: 59.473 - ETA: 23:20 - loss: 8248.8145 - custom_mae: 59.451 - ETA: 23:05 - loss: 8244.2188 - custom_mae: 59.433 - ETA: 22:51 - loss: 8241.0576 - custom_mae: 59.420 - ETA: 22:36 - loss: 8236.8525 - custom_mae: 59.403 - ETA: 22:22 - loss: 8232.5146 - custom_mae: 59.383 - ETA: 22:08 - loss: 8228.3398 - custom_mae: 59.365 - ETA: 21:53 - loss: 8224.2227 - custom_mae: 59.345 - ETA: 21:39 - loss: 8219.8486 - custom_mae: 59.327 - ETA: 21:24 - loss: 8215.3105 - custom_mae: 59.307 - ETA: 21:10 - loss: 8210.2324 - custom_mae: 59.285 - ETA: 20:55 - loss: 8207.0312 - custom_mae: 59.274 - ETA: 20:41 - loss: 8203.3076 - custom_mae: 59.258 - ETA: 20:27 - loss: 8199.8066 - custom_mae: 59.244 - ETA: 20:12 - loss: 8196.2861 - custom_mae: 59.231 - ETA: 19:58 - loss: 8192.3135 - custom_mae: 59.216 - ETA: 19:43 - loss: 8187.9990 - custom_mae: 59.199 - ETA: 19:29 - loss: 8184.3208 - custom_mae: 59.184 - ETA: 19:14 - loss: 8179.3882 - custom_mae: 59.161 - ETA: 19:00 - loss: 8174.7173 - custom_mae: 59.141 - ETA: 18:46 - loss: 8171.1885 - custom_mae: 59.127 - ETA: 18:31 - loss: 8166.5508 - custom_mae: 59.106 - ETA: 18:17 - loss: 8163.0444 - custom_mae: 59.090 - ETA: 18:02 - loss: 8158.9805 - custom_mae: 59.072 - ETA: 17:48 - loss: 8154.0068 - custom_mae: 59.050 - ETA: 17:33 - loss: 8149.9873 - custom_mae: 59.033 - ETA: 17:19 - loss: 8146.1348 - custom_mae: 59.016 - ETA: 17:04 - loss: 8142.4146 - custom_mae: 59.001 - ETA: 16:50 - loss: 8139.0542 - custom_mae: 58.988 - ETA: 16:36 - loss: 8135.0425 - custom_mae: 58.971 - ETA: 16:21 - loss: 8131.0762 - custom_mae: 58.954 - ETA: 16:07 - loss: 8127.9502 - custom_mae: 58.941 - ETA: 15:52 - loss: 8124.2129 - custom_mae: 58.925 - ETA: 15:38 - loss: 8119.6104 - custom_mae: 58.906 - ETA: 15:23 - loss: 8116.1558 - custom_mae: 58.893 - ETA: 15:09 - loss: 8112.0928 - custom_mae: 58.875 - ETA: 14:55 - loss: 8107.3105 - custom_mae: 58.853 - ETA: 14:40 - loss: 8102.5317 - custom_mae: 58.833 - ETA: 14:26 - loss: 8099.1025 - custom_mae: 58.822 - ETA: 14:11 - loss: 8095.4341 - custom_mae: 58.805 - ETA: 13:57 - loss: 8091.9150 - custom_mae: 58.789 - ETA: 13:42 - loss: 8087.6196 - custom_mae: 58.770 - ETA: 13:28 - loss: 8083.7354 - custom_mae: 58.755 - ETA: 13:13 - loss: 8079.9546 - custom_mae: 58.739 - ETA: 12:59 - loss: 8075.9014 - custom_mae: 58.722 - ETA: 12:45 - loss: 8072.2915 - custom_mae: 58.706 - ETA: 12:30 - loss: 8069.6704 - custom_mae: 58.697 - ETA: 12:16 - loss: 8065.2637 - custom_mae: 58.678 - ETA: 12:01 - loss: 8061.6919 - custom_mae: 58.663 - ETA: 11:47 - loss: 8058.2280 - custom_mae: 58.649 - ETA: 11:32 - loss: 8054.4619 - custom_mae: 58.632 - ETA: 11:18 - loss: 8050.0947 - custom_mae: 58.612 - ETA: 11:04 - loss: 8048.0508 - custom_mae: 58.604 - ETA: 10:49 - loss: 8044.0117 - custom_mae: 58.586 - ETA: 10:35 - loss: 8040.7729 - custom_mae: 58.573 - ETA: 10:20 - loss: 8037.4741 - custom_mae: 58.559 - ETA: 10:06 - loss: 8033.9702 - custom_mae: 58.543 - ETA: 9:51 - loss: 8030.2275 - custom_mae: 58.528 - ETA: 9:37 - loss: 8025.4692 - custom_mae: 58.5087"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - ETA: 9:23 - loss: 8021.6021 - custom_mae: 58.49 - ETA: 9:08 - loss: 8017.5361 - custom_mae: 58.47 - ETA: 8:54 - loss: 8014.3701 - custom_mae: 58.45 - ETA: 8:39 - loss: 8012.2012 - custom_mae: 58.44 - ETA: 8:25 - loss: 8008.2607 - custom_mae: 58.43 - ETA: 8:10 - loss: 8004.5640 - custom_mae: 58.41 - ETA: 7:56 - loss: 8000.7808 - custom_mae: 58.39 - ETA: 7:41 - loss: 7997.7314 - custom_mae: 58.38 - ETA: 7:27 - loss: 7993.4512 - custom_mae: 58.37 - ETA: 7:13 - loss: 7989.1885 - custom_mae: 58.34 - ETA: 6:58 - loss: 7985.0034 - custom_mae: 58.33 - ETA: 6:44 - loss: 7981.1831 - custom_mae: 58.31 - ETA: 6:29 - loss: 7977.3203 - custom_mae: 58.29 - ETA: 6:15 - loss: 7973.7983 - custom_mae: 58.28 - ETA: 6:00 - loss: 7970.3291 - custom_mae: 58.26 - ETA: 5:46 - loss: 7967.6548 - custom_mae: 58.25 - ETA: 5:32 - loss: 7964.1489 - custom_mae: 58.24 - ETA: 5:17 - loss: 7960.9644 - custom_mae: 58.22 - ETA: 5:03 - loss: 7958.1855 - custom_mae: 58.21 - ETA: 4:48 - loss: 7954.5059 - custom_mae: 58.20 - ETA: 4:34 - loss: 7951.0186 - custom_mae: 58.18 - ETA: 4:19 - loss: 7946.9219 - custom_mae: 58.16 - ETA: 4:05 - loss: 7943.4800 - custom_mae: 58.15 - ETA: 3:50 - loss: 7940.2803 - custom_mae: 58.14 - ETA: 3:36 - loss: 7936.4316 - custom_mae: 58.12 - ETA: 3:22 - loss: 7931.5420 - custom_mae: 58.10 - ETA: 3:07 - loss: 7927.1455 - custom_mae: 58.08 - ETA: 2:53 - loss: 7923.8281 - custom_mae: 58.06 - ETA: 2:38 - loss: 7919.9824 - custom_mae: 58.04 - ETA: 2:24 - loss: 7915.9468 - custom_mae: 58.03 - ETA: 2:09 - loss: 7912.7568 - custom_mae: 58.01 - ETA: 1:55 - loss: 7909.4688 - custom_mae: 58.00 - ETA: 1:41 - loss: 7905.2930 - custom_mae: 57.98 - ETA: 1:26 - loss: 7901.0693 - custom_mae: 57.96 - ETA: 1:12 - loss: 7897.1118 - custom_mae: 57.95 - ETA: 57s - loss: 7892.3940 - custom_mae: 57.9303 - ETA: 43s - loss: 7887.9424 - custom_mae: 57.910 - ETA: 28s - loss: 7885.1899 - custom_mae: 57.899 - ETA: 14s - loss: 7881.1899 - custom_mae: 57.882 - ETA: 0s - loss: 7877.2905 - custom_mae: 57.8649 \n",
      "Epoch 00001: val_custom_mae improved from inf to 56.73635, saving model to ..\\fast_output\\SYNTH_Regression_MSE_RESNET\\2020-05-28_Angular_Base_Custom-MAE\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "1250/1250 [==============================] - 18630s 15s/step - loss: 7877.2905 - custom_mae: 57.8649 - val_loss: 6335.5776 - val_custom_mae: 56.7364\n",
      "Epoch 2/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 152/1250 [==>...........................] - ETA: 0s - loss: 2129.6963 - custom_mae: 33.19 - ETA: 2:28:00 - loss: 2665.3613 - custom_mae: 36.151 - ETA: 3:16:27 - loss: 2883.9141 - custom_mae: 36.470 - ETA: 3:40:25 - loss: 2966.3499 - custom_mae: 37.536 - ETA: 3:55:53 - loss: 3114.2144 - custom_mae: 37.813 - ETA: 4:05:59 - loss: 3245.5352 - custom_mae: 37.957 - ETA: 4:12:32 - loss: 3269.2283 - custom_mae: 38.030 - ETA: 4:18:01 - loss: 3360.6099 - custom_mae: 38.511 - ETA: 4:21:39 - loss: 3455.0618 - custom_mae: 39.015 - ETA: 4:24:17 - loss: 3338.1062 - custom_mae: 38.201 - ETA: 4:26:49 - loss: 3260.4297 - custom_mae: 37.679 - ETA: 4:29:06 - loss: 3218.5144 - custom_mae: 37.304 - ETA: 4:30:37 - loss: 3177.7288 - custom_mae: 37.121 - ETA: 4:32:01 - loss: 3220.3972 - custom_mae: 37.290 - ETA: 4:33:13 - loss: 3384.0764 - custom_mae: 38.179 - ETA: 4:34:11 - loss: 3384.4072 - custom_mae: 38.198 - ETA: 4:34:59 - loss: 3404.3130 - custom_mae: 38.223 - ETA: 4:35:48 - loss: 3386.3350 - custom_mae: 38.218 - ETA: 4:36:19 - loss: 3410.4851 - custom_mae: 38.255 - ETA: 4:36:53 - loss: 3496.0981 - custom_mae: 38.747 - ETA: 4:37:18 - loss: 3480.1421 - custom_mae: 38.644 - ETA: 4:37:35 - loss: 3492.9111 - custom_mae: 38.698 - ETA: 4:37:54 - loss: 3462.6387 - custom_mae: 38.594 - ETA: 4:38:22 - loss: 3485.0801 - custom_mae: 38.705 - ETA: 4:38:35 - loss: 3479.7502 - custom_mae: 38.678 - ETA: 4:38:47 - loss: 3463.2778 - custom_mae: 38.665 - ETA: 4:39:09 - loss: 3428.6377 - custom_mae: 38.497 - ETA: 4:39:15 - loss: 3448.2949 - custom_mae: 38.585 - ETA: 4:39:19 - loss: 3407.4885 - custom_mae: 38.403 - ETA: 4:39:25 - loss: 3491.4700 - custom_mae: 38.788 - ETA: 4:39:29 - loss: 3483.8044 - custom_mae: 38.725 - ETA: 4:39:30 - loss: 3485.2527 - custom_mae: 38.760 - ETA: 4:39:35 - loss: 3470.7439 - custom_mae: 38.626 - ETA: 4:39:43 - loss: 3461.1638 - custom_mae: 38.551 - ETA: 4:39:44 - loss: 3469.4148 - custom_mae: 38.549 - ETA: 4:39:45 - loss: 3466.2668 - custom_mae: 38.564 - ETA: 4:39:40 - loss: 3468.6912 - custom_mae: 38.598 - ETA: 4:39:35 - loss: 3483.8911 - custom_mae: 38.698 - ETA: 4:39:31 - loss: 3470.6418 - custom_mae: 38.610 - ETA: 4:39:30 - loss: 3464.0984 - custom_mae: 38.560 - ETA: 4:39:24 - loss: 3450.3860 - custom_mae: 38.507 - ETA: 4:39:18 - loss: 3451.0269 - custom_mae: 38.552 - ETA: 4:39:18 - loss: 3450.3364 - custom_mae: 38.587 - ETA: 4:39:10 - loss: 3443.7424 - custom_mae: 38.549 - ETA: 4:39:08 - loss: 3433.6086 - custom_mae: 38.482 - ETA: 4:39:03 - loss: 3446.4243 - custom_mae: 38.553 - ETA: 4:38:58 - loss: 3435.9827 - custom_mae: 38.507 - ETA: 4:38:58 - loss: 3436.1484 - custom_mae: 38.515 - ETA: 4:38:53 - loss: 3411.6284 - custom_mae: 38.373 - ETA: 4:38:46 - loss: 3438.7419 - custom_mae: 38.466 - ETA: 4:38:42 - loss: 3454.1272 - custom_mae: 38.537 - ETA: 4:38:36 - loss: 3415.2053 - custom_mae: 38.336 - ETA: 4:38:33 - loss: 3424.0574 - custom_mae: 38.354 - ETA: 4:38:32 - loss: 3433.8889 - custom_mae: 38.428 - ETA: 4:38:26 - loss: 3434.3679 - custom_mae: 38.480 - ETA: 4:38:27 - loss: 3433.0281 - custom_mae: 38.510 - ETA: 4:38:36 - loss: 3437.3704 - custom_mae: 38.530 - ETA: 4:38:28 - loss: 3432.3103 - custom_mae: 38.513 - ETA: 4:38:19 - loss: 3442.0540 - custom_mae: 38.573 - ETA: 4:38:15 - loss: 3426.4348 - custom_mae: 38.503 - ETA: 4:38:08 - loss: 3414.0815 - custom_mae: 38.428 - ETA: 4:38:12 - loss: 3395.7080 - custom_mae: 38.361 - ETA: 4:38:16 - loss: 3402.0576 - custom_mae: 38.388 - ETA: 4:38:28 - loss: 3395.3584 - custom_mae: 38.340 - ETA: 4:38:34 - loss: 3393.0535 - custom_mae: 38.344 - ETA: 4:38:31 - loss: 3422.8188 - custom_mae: 38.487 - ETA: 4:38:35 - loss: 3420.2573 - custom_mae: 38.484 - ETA: 4:38:30 - loss: 3427.9792 - custom_mae: 38.524 - ETA: 4:38:27 - loss: 3423.5598 - custom_mae: 38.481 - ETA: 4:38:26 - loss: 3426.7102 - custom_mae: 38.501 - ETA: 4:38:19 - loss: 3428.1736 - custom_mae: 38.529 - ETA: 4:38:09 - loss: 3430.5381 - custom_mae: 38.532 - ETA: 4:37:56 - loss: 3428.1987 - custom_mae: 38.510 - ETA: 4:37:50 - loss: 3414.0366 - custom_mae: 38.440 - ETA: 4:37:42 - loss: 3413.6931 - custom_mae: 38.447 - ETA: 4:37:32 - loss: 3404.6748 - custom_mae: 38.415 - ETA: 4:37:24 - loss: 3392.9385 - custom_mae: 38.360 - ETA: 4:37:13 - loss: 3405.7876 - custom_mae: 38.449 - ETA: 4:37:02 - loss: 3400.2871 - custom_mae: 38.415 - ETA: 4:36:56 - loss: 3395.2437 - custom_mae: 38.386 - ETA: 4:36:44 - loss: 3392.6003 - custom_mae: 38.372 - ETA: 4:36:34 - loss: 3389.0510 - custom_mae: 38.378 - ETA: 4:36:22 - loss: 3385.3845 - custom_mae: 38.330 - ETA: 4:36:10 - loss: 3379.9900 - custom_mae: 38.293 - ETA: 4:35:58 - loss: 3371.7100 - custom_mae: 38.236 - ETA: 4:35:50 - loss: 3364.1523 - custom_mae: 38.201 - ETA: 4:35:38 - loss: 3355.6221 - custom_mae: 38.150 - ETA: 4:35:28 - loss: 3350.9417 - custom_mae: 38.139 - ETA: 4:35:20 - loss: 3344.2922 - custom_mae: 38.105 - ETA: 4:35:14 - loss: 3344.7034 - custom_mae: 38.122 - ETA: 4:35:13 - loss: 3339.2915 - custom_mae: 38.083 - ETA: 4:35:11 - loss: 3345.6404 - custom_mae: 38.115 - ETA: 4:35:06 - loss: 3342.1243 - custom_mae: 38.118 - ETA: 4:35:00 - loss: 3340.9446 - custom_mae: 38.113 - ETA: 4:34:56 - loss: 3346.6475 - custom_mae: 38.161 - ETA: 4:34:49 - loss: 3341.2727 - custom_mae: 38.120 - ETA: 4:34:44 - loss: 3327.6191 - custom_mae: 38.052 - ETA: 4:34:42 - loss: 3321.4695 - custom_mae: 38.024 - ETA: 4:34:38 - loss: 3314.7480 - custom_mae: 37.983 - ETA: 4:34:32 - loss: 3310.7207 - custom_mae: 37.955 - ETA: 4:34:25 - loss: 3304.4932 - custom_mae: 37.894 - ETA: 4:34:17 - loss: 3294.7852 - custom_mae: 37.809 - ETA: 4:34:02 - loss: 3291.4810 - custom_mae: 37.793 - ETA: 4:33:49 - loss: 3296.2295 - custom_mae: 37.815 - ETA: 4:33:40 - loss: 3294.8481 - custom_mae: 37.789 - ETA: 4:33:27 - loss: 3285.1533 - custom_mae: 37.751 - ETA: 4:33:13 - loss: 3287.1860 - custom_mae: 37.743 - ETA: 4:32:58 - loss: 3286.5281 - custom_mae: 37.746 - ETA: 4:32:48 - loss: 3285.6792 - custom_mae: 37.735 - ETA: 4:32:40 - loss: 3286.1685 - custom_mae: 37.733 - ETA: 4:32:32 - loss: 3292.7805 - custom_mae: 37.755 - ETA: 4:32:22 - loss: 3290.9343 - custom_mae: 37.752 - ETA: 4:32:14 - loss: 3291.3333 - custom_mae: 37.755 - ETA: 4:31:59 - loss: 3284.2278 - custom_mae: 37.719 - ETA: 4:31:44 - loss: 3289.0002 - custom_mae: 37.748 - ETA: 4:31:29 - loss: 3283.0503 - custom_mae: 37.710 - ETA: 4:31:13 - loss: 3286.0913 - custom_mae: 37.740 - ETA: 4:30:58 - loss: 3292.9431 - custom_mae: 37.763 - ETA: 4:30:43 - loss: 3294.4556 - custom_mae: 37.768 - ETA: 4:30:27 - loss: 3285.1050 - custom_mae: 37.713 - ETA: 4:30:11 - loss: 3280.3054 - custom_mae: 37.677 - ETA: 4:29:57 - loss: 3273.6541 - custom_mae: 37.648 - ETA: 4:29:42 - loss: 3272.6311 - custom_mae: 37.644 - ETA: 4:29:27 - loss: 3266.2629 - custom_mae: 37.618 - ETA: 4:29:13 - loss: 3268.5239 - custom_mae: 37.635 - ETA: 4:28:58 - loss: 3263.8496 - custom_mae: 37.635 - ETA: 4:28:43 - loss: 3265.0452 - custom_mae: 37.655 - ETA: 4:28:29 - loss: 3268.4539 - custom_mae: 37.677 - ETA: 4:28:14 - loss: 3271.8831 - custom_mae: 37.693 - ETA: 4:28:00 - loss: 3276.1626 - custom_mae: 37.687 - ETA: 4:27:45 - loss: 3268.1179 - custom_mae: 37.638 - ETA: 4:27:28 - loss: 3262.1348 - custom_mae: 37.599 - ETA: 4:27:13 - loss: 3259.7468 - custom_mae: 37.591 - ETA: 4:26:58 - loss: 3263.4407 - custom_mae: 37.600 - ETA: 4:26:42 - loss: 3255.9165 - custom_mae: 37.557 - ETA: 4:26:28 - loss: 3259.5840 - custom_mae: 37.563 - ETA: 4:26:14 - loss: 3258.2454 - custom_mae: 37.562 - ETA: 4:25:59 - loss: 3262.4373 - custom_mae: 37.582 - ETA: 4:25:44 - loss: 3258.4260 - custom_mae: 37.553 - ETA: 4:25:29 - loss: 3262.6426 - custom_mae: 37.575 - ETA: 4:25:13 - loss: 3269.7227 - custom_mae: 37.604 - ETA: 4:24:57 - loss: 3272.7039 - custom_mae: 37.622 - ETA: 4:24:43 - loss: 3275.1799 - custom_mae: 37.642 - ETA: 4:24:28 - loss: 3271.2556 - custom_mae: 37.623 - ETA: 4:24:14 - loss: 3276.4373 - custom_mae: 37.665 - ETA: 4:24:00 - loss: 3272.7520 - custom_mae: 37.634 - ETA: 4:23:45 - loss: 3268.2595 - custom_mae: 37.618 - ETA: 4:23:30 - loss: 3264.6482 - custom_mae: 37.605 - ETA: 4:23:16 - loss: 3260.1655 - custom_mae: 37.584 - ETA: 4:23:01 - loss: 3255.7595 - custom_mae: 37.550 - ETA: 4:22:44 - loss: 3252.5408 - custom_mae: 37.530 - ETA: 4:22:30 - loss: 3252.5566 - custom_mae: 37.5330"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 210/1250 [====>.........................] - ETA: 4:22:15 - loss: 3251.9355 - custom_mae: 37.537 - ETA: 4:22:00 - loss: 3246.2183 - custom_mae: 37.509 - ETA: 4:21:46 - loss: 3244.8884 - custom_mae: 37.481 - ETA: 4:21:32 - loss: 3256.1199 - custom_mae: 37.532 - ETA: 4:21:17 - loss: 3257.0876 - custom_mae: 37.534 - ETA: 4:21:03 - loss: 3254.7771 - custom_mae: 37.514 - ETA: 4:20:48 - loss: 3253.2886 - custom_mae: 37.499 - ETA: 4:20:33 - loss: 3257.4514 - custom_mae: 37.534 - ETA: 4:20:18 - loss: 3256.6777 - custom_mae: 37.533 - ETA: 4:20:04 - loss: 3253.8057 - custom_mae: 37.523 - ETA: 4:19:48 - loss: 3253.3804 - custom_mae: 37.511 - ETA: 4:19:33 - loss: 3256.8467 - custom_mae: 37.539 - ETA: 4:19:20 - loss: 3257.9761 - custom_mae: 37.547 - ETA: 4:19:05 - loss: 3248.9944 - custom_mae: 37.488 - ETA: 4:18:49 - loss: 3251.5381 - custom_mae: 37.508 - ETA: 4:18:36 - loss: 3248.3315 - custom_mae: 37.499 - ETA: 4:18:20 - loss: 3246.0918 - custom_mae: 37.488 - ETA: 4:18:05 - loss: 3240.0332 - custom_mae: 37.450 - ETA: 4:17:50 - loss: 3249.3462 - custom_mae: 37.498 - ETA: 4:17:35 - loss: 3246.1309 - custom_mae: 37.485 - ETA: 4:17:21 - loss: 3246.8870 - custom_mae: 37.475 - ETA: 4:17:07 - loss: 3248.3013 - custom_mae: 37.483 - ETA: 4:16:53 - loss: 3247.3625 - custom_mae: 37.473 - ETA: 4:16:39 - loss: 3243.1470 - custom_mae: 37.437 - ETA: 4:16:24 - loss: 3244.7632 - custom_mae: 37.444 - ETA: 4:16:12 - loss: 3238.3311 - custom_mae: 37.416 - ETA: 4:16:01 - loss: 3240.2024 - custom_mae: 37.416 - ETA: 4:15:50 - loss: 3239.6301 - custom_mae: 37.408 - ETA: 4:15:35 - loss: 3232.3606 - custom_mae: 37.366 - ETA: 4:15:23 - loss: 3233.0024 - custom_mae: 37.365 - ETA: 4:15:09 - loss: 3230.5571 - custom_mae: 37.345 - ETA: 4:14:54 - loss: 3231.0266 - custom_mae: 37.351 - ETA: 4:14:40 - loss: 3232.3132 - custom_mae: 37.342 - ETA: 4:14:26 - loss: 3235.4089 - custom_mae: 37.352 - ETA: 4:14:13 - loss: 3233.0010 - custom_mae: 37.344 - ETA: 4:13:58 - loss: 3228.1274 - custom_mae: 37.316 - ETA: 4:13:44 - loss: 3224.1624 - custom_mae: 37.294 - ETA: 4:13:30 - loss: 3224.1172 - custom_mae: 37.294 - ETA: 4:13:16 - loss: 3223.6157 - custom_mae: 37.299 - ETA: 4:13:01 - loss: 3219.3625 - custom_mae: 37.284 - ETA: 4:12:47 - loss: 3212.8850 - custom_mae: 37.251 - ETA: 4:12:32 - loss: 3205.6807 - custom_mae: 37.213 - ETA: 4:12:20 - loss: 3202.2493 - custom_mae: 37.176 - ETA: 4:12:06 - loss: 3201.2874 - custom_mae: 37.171 - ETA: 4:11:52 - loss: 3200.9124 - custom_mae: 37.174 - ETA: 4:11:38 - loss: 3203.0024 - custom_mae: 37.191 - ETA: 4:11:24 - loss: 3205.0867 - custom_mae: 37.192 - ETA: 4:11:10 - loss: 3202.2693 - custom_mae: 37.173 - ETA: 4:10:57 - loss: 3200.5945 - custom_mae: 37.172 - ETA: 4:10:42 - loss: 3203.3447 - custom_mae: 37.186 - ETA: 4:10:28 - loss: 3206.8572 - custom_mae: 37.194 - ETA: 4:10:14 - loss: 3202.6025 - custom_mae: 37.168 - ETA: 4:09:59 - loss: 3198.9478 - custom_mae: 37.148 - ETA: 4:09:45 - loss: 3195.1252 - custom_mae: 37.122 - ETA: 4:09:30 - loss: 3196.2620 - custom_mae: 37.117 - ETA: 4:09:16 - loss: 3196.9048 - custom_mae: 37.117 - ETA: 4:09:01 - loss: 3195.1880 - custom_mae: 37.105 - ETA: 4:08:46 - loss: 3191.9524 - custom_mae: 37.0776"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    #for top_results_index in range(3):\n",
    "    for top_results_index in [0, 1]:\n",
    "        #top_results_index = 1\n",
    "        _MODEL_TO_LOAD_INDEX = 0 #df.iloc[top_results_index].name\n",
    "        _MODEL_TO_LOAD = 'Best_Weights_FC_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        _TMP_DIR = '..\\\\TMP_TALOS_{}'.format(_DEVICE)\n",
    "        _CSV_RESULTS = _LOG_DIR + 'Talos_Results_Fine_Idx{}.csv'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        startTime = datetime.now()\n",
    "        \n",
    "        parameters = get_params(top_results_index)\n",
    "\n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = grid_model_fine,\n",
    "            params = parameters,\n",
    "            experiment_name = _TMP_DIR,\n",
    "            #shuffle=False,\n",
    "            reduction_metric = parameters['reduction_metric'][0],\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = True\n",
    "        )\n",
    "\n",
    "        print(\"Time taken:\", datetime.now() - startTime)\n",
    "        \n",
    "        print('Writing Device File')\n",
    "        device_file.write('Trained Model: {}'.format(_MODEL_TO_LOAD))\n",
    "\n",
    "        df_experiment_results = pd.read_csv(_TMP_DIR + '\\\\' + os.listdir(_TMP_DIR)[0])\n",
    "        df_experiment_results['Base'] = None\n",
    "        for i in range(df_experiment_results.shape[0]):\n",
    "            df_experiment_results['Base'][i] = _MODEL_TO_LOAD_INDEX\n",
    "\n",
    "        if os.path.isfile(_CSV_RESULTS):\n",
    "            df_experiment_results.to_csv(_CSV_RESULTS, mode = 'a', index = False, header = False)\n",
    "        else:\n",
    "            df_experiment_results.to_csv(_CSV_RESULTS, index = False)\n",
    "\n",
    "        shutil.rmtree(_TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Results to NAS if SSD Directory was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_directory(src, dst, symlinks = False, ignore = None):\n",
    "    maxLen = 0\n",
    "    message = ''        \n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        \n",
    "        message = 'Creating Path: {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        os.makedirs(dst)\n",
    "        \n",
    "    for item in os.listdir(src):\n",
    "        \n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        \n",
    "        if os.path.isdir(s):\n",
    "            \n",
    "            message = 'Copying Directory: {}'.format(s)\n",
    "            maxLen = max(maxLen, len(message))\n",
    "            print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "            \n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if not os.path.exists(d): #or os.stat(s).st_mtime - os.stat(d).st_mtime > 1:\n",
    "                \n",
    "                message = 'Copying File: {}'.format(s)\n",
    "                maxLen = max(maxLen, len(message))\n",
    "                print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "                \n",
    "                shutil.copy2(s, d)\n",
    "        \n",
    "        time.sleep(.5)\n",
    "     \n",
    "    message = 'Coyping... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "\n",
    "def delete_directory(src, terminator = '\\n'):\n",
    "    message = ''\n",
    "    maxLen = 0\n",
    "    \n",
    "    try:\n",
    "        message = 'Deleting {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        shutil.rmtree(src)\n",
    "        \n",
    "    except OSError as e:\n",
    "        message = 'Error: {} : {}'.format(src, e.strerror)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "        return\n",
    "    \n",
    "    message = 'Deleting... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = terminator)\n",
    "\n",
    "    \n",
    "def copy_fine_training(src, dst):\n",
    "    copy_directory(src, dst)\n",
    "    delete_directory(src, terminator = '\\r')\n",
    "    delete_directory(src + '..\\\\', terminator = '\\r')\n",
    "    if not os.listdir(src + '..\\\\..\\\\'):\n",
    "        delete_directory(src + '..\\\\..\\\\', terminator = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(storage == OutputDirectory.SSD):\n",
    "    _COPY_DIR = '..\\\\output\\\\{}'.format(_NET_DIR)\n",
    "    copy_fine_training(_LOG_DIR, _COPY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name = \"CMSE.Mixed\"></a><a href = #Top>Up</a></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
