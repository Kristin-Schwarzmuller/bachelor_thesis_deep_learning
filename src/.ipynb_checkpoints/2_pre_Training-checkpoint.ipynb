{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Training on GPU 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links <a name = \"Top\"></a>\n",
    "\n",
    "<ol>\n",
    "<li><a href = #setup>Begin Training</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Conda Environment: tf_ks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "print('Current Conda Environment: {}'.format(os.environ['CONDA_DEFAULT_ENV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  1 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3063298382326486677\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 20264236482\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12036620198010078670\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s = 1):\n",
    "    os.environ['PYTHONHASHSEED']='0'\n",
    "\n",
    "    seed(s)\n",
    "    tf.random.set_seed(s)\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "set_seed()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enum für Training-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TrainingSet(Enum):\n",
    "    SYNTHETIC = 1\n",
    "    REAL = 2\n",
    "    MIXED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "\n",
    "* <i>SSD</i>, falls genug Speicher auf SSD im SymLink <i>fast_output</i> verfügbar ist\n",
    "* <i>HDD</i>, falls möglicherweise zu wenig SSD-Speicher verfügbar ist $\\rightarrow$ <i>output</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class OutputDirectory(IntEnum):\n",
    "    HDD = 0\n",
    "    SSD = 1\n",
    "    \n",
    "output_path = ['output', 'fast_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype='float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Label_Type into suitable label names.\n",
    "$\\Rightarrow$ Angular / Normalized $\\rightarrow$ ['Elevation', 'Azimuth']\n",
    "\n",
    "$\\Rightarrow$ Stereographic $\\rightarrow$ ['S_x', 'S_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Label_Names(label_type):\n",
    "    if label_type == 'Angular' or label_type == 'Normalized':\n",
    "        return ['Elevation', 'Azimuth']\n",
    "    elif label_type == 'Stereographic':\n",
    "        return ['S_x', 'S_y']\n",
    "    else:\n",
    "        assert(True, 'LabelType Invalid')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reduction_Metric(metric):\n",
    "    \n",
    "    if metric == 'custom_mae':\n",
    "        return [custom_mae]\n",
    "    elif metric == 'tf.keras.metrics.MeanAbsoluteError()':\n",
    "        return [tf.keras.metrics.MeanAbsoluteError()]\n",
    "    elif metric == 'circular_mae':\n",
    "        return [circular_mae]\n",
    "    elif metric == 'mean_squared_error':\n",
    "        return ['mean_squared_error']\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'keras.optimizers.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Nadam'>\":\n",
    "        return Nadam\n",
    "    elif optimizer == \"<class 'keras.optimizers.Adagard'>\":\n",
    "        return Adagard\n",
    "    elif optimizer == \"<class 'keras.optimizers.RMSprop'>\":\n",
    "        return RMSprop\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsset-Typ nach String Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_to_string(ts):\n",
    "    if ts == TrainingSet.SYNTHETIC:\n",
    "        return 'Synth'\n",
    "    elif ts == TrainingSet.REAL:\n",
    "        return 'Real'\n",
    "    elif ts == TrainingSet.MIXED:\n",
    "        return 'Mixed'\n",
    "    else:\n",
    "        print('Unknown TrainingSet')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(batch_size, num_samples, label_type):\n",
    "    # if Block für synthetische Daten, um nur auf realen Daten zu trainieren _USE_SYNTHETIC_TRAIN_DATA\n",
    "    # 1. lege df_train und df_valid als leere Liste an\n",
    "    # 2. If-block um Zeile df = ... bis df_valid\n",
    "    \n",
    "    if trainingset == TrainingSet.SYNTHETIC:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "    elif trainingset == TrainingSet.MIXED:\n",
    "        df = pd.read_csv(_CSV_FILE)\n",
    "        df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "        df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "        df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "        \n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train_real = df_shuffled_real[0: int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid_real = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train_real.shape[0]])\n",
    "        df_train = df_train.drop(df_train.index[df_train.shape[0] - df_train_real.shape[0] : df_train.shape[0]])\n",
    "        df_valid = df_valid.drop(df_valid.index[df_valid.shape[0] - df_valid_real.shape[0] : df_valid.shape[0]])\n",
    "        df_train = df_train.append(df_train_real)\n",
    "        df_valid= df_valid.append(df_valid_real)\n",
    "    \n",
    "    elif trainingset == TrainingSet.REAL: # Add check for num_samples, once the real dataset increases\n",
    "        df_real = pd.read_csv(_CSV_FILE_REAL)\n",
    "        df_shuffled_real = df_real.sample(frac = 1, random_state = 1)\n",
    "        df_shuffled_real = df_shuffled_real.drop(df_shuffled_real.index[(df_shuffled_real.shape[0] - 61) : df_shuffled_real.shape[0]])\n",
    "        df_train = df_shuffled_real[0 : int(df_shuffled_real.shape[0] * 0.8 // batch_size * batch_size)]   \n",
    "        df_valid = df_shuffled_real.drop(df_shuffled_real.index[0 : df_train.shape[0]])\n",
    "        \n",
    "    else:\n",
    "        print('Create_Data :: should not have reached here')\n",
    "        \n",
    "\n",
    "        \n",
    "    if _USE_DATA_AUGMENTATION:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.25, 0.75),\n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    print('Y-Col: {}'.format(get_Label_Names(label_type)))\n",
    "    print('Train Data Generator: ', end = '')\n",
    "    \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    print('Validation Data Generator: ', end = '')\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = _IMAGE_DIR,\n",
    "        x_col = 'Filename',\n",
    "        y_col = get_Label_Names(label_type),\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Modell (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_model_fine(x, y, x_val, y_val, params):\n",
    "    print('==========================Params:')\n",
    "    print(params)\n",
    "    print('==========================')\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data(params['batch_size'], params['samples'], params['label_type'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    "    dropout_rate = params['dropout']\n",
    "    first_neuron = params['first_neuron']\n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = params['leaky_alpha'])\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    if(_NET == 'VGG16'):\n",
    "        cnn = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "    elif(_NET == 'RESNET'):\n",
    "        cnn = ResNet50(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "    else:\n",
    "        print('ERROR NET SPELLED WRONG')\n",
    "        \n",
    "    for layer in cnn.layers[:15]:\n",
    "        layer.trainable = False\n",
    "        #print(layer.name, layer.trainable)\n",
    "        \n",
    "    print('_________________________________________________________________')\n",
    "    print('{:>16} {:>16}'.format('Network Layer', 'Trainable'))\n",
    "    print('=================================================================')\n",
    "    for layer in cnn.layers:\n",
    "        print('{:>16} {:>16}'.format(layer.name, layer.trainable))\n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    model.add(cnn)\n",
    "    \n",
    "    fc = Sequential()\n",
    "    fc.add(Flatten(input_shape = model.output_shape[1:])) # (7, 7, 512)\n",
    "    \n",
    "    fc.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    print('Number Hidden Layers {}'.format(params['hidden_layers']))\n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(params['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        fc.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        fc.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            fc.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    fc.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    fc.load_weights(_MODEL_DIR + _MODEL_TO_LOAD)\n",
    "    model.add(fc)\n",
    "    print('Fully Connected Layers added to Base Network')\n",
    "    \n",
    "    print('Using Loss: {} \\nand Reduction Metric: {}'.format(\n",
    "        params['loss_function'], \n",
    "        get_Reduction_Metric(params['reduction_metric'])))\n",
    "    \n",
    "    model.compile(\n",
    "        #optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])*1e-2),\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer']) * 1e-3),\n",
    "        loss = params['loss_function'],\n",
    "        metrics = get_Reduction_Metric(params['reduction_metric'])\n",
    "    )\n",
    "    print('Model was compiled')\n",
    "    print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = _LOG_DIR + 'CNN_Base_{}_Model_and_Weights_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        monitor =  params['monitor_value'],\n",
    "        verbose = 1,\n",
    "        save_weights_only = False,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    print('Checkpointer was created')\n",
    "    \n",
    "    csv_logger = callbacks.CSVLogger(\n",
    "        filename = _LOG_DIR + 'CNN_Base_{}_Logger_{}.csv'.format(_MODEL_TO_LOAD_INDEX, train_generator.n),\n",
    "        separator = ',',\n",
    "        append = False\n",
    "    )\n",
    "    print('CSV Logger was created')\n",
    "\n",
    "    lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 13,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        min_delta = 0.0001\n",
    "    )\n",
    "    print('Learning Rate Reducer was created')\n",
    "    \n",
    "    early_stopper = callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0,\n",
    "        #patience = 15,\n",
    "        patience = 20,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    print('Early Stopper was created')\n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_data = valid_generator,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        callbacks = [checkpointer, csv_logger, lr_reducer, early_stopper],\n",
    "        epochs = params['epochs'],\n",
    "        workers = 8\n",
    "    )\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feinoptimierung <a name = \"setup\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "global_hyper_parameter = {\n",
    "    'samples': None,\n",
    "    'epochs': None,\n",
    "    'batch_size': None,\n",
    "    'optimizer': None,\n",
    "    'lr': None,\n",
    "    'first_neuron': None,\n",
    "    'dropout': None,\n",
    "    'activation': None,\n",
    "    'leaky_alpha': None,\n",
    "    'hidden_layers': None,\n",
    "    # beginning from here, Values should only contain one single entry:\n",
    "    # ===============================================================\n",
    "    'label_type': ['Angular'], # Stereographic, Angular, Normalized\n",
    "    'loss_function': None,\n",
    "    'reduction_metric': None,\n",
    "    'monitor_value': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RUN = 'SYNTH'\n",
    "_LOSS = 'MSE'\n",
    "_DATASET_NAME = '201129_2031'\n",
    "_DEVICE = 'GeForce_RTX_2080_Ti'#'TITAN_GPU1'\n",
    "\n",
    "storage = OutputDirectory.SSD # 'fast_output' if ssd storage may suffice, 'output' otherwise\n",
    "\n",
    "if global_hyper_parameter['label_type'][0] == 'Stereographic':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_stereographic.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_stereographic.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Angular':\n",
    "    _CSV_SYNTH_FILE_NAME = 'labels_ks_RGB.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real.csv'\n",
    "    \n",
    "elif global_hyper_parameter['label_type'][0] == 'Normalized':\n",
    "    _CSV_SYNTH_FILE_NAME = 'images_synthetisch_normalized.csv'\n",
    "    _CSV_REAL_FILE_NAME = 'images_real_normalized.csv'\n",
    "    \n",
    "else:\n",
    "    assert(True, 'Label Type Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = TrainingSet.SYNTHETIC\n",
    "_USE_DATA_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_IMAGE_DIR = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(_DATASET_NAME)\n",
    "_CSV_FILE = _IMAGE_DIR + _CSV_SYNTH_FILE_NAME\n",
    "_CSV_FILE_REAL = _IMAGE_DIR + _CSV_REAL_FILE_NAME\n",
    "\n",
    "_note = '_Custom-MAE'\n",
    "\n",
    "_NET = 'RESNET' # RESNET vs VGG16\n",
    "\n",
    "_MODEL_DIR = '..\\\\output\\\\{}_Regression_{}\\\\{}_{}_Base{}\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "_NET_DIR = '{}_Regression_{}\\\\{}_{}_Top_1{}\\\\{}_TD\\\\'.format(_RUN, _LOSS, _DATASET_NAME, global_hyper_parameter['label_type'][0], _note, trainingset_to_string(trainingset))\n",
    "_LOG_DIR = '..\\\\{}\\\\{}'.format(output_path[storage], _NET_DIR)\n",
    "\n",
    "if(not os.path.exists(_LOG_DIR)):\n",
    "    os.makedirs(_LOG_DIR)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(_LOG_DIR))\n",
    "\n",
    "device_file = open(_LOG_DIR + '{}.txt'.format(_DEVICE), \"a+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 FC-Gewichte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: ..\\output\\SYNTH_Regression_MSE\\201129_2031_Angular_Base_Custom-MAE\\..\\201129_2031_Angular_Base_Custom-MAE_Results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>loss</th>\n",
       "      <th>custom_mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_custom_mae</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>label_type</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>lr</th>\n",
       "      <th>monitor_value</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>reduction_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12/07/20-183456</td>\n",
       "      <td>12/07/20-184748</td>\n",
       "      <td>772.346002</td>\n",
       "      <td>5270.327148</td>\n",
       "      <td>50.029804</td>\n",
       "      <td>4460.808105</td>\n",
       "      <td>43.551151</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>210</td>\n",
       "      <td>12/08/20-103642</td>\n",
       "      <td>12/08/20-104622</td>\n",
       "      <td>580.297004</td>\n",
       "      <td>5304.337891</td>\n",
       "      <td>49.881985</td>\n",
       "      <td>4546.641602</td>\n",
       "      <td>44.258087</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>12/08/20-094221</td>\n",
       "      <td>12/08/20-094714</td>\n",
       "      <td>292.944500</td>\n",
       "      <td>5927.089355</td>\n",
       "      <td>54.068233</td>\n",
       "      <td>4472.035645</td>\n",
       "      <td>44.351097</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>211</td>\n",
       "      <td>12/08/20-104623</td>\n",
       "      <td>12/08/20-105603</td>\n",
       "      <td>580.146001</td>\n",
       "      <td>5644.216797</td>\n",
       "      <td>51.848598</td>\n",
       "      <td>4556.329590</td>\n",
       "      <td>44.426178</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>183</td>\n",
       "      <td>12/08/20-085308</td>\n",
       "      <td>12/08/20-085536</td>\n",
       "      <td>148.523002</td>\n",
       "      <td>5588.403320</td>\n",
       "      <td>52.872990</td>\n",
       "      <td>4465.057129</td>\n",
       "      <td>44.651348</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>184</td>\n",
       "      <td>12/08/20-085537</td>\n",
       "      <td>12/08/20-085806</td>\n",
       "      <td>148.669999</td>\n",
       "      <td>5780.605469</td>\n",
       "      <td>54.269981</td>\n",
       "      <td>4447.094238</td>\n",
       "      <td>44.738457</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>12/08/20-093727</td>\n",
       "      <td>12/08/20-094220</td>\n",
       "      <td>292.659001</td>\n",
       "      <td>5605.941895</td>\n",
       "      <td>52.833996</td>\n",
       "      <td>4601.835938</td>\n",
       "      <td>44.808548</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>12/08/20-092300</td>\n",
       "      <td>12/08/20-092749</td>\n",
       "      <td>289.176500</td>\n",
       "      <td>5275.107910</td>\n",
       "      <td>50.352421</td>\n",
       "      <td>4632.517578</td>\n",
       "      <td>44.932785</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>181</td>\n",
       "      <td>12/08/20-084812</td>\n",
       "      <td>12/08/20-085039</td>\n",
       "      <td>147.158491</td>\n",
       "      <td>5403.254395</td>\n",
       "      <td>51.578785</td>\n",
       "      <td>4533.847168</td>\n",
       "      <td>44.964966</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>214</td>\n",
       "      <td>12/08/20-111547</td>\n",
       "      <td>12/08/20-112544</td>\n",
       "      <td>597.896999</td>\n",
       "      <td>7338.146484</td>\n",
       "      <td>55.755199</td>\n",
       "      <td>4518.758301</td>\n",
       "      <td>45.057968</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>Angular</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2</td>\n",
       "      <td>val_custom_mae</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>custom_mae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0            start              end    duration         loss  \\\n",
       "0             0  12/07/20-183456  12/07/20-184748  772.346002  5270.327148   \n",
       "210         210  12/08/20-103642  12/08/20-104622  580.297004  5304.337891   \n",
       "199         199  12/08/20-094221  12/08/20-094714  292.944500  5927.089355   \n",
       "211         211  12/08/20-104623  12/08/20-105603  580.146001  5644.216797   \n",
       "183         183  12/08/20-085308  12/08/20-085536  148.523002  5588.403320   \n",
       "184         184  12/08/20-085537  12/08/20-085806  148.669999  5780.605469   \n",
       "198         198  12/08/20-093727  12/08/20-094220  292.659001  5605.941895   \n",
       "195         195  12/08/20-092300  12/08/20-092749  289.176500  5275.107910   \n",
       "181         181  12/08/20-084812  12/08/20-085039  147.158491  5403.254395   \n",
       "214         214  12/08/20-111547  12/08/20-112544  597.896999  7338.146484   \n",
       "\n",
       "     custom_mae     val_loss  val_custom_mae activation  batch_size  dropout  \\\n",
       "0     50.029804  4460.808105       43.551151  leakyrelu          32     0.25   \n",
       "210   49.881985  4546.641602       44.258087       relu          32     0.25   \n",
       "199   54.068233  4472.035645       44.351097       relu          32     0.25   \n",
       "211   51.848598  4556.329590       44.426178       relu          32     0.25   \n",
       "183   52.872990  4465.057129       44.651348       relu          32     0.25   \n",
       "184   54.269981  4447.094238       44.738457       relu          32     0.25   \n",
       "198   52.833996  4601.835938       44.808548       relu          32     0.25   \n",
       "195   50.352421  4632.517578       44.932785       relu          32     0.25   \n",
       "181   51.578785  4533.847168       44.964966       relu          32     0.25   \n",
       "214   55.755199  4518.758301       45.057968       relu          32     0.25   \n",
       "\n",
       "     first_neuron  hidden_layers label_type       loss_function  lr  \\\n",
       "0            1024              0    Angular  mean_squared_error   1   \n",
       "210          4096              0    Angular  mean_squared_error   1   \n",
       "199          2048              1    Angular  mean_squared_error   2   \n",
       "211          4096              0    Angular  mean_squared_error   2   \n",
       "183          1024              1    Angular  mean_squared_error   1   \n",
       "184          1024              1    Angular  mean_squared_error   2   \n",
       "198          2048              1    Angular  mean_squared_error   1   \n",
       "195          2048              0    Angular  mean_squared_error   1   \n",
       "181          1024              0    Angular  mean_squared_error   2   \n",
       "214          4096              1    Angular  mean_squared_error   2   \n",
       "\n",
       "      monitor_value                                          optimizer  \\\n",
       "0    val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "210  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "199  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "211  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "183  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "184  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "198  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "195  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "181  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "214  val_custom_mae  <class 'tensorflow.python.keras.optimizer_v2.a...   \n",
       "\n",
       "    reduction_metric  \n",
       "0         custom_mae  \n",
       "210       custom_mae  \n",
       "199       custom_mae  \n",
       "211       custom_mae  \n",
       "183       custom_mae  \n",
       "184       custom_mae  \n",
       "198       custom_mae  \n",
       "195       custom_mae  \n",
       "181       custom_mae  \n",
       "214       custom_mae  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_results = _MODEL_DIR + '..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, global_hyper_parameter['label_type'][0], _note)\n",
    "df = pd.read_csv(base_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "sort_value = df['monitor_value'][0]\n",
    "df = df.sort_values(sort_value, axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(base_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(top_results_index):\n",
    "    \n",
    "    #     Adam = RMSprop + Momentum (lr=0.001)\n",
    "    #     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "    #     RMSprop = (lr=0.001)\n",
    "    #     SGD = (lr=0.01)\n",
    "    #     Adagrad\n",
    "\n",
    "    hyper_parameter = global_hyper_parameter\n",
    "\n",
    "    hyper_parameter['samples'] = [100000] \n",
    "    hyper_parameter['epochs'] = [400]\n",
    "    hyper_parameter['batch_size'] = [df.iloc[top_results_index]['batch_size']]\n",
    "    hyper_parameter['optimizer'] = [make_optimizer(df.loc[top_results_index]['optimizer'])]\n",
    "    hyper_parameter['lr'] = [df.iloc[top_results_index]['lr']]\n",
    "    hyper_parameter['first_neuron'] = [df.iloc[top_results_index]['first_neuron']]\n",
    "    hyper_parameter['dropout'] = [df.iloc[top_results_index]['dropout']]\n",
    "    hyper_parameter['activation'] = [df.iloc[top_results_index]['activation']]\n",
    "    hyper_parameter['leaky_alpha'] = [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "    hyper_parameter['hidden_layers'] = [df.iloc[top_results_index]['hidden_layers']]\n",
    "    \n",
    "    hyper_parameter['loss_function'] = [df.iloc[top_results_index]['loss_function']]\n",
    "    hyper_parameter['reduction_metric'] = [df.iloc[top_results_index]['reduction_metric']]\n",
    "    hyper_parameter['monitor_value'] = [df.iloc[top_results_index]['monitor_value']]\n",
    "\n",
    "    return hyper_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#print(_CSV_FILE)\n",
    "#df = pd.read_csv(_CSV_FILE)\n",
    "#df.head()\n",
    "#print(df.iloc[0]['Filename RGB'])\n",
    "##Image(df.iloc[0]['Filename RGB'])\n",
    "##Image('../buddha00000001-0-5-0-10.png')\n",
    "##Image('../../data_generation/dataset/201019_2253_final/buddha/rgb/buddha00000000-0-5-0-5.png')\n",
    "#Image('..\\\\..\\\\data_generation\\\\dataset\\\\201019_2253_final\\\\buddha\\\\rgb\\\\buddha00000000-0-5-0-5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 1024, 'hidden_layers': 0, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================Params:\n",
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 400, 'first_neuron': 1024, 'hidden_layers': 0, 'label_type': 'Angular', 'leaky_alpha': 0.1, 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 100000}\n",
      "==========================\n",
      "Y-Col: ['Elevation', 'Azimuth']\n",
      "Train Data Generator: Found 80000 validated image filenames.\n",
      "Validation Data Generator: Found 20000 validated image filenames.\n",
      "Steps per Epoch: 2500, Validation Steps: 625\n",
      "_________________________________________________________________\n",
      "   Network Layer        Trainable\n",
      "=================================================================\n",
      "         input_1                0\n",
      "       conv1_pad                0\n",
      "      conv1_conv                0\n",
      "        conv1_bn                0\n",
      "      conv1_relu                0\n",
      "       pool1_pad                0\n",
      "      pool1_pool                0\n",
      "conv2_block1_1_conv                0\n",
      "conv2_block1_1_bn                0\n",
      "conv2_block1_1_relu                0\n",
      "conv2_block1_2_conv                0\n",
      "conv2_block1_2_bn                0\n",
      "conv2_block1_2_relu                0\n",
      "conv2_block1_0_conv                0\n",
      "conv2_block1_3_conv                0\n",
      "conv2_block1_0_bn                1\n",
      "conv2_block1_3_bn                1\n",
      "conv2_block1_add                1\n",
      "conv2_block1_out                1\n",
      "conv2_block2_1_conv                1\n",
      "conv2_block2_1_bn                1\n",
      "conv2_block2_1_relu                1\n",
      "conv2_block2_2_conv                1\n",
      "conv2_block2_2_bn                1\n",
      "conv2_block2_2_relu                1\n",
      "conv2_block2_3_conv                1\n",
      "conv2_block2_3_bn                1\n",
      "conv2_block2_add                1\n",
      "conv2_block2_out                1\n",
      "conv2_block3_1_conv                1\n",
      "conv2_block3_1_bn                1\n",
      "conv2_block3_1_relu                1\n",
      "conv2_block3_2_conv                1\n",
      "conv2_block3_2_bn                1\n",
      "conv2_block3_2_relu                1\n",
      "conv2_block3_3_conv                1\n",
      "conv2_block3_3_bn                1\n",
      "conv2_block3_add                1\n",
      "conv2_block3_out                1\n",
      "conv3_block1_1_conv                1\n",
      "conv3_block1_1_bn                1\n",
      "conv3_block1_1_relu                1\n",
      "conv3_block1_2_conv                1\n",
      "conv3_block1_2_bn                1\n",
      "conv3_block1_2_relu                1\n",
      "conv3_block1_0_conv                1\n",
      "conv3_block1_3_conv                1\n",
      "conv3_block1_0_bn                1\n",
      "conv3_block1_3_bn                1\n",
      "conv3_block1_add                1\n",
      "conv3_block1_out                1\n",
      "conv3_block2_1_conv                1\n",
      "conv3_block2_1_bn                1\n",
      "conv3_block2_1_relu                1\n",
      "conv3_block2_2_conv                1\n",
      "conv3_block2_2_bn                1\n",
      "conv3_block2_2_relu                1\n",
      "conv3_block2_3_conv                1\n",
      "conv3_block2_3_bn                1\n",
      "conv3_block2_add                1\n",
      "conv3_block2_out                1\n",
      "conv3_block3_1_conv                1\n",
      "conv3_block3_1_bn                1\n",
      "conv3_block3_1_relu                1\n",
      "conv3_block3_2_conv                1\n",
      "conv3_block3_2_bn                1\n",
      "conv3_block3_2_relu                1\n",
      "conv3_block3_3_conv                1\n",
      "conv3_block3_3_bn                1\n",
      "conv3_block3_add                1\n",
      "conv3_block3_out                1\n",
      "conv3_block4_1_conv                1\n",
      "conv3_block4_1_bn                1\n",
      "conv3_block4_1_relu                1\n",
      "conv3_block4_2_conv                1\n",
      "conv3_block4_2_bn                1\n",
      "conv3_block4_2_relu                1\n",
      "conv3_block4_3_conv                1\n",
      "conv3_block4_3_bn                1\n",
      "conv3_block4_add                1\n",
      "conv3_block4_out                1\n",
      "conv4_block1_1_conv                1\n",
      "conv4_block1_1_bn                1\n",
      "conv4_block1_1_relu                1\n",
      "conv4_block1_2_conv                1\n",
      "conv4_block1_2_bn                1\n",
      "conv4_block1_2_relu                1\n",
      "conv4_block1_0_conv                1\n",
      "conv4_block1_3_conv                1\n",
      "conv4_block1_0_bn                1\n",
      "conv4_block1_3_bn                1\n",
      "conv4_block1_add                1\n",
      "conv4_block1_out                1\n",
      "conv4_block2_1_conv                1\n",
      "conv4_block2_1_bn                1\n",
      "conv4_block2_1_relu                1\n",
      "conv4_block2_2_conv                1\n",
      "conv4_block2_2_bn                1\n",
      "conv4_block2_2_relu                1\n",
      "conv4_block2_3_conv                1\n",
      "conv4_block2_3_bn                1\n",
      "conv4_block2_add                1\n",
      "conv4_block2_out                1\n",
      "conv4_block3_1_conv                1\n",
      "conv4_block3_1_bn                1\n",
      "conv4_block3_1_relu                1\n",
      "conv4_block3_2_conv                1\n",
      "conv4_block3_2_bn                1\n",
      "conv4_block3_2_relu                1\n",
      "conv4_block3_3_conv                1\n",
      "conv4_block3_3_bn                1\n",
      "conv4_block3_add                1\n",
      "conv4_block3_out                1\n",
      "conv4_block4_1_conv                1\n",
      "conv4_block4_1_bn                1\n",
      "conv4_block4_1_relu                1\n",
      "conv4_block4_2_conv                1\n",
      "conv4_block4_2_bn                1\n",
      "conv4_block4_2_relu                1\n",
      "conv4_block4_3_conv                1\n",
      "conv4_block4_3_bn                1\n",
      "conv4_block4_add                1\n",
      "conv4_block4_out                1\n",
      "conv4_block5_1_conv                1\n",
      "conv4_block5_1_bn                1\n",
      "conv4_block5_1_relu                1\n",
      "conv4_block5_2_conv                1\n",
      "conv4_block5_2_bn                1\n",
      "conv4_block5_2_relu                1\n",
      "conv4_block5_3_conv                1\n",
      "conv4_block5_3_bn                1\n",
      "conv4_block5_add                1\n",
      "conv4_block5_out                1\n",
      "conv4_block6_1_conv                1\n",
      "conv4_block6_1_bn                1\n",
      "conv4_block6_1_relu                1\n",
      "conv4_block6_2_conv                1\n",
      "conv4_block6_2_bn                1\n",
      "conv4_block6_2_relu                1\n",
      "conv4_block6_3_conv                1\n",
      "conv4_block6_3_bn                1\n",
      "conv4_block6_add                1\n",
      "conv4_block6_out                1\n",
      "conv5_block1_1_conv                1\n",
      "conv5_block1_1_bn                1\n",
      "conv5_block1_1_relu                1\n",
      "conv5_block1_2_conv                1\n",
      "conv5_block1_2_bn                1\n",
      "conv5_block1_2_relu                1\n",
      "conv5_block1_0_conv                1\n",
      "conv5_block1_3_conv                1\n",
      "conv5_block1_0_bn                1\n",
      "conv5_block1_3_bn                1\n",
      "conv5_block1_add                1\n",
      "conv5_block1_out                1\n",
      "conv5_block2_1_conv                1\n",
      "conv5_block2_1_bn                1\n",
      "conv5_block2_1_relu                1\n",
      "conv5_block2_2_conv                1\n",
      "conv5_block2_2_bn                1\n",
      "conv5_block2_2_relu                1\n",
      "conv5_block2_3_conv                1\n",
      "conv5_block2_3_bn                1\n",
      "conv5_block2_add                1\n",
      "conv5_block2_out                1\n",
      "conv5_block3_1_conv                1\n",
      "conv5_block3_1_bn                1\n",
      "conv5_block3_1_relu                1\n",
      "conv5_block3_2_conv                1\n",
      "conv5_block3_2_bn                1\n",
      "conv5_block3_2_relu                1\n",
      "conv5_block3_3_conv                1\n",
      "conv5_block3_3_bn                1\n",
      "conv5_block3_add                1\n",
      "conv5_block3_out                1\n",
      "_________________________________________________________________\n",
      "\n",
      "Number Hidden Layers 0\n",
      "Fully Connected Layers added to Base Network\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000225161FD438>]\n",
      "Model was compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 2)                 102763522 \n",
      "=================================================================\n",
      "Total params: 126,351,234\n",
      "Trainable params: 126,213,890\n",
      "Non-trainable params: 137,344\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Checkpointer was created\n",
      "CSV Logger was created\n",
      "Learning Rate Reducer was created\n",
      "Early Stopper was created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2500 steps, validate for 625 steps\n",
      "Epoch 1/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22604.3414 - custom_mae: 112.4424 ETA: 8s - loss: 22599.4599 - \n",
      "Epoch 00001: val_custom_mae improved from inf to 112.77176, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 1976s 790ms/step - loss: 22601.6028 - custom_mae: 112.4345 - val_loss: 22689.0492 - val_custom_mae: 112.7718\n",
      "Epoch 2/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22590.3465 - custom_mae: 112.3933\n",
      "Epoch 00002: val_custom_mae improved from 112.77176 to 112.74160, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 866s 346ms/step - loss: 22588.5459 - custom_mae: 112.3891 - val_loss: 22680.3453 - val_custom_mae: 112.7416\n",
      "Epoch 3/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22578.6550 - custom_mae: 112.3547\n",
      "Epoch 00003: val_custom_mae improved from 112.74160 to 112.70411, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 869s 348ms/step - loss: 22578.7998 - custom_mae: 112.3562 - val_loss: 22669.5187 - val_custom_mae: 112.7041\n",
      "Epoch 4/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22569.2921 - custom_mae: 112.3187\n",
      "Epoch 00004: val_custom_mae improved from 112.70411 to 112.66026, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 866s 346ms/step - loss: 22567.0200 - custom_mae: 112.3151 - val_loss: 22656.7955 - val_custom_mae: 112.6603\n",
      "Epoch 5/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22554.7992 - custom_mae: 112.2702\n",
      "Epoch 00005: val_custom_mae improved from 112.66026 to 112.60814, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 866s 347ms/step - loss: 22553.5125 - custom_mae: 112.2677 - val_loss: 22642.4476 - val_custom_mae: 112.6081\n",
      "Epoch 6/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22539.6353 - custom_mae: 112.2178\n",
      "Epoch 00006: val_custom_mae improved from 112.60814 to 112.55058, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 871s 348ms/step - loss: 22538.4211 - custom_mae: 112.2129 - val_loss: 22626.4862 - val_custom_mae: 112.5506\n",
      "Epoch 7/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22521.7803 - custom_mae: 112.1502\n",
      "Epoch 00007: val_custom_mae improved from 112.55058 to 112.48586, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 872s 349ms/step - loss: 22521.7547 - custom_mae: 112.1519 - val_loss: 22609.0147 - val_custom_mae: 112.4859\n",
      "Epoch 8/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22500.5814 - custom_mae: 112.0735\n",
      "Epoch 00008: val_custom_mae improved from 112.48586 to 112.41515, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 870s 348ms/step - loss: 22503.5711 - custom_mae: 112.0836 - val_loss: 22590.0181 - val_custom_mae: 112.4152\n",
      "Epoch 9/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22485.8230 - custom_mae: 112.0161\n",
      "Epoch 00009: val_custom_mae improved from 112.41515 to 112.33615, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 867s 347ms/step - loss: 22483.9103 - custom_mae: 112.0090 - val_loss: 22569.5606 - val_custom_mae: 112.3362\n",
      "Epoch 10/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22463.6074 - custom_mae: 111.9297\n",
      "Epoch 00010: val_custom_mae improved from 112.33615 to 112.25176, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 861s 344ms/step - loss: 22462.8036 - custom_mae: 111.9282 - val_loss: 22547.6183 - val_custom_mae: 112.2518\n",
      "Epoch 11/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22436.8782 - custom_mae: 111.8319\n",
      "Epoch 00011: val_custom_mae improved from 112.25176 to 112.16183, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 22440.2026 - custom_mae: 111.8405 - val_loss: 22524.2135 - val_custom_mae: 112.1618\n",
      "Epoch 12/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22417.2034 - custom_mae: 111.7500\n",
      "Epoch 00012: val_custom_mae improved from 112.16183 to 112.06315, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 863s 345ms/step - loss: 22416.1494 - custom_mae: 111.7461 - val_loss: 22499.3215 - val_custom_mae: 112.0631\n",
      "Epoch 13/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22392.4348 - custom_mae: 111.6499\n",
      "Epoch 00013: val_custom_mae improved from 112.06315 to 111.95905, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 863s 345ms/step - loss: 22390.5423 - custom_mae: 111.6451 - val_loss: 22472.9692 - val_custom_mae: 111.9591\n",
      "Epoch 14/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22364.2393 - custom_mae: 111.5406\n",
      "Epoch 00014: val_custom_mae improved from 111.95905 to 111.84805, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 866s 346ms/step - loss: 22363.6160 - custom_mae: 111.5379 - val_loss: 22445.1758 - val_custom_mae: 111.8481\n",
      "Epoch 15/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22334.6434 - custom_mae: 111.4228\n",
      "Epoch 00015: val_custom_mae improved from 111.84805 to 111.73159, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 22335.1236 - custom_mae: 111.4242 - val_loss: 22415.9033 - val_custom_mae: 111.7316\n",
      "Epoch 16/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22305.5927 - custom_mae: 111.3040\n",
      "Epoch 00016: val_custom_mae improved from 111.73159 to 111.60826, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 22305.3171 - custom_mae: 111.3041 - val_loss: 22385.2279 - val_custom_mae: 111.6083\n",
      "Epoch 17/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22273.2930 - custom_mae: 111.1779\n",
      "Epoch 00017: val_custom_mae improved from 111.60826 to 111.47774, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 863s 345ms/step - loss: 22273.9895 - custom_mae: 111.1769 - val_loss: 22353.1333 - val_custom_mae: 111.4777\n",
      "Epoch 18/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22239.3481 - custom_mae: 111.0395\n",
      "Epoch 00018: val_custom_mae improved from 111.47774 to 111.34054, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 22241.2563 - custom_mae: 111.0438 - val_loss: 22319.5799 - val_custom_mae: 111.3405\n",
      "Epoch 19/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22207.5748 - custom_mae: 110.9072\n",
      "Epoch 00019: val_custom_mae improved from 111.34054 to 111.19758, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 863s 345ms/step - loss: 22207.1769 - custom_mae: 110.9043 - val_loss: 22284.6262 - val_custom_mae: 111.1976\n",
      "Epoch 20/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22174.3014 - custom_mae: 110.7659\n",
      "Epoch 00020: val_custom_mae improved from 111.19758 to 111.04813, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 861s 345ms/step - loss: 22171.6308 - custom_mae: 110.7580 - val_loss: 22248.2387 - val_custom_mae: 111.0481\n",
      "Epoch 21/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22136.0668 - custom_mae: 110.6094\n",
      "Epoch 00021: val_custom_mae improved from 111.04813 to 110.89230, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 867s 347ms/step - loss: 22134.6055 - custom_mae: 110.6051 - val_loss: 22210.4622 - val_custom_mae: 110.8923\n",
      "Epoch 22/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22096.3866 - custom_mae: 110.4493\n",
      "Epoch 00022: val_custom_mae improved from 110.89230 to 110.73044, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 868s 347ms/step - loss: 22096.2743 - custom_mae: 110.4462 - val_loss: 22171.2861 - val_custom_mae: 110.7304\n",
      "Epoch 23/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22056.4359 - custom_mae: 110.2806\n",
      "Epoch 00023: val_custom_mae improved from 110.73044 to 110.56113, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 22056.5199 - custom_mae: 110.2803 - val_loss: 22130.7494 - val_custom_mae: 110.5611\n",
      "Epoch 24/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 22015.9209 - custom_mae: 110.1104\n",
      "Epoch 00024: val_custom_mae improved from 110.56113 to 110.38526, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 863s 345ms/step - loss: 22015.4240 - custom_mae: 110.1084 - val_loss: 22088.8326 - val_custom_mae: 110.3853\n",
      "Epoch 25/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21974.0981 - custom_mae: 109.9343\n",
      "Epoch 00025: val_custom_mae improved from 110.38526 to 110.20412, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 864s 345ms/step - loss: 21973.0360 - custom_mae: 109.9306 - val_loss: 22045.4988 - val_custom_mae: 110.2041\n",
      "Epoch 26/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21929.6483 - custom_mae: 109.7463\n",
      "Epoch 00026: val_custom_mae improved from 110.20412 to 110.01588, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 863s 345ms/step - loss: 21929.2044 - custom_mae: 109.7456 - val_loss: 22000.8214 - val_custom_mae: 110.0159\n",
      "Epoch 27/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21885.8689 - custom_mae: 109.5578\n",
      "Epoch 00027: val_custom_mae improved from 110.01588 to 109.82072, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 868s 347ms/step - loss: 21883.9453 - custom_mae: 109.5540 - val_loss: 21954.8076 - val_custom_mae: 109.8207\n",
      "Epoch 28/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21838.1717 - custom_mae: 109.3586\n",
      "Epoch 00028: val_custom_mae improved from 109.82072 to 109.61964, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 21837.4126 - custom_mae: 109.3563 - val_loss: 21907.4735 - val_custom_mae: 109.6196\n",
      "Epoch 29/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21791.8381 - custom_mae: 109.1607\n",
      "Epoch 00029: val_custom_mae improved from 109.61964 to 109.41197, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 21789.6294 - custom_mae: 109.1528 - val_loss: 21858.7520 - val_custom_mae: 109.4120\n",
      "Epoch 30/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21742.3829 - custom_mae: 108.9468\n",
      "Epoch 00030: val_custom_mae improved from 109.41197 to 109.19913, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 21740.1805 - custom_mae: 108.9414 - val_loss: 21808.7199 - val_custom_mae: 109.1991\n",
      "Epoch 31/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21688.0809 - custom_mae: 108.7203\n",
      "Epoch 00031: val_custom_mae improved from 109.19913 to 108.97764, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 862s 345ms/step - loss: 21689.7418 - custom_mae: 108.7250 - val_loss: 21757.3387 - val_custom_mae: 108.9776\n",
      "Epoch 32/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21635.5465 - custom_mae: 108.4943\n",
      "Epoch 00032: val_custom_mae improved from 108.97764 to 108.75079, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 864s 346ms/step - loss: 21637.9075 - custom_mae: 108.5016 - val_loss: 21704.6612 - val_custom_mae: 108.7508\n",
      "Epoch 33/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21586.0869 - custom_mae: 108.2752\n",
      "Epoch 00033: val_custom_mae improved from 108.75079 to 108.51852, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 863s 345ms/step - loss: 21584.8002 - custom_mae: 108.2721 - val_loss: 21650.6584 - val_custom_mae: 108.5185\n",
      "Epoch 34/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21530.7122 - custom_mae: 108.0387\n",
      "Epoch 00034: val_custom_mae improved from 108.51852 to 108.28252, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 864s 346ms/step - loss: 21530.2867 - custom_mae: 108.0377 - val_loss: 21595.4332 - val_custom_mae: 108.2825\n",
      "Epoch 35/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21475.2450 - custom_mae: 107.7997\n",
      "Epoch 00035: val_custom_mae improved from 108.28252 to 108.03960, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 863s 345ms/step - loss: 21474.7506 - custom_mae: 107.7990 - val_loss: 21538.9061 - val_custom_mae: 108.0396\n",
      "Epoch 36/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 21419.0791 - custom_mae: 107.5583\n",
      "Epoch 00036: val_custom_mae improved from 108.03960 to 107.79070, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201129_2031_Angular_Top_1_Custom-MAE\\Synth_TD\\CNN_Base_0_Model_and_Weights_80000.hdf5\n",
      "2500/2500 [==============================] - 866s 346ms/step - loss: 21417.6495 - custom_mae: 107.5544 - val_loss: 21481.0748 - val_custom_mae: 107.7907\n",
      "Epoch 37/400\n",
      "2157/2500 [========================>.....] - ETA: 2:30 - loss: 21370.2089 - custom_mae: 107.3415"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    #for top_results_index in range(3):\n",
    "    #for top_results_index in [0, 1]:\n",
    "    top_results_index = 0\n",
    "    _MODEL_TO_LOAD_INDEX = df.iloc[top_results_index].name\n",
    "    _MODEL_TO_LOAD = 'Best_Weights_FC_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "    _TMP_DIR = '..\\\\TMP_TALOS_{}'.format(_DEVICE)\n",
    "    _CSV_RESULTS = _LOG_DIR + 'Talos_Results_Fine_Idx{}.csv'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "    startTime = datetime.now()\n",
    "\n",
    "    parameters = get_params(top_results_index)\n",
    "\n",
    "    t = ta.Scan(\n",
    "        x = dummy_x,\n",
    "        y = dummy_y,\n",
    "        model = grid_model_fine,\n",
    "        params = parameters,\n",
    "        experiment_name = _TMP_DIR,\n",
    "        #shuffle=False,\n",
    "        reduction_metric = parameters['reduction_metric'][0],\n",
    "        disable_progress_bar = False,\n",
    "        print_params = True,\n",
    "        clear_session = 'tf'\n",
    "    )\n",
    "\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "\n",
    "    print('Writing Device File')\n",
    "    device_file.write('Trained Model: {}'.format(_MODEL_TO_LOAD))\n",
    "\n",
    "    df_experiment_results = pd.read_csv(_TMP_DIR + '\\\\' + os.listdir(_TMP_DIR)[0])\n",
    "    df_experiment_results['Base'] = None\n",
    "    for i in range(df_experiment_results.shape[0]):\n",
    "        df_experiment_results['Base'][i] = _MODEL_TO_LOAD_INDEX\n",
    "\n",
    "    if os.path.isfile(_CSV_RESULTS):\n",
    "        df_experiment_results.to_csv(_CSV_RESULTS, mode = 'a', index = False, header = False)\n",
    "    else:\n",
    "        df_experiment_results.to_csv(_CSV_RESULTS, index = False)\n",
    "\n",
    "    shutil.rmtree(_TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Results to NAS if SSD Directory was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_directory(src, dst, symlinks = False, ignore = None):\n",
    "    maxLen = 0\n",
    "    message = ''        \n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        \n",
    "        message = 'Creating Path: {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        os.makedirs(dst)\n",
    "        \n",
    "    for item in os.listdir(src):\n",
    "        \n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        \n",
    "        if os.path.isdir(s):\n",
    "            \n",
    "            message = 'Copying Directory: {}'.format(s)\n",
    "            maxLen = max(maxLen, len(message))\n",
    "            print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "            \n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if not os.path.exists(d): #or os.stat(s).st_mtime - os.stat(d).st_mtime > 1:\n",
    "                \n",
    "                message = 'Copying File: {}'.format(s)\n",
    "                maxLen = max(maxLen, len(message))\n",
    "                print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "                \n",
    "                shutil.copy2(s, d)\n",
    "        \n",
    "        time.sleep(.5)\n",
    "     \n",
    "    message = 'Coyping... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "\n",
    "def delete_directory(src, terminator = '\\n'):\n",
    "    message = ''\n",
    "    maxLen = 0\n",
    "    \n",
    "    try:\n",
    "        message = 'Deleting {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        shutil.rmtree(src)\n",
    "        \n",
    "    except OSError as e:\n",
    "        message = 'Error: {} : {}'.format(src, e.strerror)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "        return\n",
    "    \n",
    "    message = 'Deleting... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = terminator)\n",
    "\n",
    "    \n",
    "def copy_fine_training(src, dst):\n",
    "    copy_directory(src, dst)\n",
    "    delete_directory(src, terminator = '\\r')\n",
    "    delete_directory(src + '..\\\\', terminator = '\\r')\n",
    "    if not os.listdir(src + '..\\\\..\\\\'):\n",
    "        delete_directory(src + '..\\\\..\\\\', terminator = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(storage == OutputDirectory.SSD):\n",
    "    _COPY_DIR = '..\\\\output\\\\{}'.format(_NET_DIR)\n",
    "    copy_fine_training(_LOG_DIR, _COPY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name = \"CMSE.Mixed\"></a><a href = #Top>Up</a></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
