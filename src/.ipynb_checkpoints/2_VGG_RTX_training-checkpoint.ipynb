{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kristins Alex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  2 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4630996637920958864\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9105744200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17050928178430162046\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9104897474\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12615745204916590\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\"\n",
      "]\n",
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(activation, leaky_alpha, dropout):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation=activation_layer, input_shape=(224,224,Global.num_image_channels)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(units = 2, activation=activation_layer)\n",
    "        #Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(activation, leaky_alpha):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape=(224,224,Global.num_image_channels), filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    #model.add(Dense(units = 2, activation = \"softmax\"))\n",
    "    model.add(Dense(units = 2, activation=activation_layer))\n",
    "\n",
    "    #opt = Adam(lr = 0.001)\n",
    "    #model.compile(optimizer = opt, loss= keras.losses.categorical_crossentropy, metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_net(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data_pipline(params['batch_size'], params['samples'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    "    #model = alexnet(params['activation'], params['leaky_alpha'], params['drop'])\n",
    "    model = vgg16(params['activation'], params['leaky_alpha'])\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer'])), \n",
    "        loss = Global.loss_function, \n",
    "        metrics = get_reduction_metric(Global.reduction_metric)\n",
    "    )\n",
    "\n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    print('Model was compiled')\n",
    "    print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = Global.target_dir + 'CNN_ALEX_Model_and_Weights_{}.hdf5'.format(Global.image_channels),\n",
    "        monitor =  Global.monitor_value,\n",
    "        verbose = 1,\n",
    "        save_weights_only = False,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    print('Checkpointer was created')\n",
    "    \n",
    "    csv_logger = callbacks.CSVLogger(\n",
    "        filename = Global.target_dir + 'CNN_ALEX_Logger_{}.csv'.format(Global.image_channels),\n",
    "        separator = ',',\n",
    "        append = False\n",
    "    )\n",
    "    print('CSV Logger was created')\n",
    "\n",
    "    lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 13,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        min_delta = 0.0001\n",
    "    )\n",
    "    print('Learning Rate Reducer was created')\n",
    "    \n",
    "    early_stopper = callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0,\n",
    "        #patience = 15,\n",
    "        patience = 20,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    print('Early Stopper was created')\n",
    "    \n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        epochs = params['epochs'],\n",
    "        validation_data = valid_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        callbacks = [checkpointer, csv_logger, lr_reducer, early_stopper],\n",
    "        workers = 8\n",
    "    )\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolut_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduction_metric(metric):\n",
    "    \n",
    "    if metric == 'mean_absolut_error':\n",
    "        return [mean_absolut_error]\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatische Optimizer Generierung aus String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer):\n",
    "    # [Adam, Nadam, Adagrad, RMSprop]\n",
    "    if optimizer == \"<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>\":\n",
    "        return Adam\n",
    "    else:\n",
    "        print('ERROR::: Unspecified Optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct for global parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class global_parameter:\n",
    "    loss_function: str = 'mean_squared_error'\n",
    "    reduction_metric: str = 'mean_absolut_error'\n",
    "    monitor_value: str = 'val_mean_absolut_error'\n",
    "            \n",
    "    net_architecture = 'VGG16' # 'AlexNet'\n",
    "    \n",
    "    dataset: str = '201019_2253_final'\n",
    "    device: str = 'RTX_2080_Ti'\n",
    "    data_augmentation: bool = True\n",
    "    image_channels: str = 'rgb' # just change this, everything else will automaticlly adjusted\n",
    "    num_image_channels: int = 3\n",
    "    image_dir: str = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(dataset)\n",
    "    \n",
    "    csv_file_name: str = 'labels_ks_RGB.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(net_architecture, dataset, image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(dataset, image_channels)\n",
    "    results_man: str = '\\\\..\\\\201019_2253_final_rgb_Results_manual.csv'\n",
    "\n",
    "\n",
    "        \n",
    "Global = global_parameter\n",
    "\n",
    "if(Global.image_channels == 'rgba'):\n",
    "    Global.num_image_channels = 4\n",
    "    csv_file_name: str = 'lables_ks_RGBD.csv'\n",
    "    csv_file: str = image_dir + csv_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst fÃ¼r Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_pipline(batch_size, num_samples):\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(Global.csv_file)\n",
    "    df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "    df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "    df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "    #print(df_valid)\n",
    "    \n",
    "    if Global.data_augmentation:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.5, 1.0), \n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    #print(df_train)\n",
    "        \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    #print(df_train)\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\output\\201019_2253_final_rgb\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n"
     ]
    }
   ],
   "source": [
    "if(not os.path.exists(Global.target_dir)):\n",
    "    os.makedirs(Global.target_dir)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(Global.target_dir))\n",
    "\n",
    "device_file = open(Global.target_dir + '{}.txt'.format(Global.device), \"a+\")\n",
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: ..\\output\\201019_2253_final_rgb\\\\..\\201019_2253_final_rgb_Results_manual.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_absolut_error</th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolut_error</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>leaky_alpha</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>10</td>\n",
       "      <td>11/10/20-141428</td>\n",
       "      <td>11/10/20-141525</td>\n",
       "      <td>57.297635</td>\n",
       "      <td>4108.879395</td>\n",
       "      <td>41.778568</td>\n",
       "      <td>6010.239789</td>\n",
       "      <td>53.150177</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>23</td>\n",
       "      <td>11/09/20-160436</td>\n",
       "      <td>11/09/20-160539</td>\n",
       "      <td>63.461800</td>\n",
       "      <td>4651.241211</td>\n",
       "      <td>42.399044</td>\n",
       "      <td>6957.538744</td>\n",
       "      <td>55.006474</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>11/09/20-152438</td>\n",
       "      <td>11/09/20-152541</td>\n",
       "      <td>62.956922</td>\n",
       "      <td>4807.504883</td>\n",
       "      <td>42.973434</td>\n",
       "      <td>6722.161218</td>\n",
       "      <td>54.768200</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2</td>\n",
       "      <td>11/10/20-124118</td>\n",
       "      <td>11/10/20-124213</td>\n",
       "      <td>54.611791</td>\n",
       "      <td>4609.726074</td>\n",
       "      <td>43.298336</td>\n",
       "      <td>6051.426510</td>\n",
       "      <td>52.527298</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11/09/20-150631</td>\n",
       "      <td>11/09/20-150724</td>\n",
       "      <td>53.074127</td>\n",
       "      <td>4551.695312</td>\n",
       "      <td>44.134876</td>\n",
       "      <td>6316.580711</td>\n",
       "      <td>54.332478</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>5</td>\n",
       "      <td>11/09/20-181241</td>\n",
       "      <td>11/09/20-181336</td>\n",
       "      <td>55.279167</td>\n",
       "      <td>5065.450195</td>\n",
       "      <td>44.154381</td>\n",
       "      <td>7182.103573</td>\n",
       "      <td>57.703896</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>11/09/20-151007</td>\n",
       "      <td>11/09/20-151101</td>\n",
       "      <td>54.582515</td>\n",
       "      <td>4447.951660</td>\n",
       "      <td>44.235825</td>\n",
       "      <td>5920.551208</td>\n",
       "      <td>52.696514</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>8</td>\n",
       "      <td>11/10/20-124653</td>\n",
       "      <td>11/10/20-124751</td>\n",
       "      <td>58.295156</td>\n",
       "      <td>5182.400879</td>\n",
       "      <td>45.300297</td>\n",
       "      <td>6063.520243</td>\n",
       "      <td>53.520336</td>\n",
       "      <td>relu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>11/09/20-172013</td>\n",
       "      <td>11/09/20-172107</td>\n",
       "      <td>54.318614</td>\n",
       "      <td>4049.460205</td>\n",
       "      <td>45.346073</td>\n",
       "      <td>5809.329929</td>\n",
       "      <td>52.325111</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2</td>\n",
       "      <td>11/10/20-133846</td>\n",
       "      <td>11/10/20-133941</td>\n",
       "      <td>54.370764</td>\n",
       "      <td>5233.627930</td>\n",
       "      <td>45.512722</td>\n",
       "      <td>6016.750846</td>\n",
       "      <td>52.590275</td>\n",
       "      <td>leakyrelu</td>\n",
       "      <td>32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0            start              end   duration     val_loss  \\\n",
       "298          10  11/10/20-141428  11/10/20-141525  57.297635  4108.879395   \n",
       "55           23  11/09/20-160436  11/09/20-160539  63.461800  4651.241211   \n",
       "21           21  11/09/20-152438  11/09/20-152541  62.956922  4807.504883   \n",
       "258           2  11/10/20-124118  11/10/20-124213  54.611791  4609.726074   \n",
       "2             2  11/09/20-150631  11/09/20-150724  53.074127  4551.695312   \n",
       "133           5  11/09/20-181241  11/09/20-181336  55.279167  5065.450195   \n",
       "6             6  11/09/20-151007  11/09/20-151101  54.582515  4447.951660   \n",
       "264           8  11/10/20-124653  11/10/20-124751  58.295156  5182.400879   \n",
       "96            0  11/09/20-172013  11/09/20-172107  54.318614  4049.460205   \n",
       "274           2  11/10/20-133846  11/10/20-133941  54.370764  5233.627930   \n",
       "\n",
       "     val_mean_absolut_error         loss  mean_absolut_error activation  \\\n",
       "298               41.778568  6010.239789           53.150177       relu   \n",
       "55                42.399044  6957.538744           55.006474       relu   \n",
       "21                42.973434  6722.161218           54.768200       relu   \n",
       "258               43.298336  6051.426510           52.527298  leakyrelu   \n",
       "2                 44.134876  6316.580711           54.332478  leakyrelu   \n",
       "133               44.154381  7182.103573           57.703896  leakyrelu   \n",
       "6                 44.235825  5920.551208           52.696514  leakyrelu   \n",
       "264               45.300297  6063.520243           53.520336       relu   \n",
       "96                45.346073  5809.329929           52.325111  leakyrelu   \n",
       "274               45.512722  6016.750846           52.590275  leakyrelu   \n",
       "\n",
       "     batch_size  dropout  first_neuron  hidden_layers  leaky_alpha  lr  \\\n",
       "298          32     0.50          4096              3          0.1   1   \n",
       "55           32     0.50          2048              1          0.1   2   \n",
       "21           32     0.50          1024              0          0.1   2   \n",
       "258          32     0.50          4096              1          0.1   1   \n",
       "2            32     0.25          2048              0          0.1   1   \n",
       "133          32     0.50          1024              4          0.1   2   \n",
       "6            32     0.50          2048              0          0.1   1   \n",
       "264          32     0.25          4096              1          0.1   1   \n",
       "96           32     0.25          1024              3          0.1   1   \n",
       "274          32     0.50          4096              2          0.1   1   \n",
       "\n",
       "                                             optimizer  \n",
       "298  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "55   <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "21   <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "258  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "2    <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "133  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "6    <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "264  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "96   <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "274  <class 'tensorflow.python.keras.optimizer_v2.a...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_results = Global.target_dir + Global.results_man\n",
    "df = pd.read_csv(base_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "sort_value = 'val_mean_absolut_error'\n",
    "df = df.sort_values(sort_value, axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(base_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "global_hyper_parameter = {\n",
    "    'samples': None,\n",
    "    'epochs': None,\n",
    "    'batch_size': None,\n",
    "    'optimizer': None,\n",
    "    'lr': None,\n",
    "    'first_neuron': None,\n",
    "    'dropout': None,\n",
    "    'activation': None,\n",
    "    'leaky_alpha': None,\n",
    "    'hidden_layers': None,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(top_results_index):\n",
    "    \n",
    "    #     Adam = RMSprop + Momentum (lr=0.001)\n",
    "    #     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "    #     RMSprop = (lr=0.001)\n",
    "    #     SGD = (lr=0.01)\n",
    "    #     Adagrad\n",
    "\n",
    "    hyper_parameter = global_hyper_parameter\n",
    "\n",
    "    hyper_parameter['samples'] = [100000]\n",
    "    hyper_parameter['epochs'] = [400]\n",
    "    hyper_parameter['batch_size'] = [df.iloc[top_results_index]['batch_size']]\n",
    "    hyper_parameter['optimizer'] = [make_optimizer(df.loc[top_results_index]['optimizer'])]\n",
    "    hyper_parameter['lr'] = [df.iloc[top_results_index]['lr']]\n",
    "    hyper_parameter['first_neuron'] = [df.iloc[top_results_index]['first_neuron']]\n",
    "    hyper_parameter['dropout'] = [df.iloc[top_results_index]['dropout']]\n",
    "    hyper_parameter['activation'] = [df.iloc[top_results_index]['activation']]\n",
    "    hyper_parameter['leaky_alpha'] = [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "    hyper_parameter['hidden_layers'] = [df.iloc[top_results_index]['hidden_layers']]\n",
    "    \n",
    "    return hyper_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 400, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 100000}\n",
      "Found 80000 validated image filenames.\n",
      "Found 20000 validated image filenames.\n",
      "Steps per Epoch: 2500, Validation Steps: 625\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,121,154\n",
      "Trainable params: 134,121,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model was compiled\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,121,154\n",
      "Trainable params: 134,121,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Checkpointer was created\n",
      "CSV Logger was created\n",
      "Learning Rate Reducer was created\n",
      "Early Stopper was created\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2500 steps, validate for 625 steps\n",
      "Epoch 1/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 6152.8142 - mean_absolut_error: 57.6553\n",
      "Epoch 00001: val_mean_absolut_error improved from inf to 56.21523, saving model to ..\\output\\201019_2253_final_rgb\\CNN_ALEX_Model_and_Weights_rgb.hdf5\n",
      "2500/2500 [==============================] - 938s 375ms/step - loss: 6153.2901 - mean_absolut_error: 57.6577 - val_loss: 5770.3620 - val_mean_absolut_error: 56.2152\n",
      "Epoch 2/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5813.3823 - mean_absolut_error: 56.5620\n",
      "Epoch 00002: val_mean_absolut_error improved from 56.21523 to 56.15400, saving model to ..\\output\\201019_2253_final_rgb\\CNN_ALEX_Model_and_Weights_rgb.hdf5\n",
      "2500/2500 [==============================] - 939s 376ms/step - loss: 5813.7993 - mean_absolut_error: 56.5639 - val_loss: 5751.7221 - val_mean_absolut_error: 56.1540\n",
      "Epoch 3/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5793.9410 - mean_absolut_error: 56.4787\n",
      "Epoch 00003: val_mean_absolut_error improved from 56.15400 to 56.07279, saving model to ..\\output\\201019_2253_final_rgb\\CNN_ALEX_Model_and_Weights_rgb.hdf5\n",
      "2500/2500 [==============================] - 942s 377ms/step - loss: 5793.6907 - mean_absolut_error: 56.4774 - val_loss: 5725.5980 - val_mean_absolut_error: 56.0728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5787.6588 - mean_absolut_error: 56.4363\n",
      "Epoch 00004: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 881s 353ms/step - loss: 5787.3589 - mean_absolut_error: 56.4359 - val_loss: 5728.2889 - val_mean_absolut_error: 56.0923\n",
      "Epoch 5/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5783.1438 - mean_absolut_error: 56.4267   ETA: 1:08 - loss: 5782.7235 - mean_absolu - ETA: 1:05 - l - ETA: 57s - loss: 5779.0658 - m\n",
      "Epoch 00005: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5783.4708 - mean_absolut_error: 56.4284 - val_loss: 5726.8896 - val_mean_absolut_error: 56.0810\n",
      "Epoch 6/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5780.0089 - mean_absolut_error: 56.4141\n",
      "Epoch 00006: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 852s 341ms/step - loss: 5779.4038 - mean_absolut_error: 56.4109 - val_loss: 5728.5967 - val_mean_absolut_error: 56.0836\n",
      "Epoch 7/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5779.2802 - mean_absolut_error: 56.4036 - ETA: 22s - los - E\n",
      "Epoch 00007: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 852s 341ms/step - loss: 5779.5878 - mean_absolut_error: 56.4054 - val_loss: 5767.3366 - val_mean_absolut_error: 56.2420\n",
      "Epoch 8/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5780.2827 - mean_absolut_error: 56.4076\n",
      "Epoch 00008: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 852s 341ms/step - loss: 5780.5489 - mean_absolut_error: 56.4093 - val_loss: 5751.4139 - val_mean_absolut_error: 56.3166\n",
      "Epoch 9/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5773.2882 - mean_absolut_error: 56.3922\n",
      "Epoch 00009: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 852s 341ms/step - loss: 5773.1274 - mean_absolut_error: 56.3912 - val_loss: 5727.4794 - val_mean_absolut_error: 56.0913\n",
      "Epoch 10/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5767.0539 - mean_absolut_error: 56.3540\n",
      "Epoch 00010: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 851s 341ms/step - loss: 5766.7505 - mean_absolut_error: 56.3529 - val_loss: 5816.2404 - val_mean_absolut_error: 56.3915\n",
      "Epoch 11/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5768.3512 - mean_absolut_error: 56.3716\n",
      "Epoch 00011: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 852s 341ms/step - loss: 5768.3401 - mean_absolut_error: 56.3706 - val_loss: 5785.5859 - val_mean_absolut_error: 56.2486\n",
      "Epoch 12/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5768.0774 - mean_absolut_error: 56.3534\n",
      "Epoch 00012: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5767.6981 - mean_absolut_error: 56.3511 - val_loss: 5727.4021 - val_mean_absolut_error: 56.0869\n",
      "Epoch 13/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5767.5031 - mean_absolut_error: 56.3605\n",
      "Epoch 00013: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 852s 341ms/step - loss: 5767.4433 - mean_absolut_error: 56.3598 - val_loss: 5731.2073 - val_mean_absolut_error: 56.1260\n",
      "Epoch 14/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5761.7183 - mean_absolut_error: 56.3237\n",
      "Epoch 00014: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 342ms/step - loss: 5761.7609 - mean_absolut_error: 56.3236 - val_loss: 5770.7733 - val_mean_absolut_error: 56.2077\n",
      "Epoch 15/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5763.9588 - mean_absolut_error: 56.3489\n",
      "Epoch 00015: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 852s 341ms/step - loss: 5763.9580 - mean_absolut_error: 56.3501 - val_loss: 5727.4417 - val_mean_absolut_error: 56.0923\n",
      "Epoch 16/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5765.2576 - mean_absolut_error: 56.3431\n",
      "Epoch 00016: val_mean_absolut_error did not improve from 56.07279\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5765.7914 - mean_absolut_error: 56.3467 - val_loss: 5742.4893 - val_mean_absolut_error: 56.1352\n",
      "Epoch 17/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.5176 - mean_absolut_error: 56.2720\n",
      "Epoch 00017: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 341ms/step - loss: 5741.4771 - mean_absolut_error: 56.2722 - val_loss: 5722.6250 - val_mean_absolut_error: 56.0812\n",
      "Epoch 18/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.9455 - mean_absolut_error: 56.2667\n",
      "Epoch 00018: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 341ms/step - loss: 5741.1165 - mean_absolut_error: 56.2670 - val_loss: 5722.4284 - val_mean_absolut_error: 56.0782\n",
      "Epoch 19/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5742.4846 - mean_absolut_error: 56.2789\n",
      "Epoch 00019: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5742.0034 - mean_absolut_error: 56.2766 - val_loss: 5724.5050 - val_mean_absolut_error: 56.0844\n",
      "Epoch 20/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.8519 - mean_absolut_error: 56.2626\n",
      "Epoch 00020: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5740.9534 - mean_absolut_error: 56.2637 - val_loss: 5722.4899 - val_mean_absolut_error: 56.0778\n",
      "Epoch 21/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.8220 - mean_absolut_error: 56.2700\n",
      "Epoch 00021: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 341ms/step - loss: 5740.6850 - mean_absolut_error: 56.2689 - val_loss: 5723.3646 - val_mean_absolut_error: 56.0821\n",
      "Epoch 22/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.2832 - mean_absolut_error: 56.2716\n",
      "Epoch 00022: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5741.3535 - mean_absolut_error: 56.2723 - val_loss: 5723.0833 - val_mean_absolut_error: 56.0769\n",
      "Epoch 23/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.5961 - mean_absolut_error: 56.2706\n",
      "Epoch 00023: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5741.7222 - mean_absolut_error: 56.2710 - val_loss: 5725.5249 - val_mean_absolut_error: 56.0846\n",
      "Epoch 24/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.4142 - mean_absolut_error: 56.2641\n",
      "Epoch 00024: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5740.7935 - mean_absolut_error: 56.2671 - val_loss: 5735.8860 - val_mean_absolut_error: 56.1188\n",
      "Epoch 25/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.5704 - mean_absolut_error: 56.2697 ETA: \n",
      "Epoch 00025: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5740.9974 - mean_absolut_error: 56.2673 - val_loss: 5723.3117 - val_mean_absolut_error: 56.0762\n",
      "Epoch 26/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.8351 - mean_absolut_error: 56.2677\n",
      "Epoch 00026: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5740.9926 - mean_absolut_error: 56.2688 - val_loss: 5723.3796 - val_mean_absolut_error: 56.0822\n",
      "Epoch 27/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.8928 - mean_absolut_error: 56.2697 - ETA: 18s - loss: 5741.6394 -\n",
      "Epoch 00027: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 342ms/step - loss: 5740.6626 - mean_absolut_error: 56.2681 - val_loss: 5726.8366 - val_mean_absolut_error: 56.0863\n",
      "Epoch 28/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.7481 - mean_absolut_error: 56.2646\n",
      "Epoch 00028: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 342ms/step - loss: 5741.1240 - mean_absolut_error: 56.2669 - val_loss: 5722.5923 - val_mean_absolut_error: 56.0811\n",
      "Epoch 29/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.6308 - mean_absolut_error: 56.2641\n",
      "Epoch 00029: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 342ms/step - loss: 5741.0099 - mean_absolut_error: 56.2671 - val_loss: 5722.4442 - val_mean_absolut_error: 56.0803\n",
      "Epoch 30/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.6354 - mean_absolut_error: 56.2683\n",
      "Epoch 00030: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 855s 342ms/step - loss: 5740.4437 - mean_absolut_error: 56.2666 - val_loss: 5731.4013 - val_mean_absolut_error: 56.1028\n",
      "Epoch 31/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.4607 - mean_absolut_error: 56.2742\n",
      "Epoch 00031: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 855s 342ms/step - loss: 5741.2916 - mean_absolut_error: 56.2730 - val_loss: 5722.4087 - val_mean_absolut_error: 56.0782\n",
      "Epoch 32/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.6625 - mean_absolut_error: 56.2717 ETA: 2s - loss: 5741.6730 - mean_absolut_er\n",
      "Epoch 00032: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 853s 341ms/step - loss: 5741.7692 - mean_absolut_error: 56.2708 - val_loss: 5722.7238 - val_mean_absolut_error: 56.0784\n",
      "Epoch 33/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.3122 - mean_absolut_error: 56.2661\n",
      "Epoch 00033: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 855s 342ms/step - loss: 5739.8906 - mean_absolut_error: 56.2645 - val_loss: 5722.3715 - val_mean_absolut_error: 56.0784\n",
      "Epoch 34/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.5471 - mean_absolut_error: 56.2725\n",
      "Epoch 00034: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 857s 343ms/step - loss: 5741.3649 - mean_absolut_error: 56.2724 - val_loss: 5722.3524 - val_mean_absolut_error: 56.0796\n",
      "Epoch 35/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.4509 - mean_absolut_error: 56.2703 - ETA: 20s - loss: 5738.6948\n",
      "Epoch 00035: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 857s 343ms/step - loss: 5740.4018 - mean_absolut_error: 56.2705 - val_loss: 5732.9039 - val_mean_absolut_error: 56.1091\n",
      "Epoch 36/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.4787 - mean_absolut_error: 56.2728-\n",
      "Epoch 00036: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 856s 342ms/step - loss: 5741.5664 - mean_absolut_error: 56.2734 - val_loss: 5728.2060 - val_mean_absolut_error: 56.0896\n",
      "Epoch 37/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.1152 - mean_absolut_error: 56.2733\n",
      "Epoch 00037: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 855s 342ms/step - loss: 5741.0965 - mean_absolut_error: 56.2727 - val_loss: 5722.3209 - val_mean_absolut_error: 56.0791\n",
      "Epoch 38/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.1059 - mean_absolut_error: 56.2692\n",
      "Epoch 00038: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 856s 342ms/step - loss: 5740.9977 - mean_absolut_error: 56.2673 - val_loss: 5724.7888 - val_mean_absolut_error: 56.0840\n",
      "Epoch 39/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.3006 - mean_absolut_error: 56.2689\n",
      "Epoch 00039: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 855s 342ms/step - loss: 5740.7098 - mean_absolut_error: 56.2702 - val_loss: 5722.3835 - val_mean_absolut_error: 56.0801\n",
      "Epoch 40/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5742.1482 - mean_absolut_error: 56.2776\n",
      "Epoch 00040: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 855s 342ms/step - loss: 5741.7464 - mean_absolut_error: 56.2743 - val_loss: 5722.4419 - val_mean_absolut_error: 56.0793\n",
      "Epoch 41/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.4519 - mean_absolut_error: 56.2641\n",
      "Epoch 00041: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 855s 342ms/step - loss: 5740.2977 - mean_absolut_error: 56.2630 - val_loss: 5723.0330 - val_mean_absolut_error: 56.0795\n",
      "Epoch 42/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.8612 - mean_absolut_error: 56.2706\n",
      "Epoch 00042: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 856s 342ms/step - loss: 5740.8250 - mean_absolut_error: 56.2708 - val_loss: 5723.0643 - val_mean_absolut_error: 56.0769\n",
      "Epoch 43/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.4835 - mean_absolut_error: 56.2625\n",
      "Epoch 00043: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 858s 343ms/step - loss: 5740.4692 - mean_absolut_error: 56.2630 - val_loss: 5724.8764 - val_mean_absolut_error: 56.0816\n",
      "Epoch 44/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.1851 - mean_absolut_error: 56.2673\n",
      "Epoch 00044: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 342ms/step - loss: 5740.8668 - mean_absolut_error: 56.2662 - val_loss: 5730.6405 - val_mean_absolut_error: 56.1011\n",
      "Epoch 45/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.4022 - mean_absolut_error: 56.2698\n",
      "Epoch 00045: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 855s 342ms/step - loss: 5740.8228 - mean_absolut_error: 56.2669 - val_loss: 5723.9292 - val_mean_absolut_error: 56.0832\n",
      "Epoch 46/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.6342 - mean_absolut_error: 56.2732\n",
      "Epoch 00046: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 342ms/step - loss: 5741.3294 - mean_absolut_error: 56.2714 - val_loss: 5724.3731 - val_mean_absolut_error: 56.0815\n",
      "Epoch 47/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.3247 - mean_absolut_error: 56.2735 ETA: 6s - loss: 5740.8216 - mean_absolut_error: 56. - ETA: 6s - loss: 574\n",
      "Epoch 00047: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 854s 341ms/step - loss: 5741.1888 - mean_absolut_error: 56.2716 - val_loss: 5722.7545 - val_mean_absolut_error: 56.0769\n",
      "Epoch 48/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5740.8684 - mean_absolut_error: 56.2674\n",
      "Epoch 00048: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 858s 343ms/step - loss: 5740.6065 - mean_absolut_error: 56.2662 - val_loss: 5722.8215 - val_mean_absolut_error: 56.0771\n",
      "Epoch 49/400\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 5741.0189 - mean_absolut_error: 56.2657\n",
      "Epoch 00049: val_mean_absolut_error did not improve from 56.07279\n",
      "2500/2500 [==============================] - 857s 343ms/step - loss: 5741.1342 - mean_absolut_error: 56.2663 - val_loss: 5725.5553 - val_mean_absolut_error: 56.0855\n",
      "Epoch 50/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2197/2500 [=========================>....] - ETA: 1:39 - loss: 5741.3700 - mean_absolut_error: 56.2717"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, Global.num_image_channels, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    #for top_results_index in [0, 1]:\n",
    "        top_results_index = 0\n",
    "        #_MODEL_TO_LOAD_INDEX = df.iloc[top_results_index].name\n",
    "        #_MODEL_TO_LOAD = 'Best_Weights_FC_{}.hdf5'.format(_MODEL_TO_LOAD_INDEX)\n",
    "\n",
    "        tmp_dir = '..\\\\TMP_TALOS_{}'.format(Global.device)\n",
    "        trained_results = Global.target_dir + 'Talos_Results_Fine_Idx{}.csv'.format(Global.image_channels)\n",
    "\n",
    "        startTime = datetime.now()\n",
    "        \n",
    "        parameters = get_params(top_results_index)\n",
    "\n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = gen_net,\n",
    "            params = parameters,\n",
    "            experiment_name = tmp_dir,\n",
    "            #shuffle=False,\n",
    "            reduction_metric = get_reduction_metric(Global.reduction_metric),\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = 'tf'\n",
    "        )\n",
    "\n",
    "        print(\"Time taken:\", datetime.now() - startTime)\n",
    "        \n",
    "        print('Writing Device File')\n",
    "        device_file.write('Trained Model: {}'.format(Global.image_channels))\n",
    "\n",
    "        df_experiment_results = pd.read_csv(tmp_dir + '\\\\' + os.listdir(tmp_dir)[0])\n",
    "        df_experiment_results['Base'] = None\n",
    "        for i in range(df_experiment_results.shape[0]):\n",
    "            df_experiment_results['Base'][i] = Global.image_channles\n",
    "\n",
    "        if os.path.isfile(trained_results):\n",
    "            df_experiment_results.to_csv(trained_results, mode = 'a', index = False, header = False)\n",
    "        else:\n",
    "            df_experiment_results.to_csv(trained_results, index = False)\n",
    "\n",
    "        shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
