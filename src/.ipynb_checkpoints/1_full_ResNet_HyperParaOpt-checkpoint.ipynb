{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kristins Alex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U git+https://github.com/keras-team/keras git+https://github.com/keras-team/keras-applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  1 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13054220104290407791\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 20264236482\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2484713828242976749\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\"\n",
      "]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "import talos as ta\n",
    "from talos.model import lr_normalizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)\n",
    "\n",
    "#gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "#session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(activation, leaky_alpha, dropout):\n",
    "    \n",
    "    K.clear_session()\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation=activation_layer, input_shape=(224,224,Global.num_image_channels)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation=activation_layer, padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(4096, activation=activation_layer),\n",
    "        Dropout(dropout),\n",
    "        Dense(units = 2, activation=activation_layer)\n",
    "        #Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16v1(activation, leaky_alpha):\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape=(224,224,Global.num_image_channels), filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = activation_layer))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    model.add(Dense(units = 4096, activation = activation_layer))\n",
    "    #model.add(Dense(units = 2, activation = \"softmax\"))\n",
    "    model.add(Dense(units = 2, activation=activation_layer))\n",
    "\n",
    "    #opt = Adam(lr = 0.001)\n",
    "    #model.compile(optimizer = opt, loss= keras.losses.categorical_crossentropy, metrics = ['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(activation, leaky_alpha):\n",
    "    \n",
    "    K.clear_session()\n",
    "        \n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "    \n",
    "    model = Sequential()\n",
    "    cnn = VGG16(include_top=False, weights=None, input_tensor=None, input_shape=(224, 224, Global.num_image_channels))\n",
    "    for layer in cnn.layers:\n",
    "        layer.trainable = True\n",
    "    x = cnn.output\n",
    "\n",
    "    x =  AveragePooling2D((7, 7), padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units = 512, activation = activation_layer)(x)\n",
    "    x = Dense(units = 256, activation = activation_layer)(x)\n",
    "    x = Dense(units = 64, activation = activation_layer)(x)\n",
    "    x = Dense(units = 2, activation = activation_layer)(x)\n",
    "    model = Model(cnn.input, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(activation, leaky_alpha):\n",
    "    \n",
    "    K.clear_session()\n",
    "         \n",
    "    activation_layer = ReLU()\n",
    "    if activation == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = leaky_alpha)\n",
    "    elif activation == 'relu':\n",
    "        activation_layer = ReLU()\n",
    "        \n",
    "    model = Sequential()\n",
    "    cnn = ResNet50(include_top=False, weights=None, input_tensor=None, input_shape=(224, 224, Global.num_image_channels))\n",
    "    for layer in cnn.layers:\n",
    "        layer.trainable = True\n",
    "    x = cnn.output\n",
    "\n",
    "    x =  AveragePooling2D((7, 7), padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units = 512, activation = activation_layer)(x)\n",
    "    x = Dense(units = 256, activation = activation_layer)(x)\n",
    "    x = Dense(units = 64, activation = activation_layer)(x)\n",
    "    x = Dense(units = 2, activation = activation_layer)(x)\n",
    "    model = Model(cnn.input, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_net(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    train_generator, valid_generator = create_data_pipline(params['batch_size'], params['samples'])\n",
    "    tg_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "    vg_validation_steps = valid_generator.n // valid_generator.batch_size\n",
    "    print('Steps per Epoch: {}, Validation Steps: {}'.format(tg_steps_per_epoch, vg_validation_steps))\n",
    "    \n",
    " \n",
    "    if(Global.net_architecture == 'ALEX'):\n",
    "        model = alexnet(params['activation'], params['leaky_alpha'], params['dropout'])\n",
    "    elif(Global.net_architecture =='VGG16'):\n",
    "        model = vgg16(params['activation'], params['leaky_alpha'])\n",
    "    elif(Global.net_architecture == 'RESNET'):\n",
    "        model = resnet(params['leaky_alpha'], params['leaky_alpha'])\n",
    "    else:\n",
    "        print('Wrong net architecture!')\n",
    "            \n",
    "    model.compile(\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer'])), \n",
    "        loss = Global.loss_function, \n",
    "        metrics = get_reduction_metric(Global.reduction_metric)\n",
    "    )\n",
    "    #print(model.summary())\n",
    "    print('_________________________________________________________________')\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        epochs = params['epochs'],\n",
    "        validation_data = valid_generator,\n",
    "        steps_per_epoch = tg_steps_per_epoch,\n",
    "        validation_steps = vg_validation_steps,\n",
    "        workers = 8\n",
    "    )\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolut_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduction_metric(metric):\n",
    "    \n",
    "    if metric == 'mean_absolut_error':\n",
    "        return [mean_absolut_error]\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct for global parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class global_parameter:\n",
    "    loss_function: str = 'mean_squared_error'\n",
    "    reduction_metric: str = 'mean_absolut_error'\n",
    "    monitor_value: str = 'val_mean_absolut_error'\n",
    "        \n",
    "    net_architecture = 'VGG16' # 'ALEX' vs 'VGG16' vs 'RESNET'\n",
    "    \n",
    "    dataset: str = '201129_2031'\n",
    "    device: str = 'RTX_2080_Ti'\n",
    "    data_augmentation: bool = True\n",
    "    image_channels: str = 'rgb' # just change this, everything else will automaticlly adjusted\n",
    "    num_image_channels: int = 3\n",
    "    image_dir: str = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(dataset)\n",
    "    \n",
    "    csv_file_name: str = 'labels_ks_RGB.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(net_architecture, dataset, image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(net_architecture, dataset)\n",
    "\n",
    "        \n",
    "Global = global_parameter\n",
    "\n",
    "if(Global.image_channels == 'rgba'):\n",
    "    Global.num_image_channels = 4\n",
    "    csv_file_name: str = 'lables_ks_RGBD.csv'\n",
    "    csv_file: str = image_dir + csv_file_name\n",
    "    target_dir: str = '..\\\\output\\\\{}_{}_{}\\\\'.format(net_architecture, dataset, image_channels)\n",
    "    results: str = '\\\\..\\\\{}_{}_Results.csv'.format(net_architecture, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_pipline(batch_size, num_samples):\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(Global.csv_file)\n",
    "    df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "    df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]\n",
    "    df_valid = df_shuffled.drop(df_shuffled.index[0 : df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "    \n",
    "    if Global.data_augmentation:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            width_shift_range = 0.1,\n",
    "            height_shift_range = 0.1,\n",
    "            zoom_range = 0.1,\n",
    "            brightness_range = (0.5, 1.0), \n",
    "            fill_mode = 'nearest'\n",
    "        )\n",
    "    else:\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "        \n",
    "    train_generator = train_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_train,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = True,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "        \n",
    "    valid_data_generator = ImageDataGenerator(\n",
    "        rescale = 1./255\n",
    "    )\n",
    "    \n",
    "    valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "        dataframe = df_valid,\n",
    "        directory = Global.image_dir,\n",
    "        x_col = 'Filename',\n",
    "        y_col = ['Elevation', 'Azimuth'],\n",
    "        class_mode = 'raw',\n",
    "        target_size = (224, 224),\n",
    "        color_mode = Global.image_channels,\n",
    "        shuffle = False,\n",
    "        seed = 77,\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory >>| ..\\output\\VGG16_201129_2031_rgb\\ |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if(not os.path.exists(Global.target_dir)):\n",
    "    os.makedirs(Global.target_dir)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(Global.target_dir))\n",
    "\n",
    "device_file = open(Global.target_dir + '{}.txt'.format(Global.device), \"a+\")\n",
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "#\n",
    "#hyper_parameter = {\n",
    "#    'samples': [20000],\n",
    "#    'epochs': [1],\n",
    "#    'batch_size': [32, 64],\n",
    "#    'optimizer': [Adam],\n",
    "#    'lr': [1, 2, 5],\n",
    "#    'first_neuron': [1024, 2048, 4096],\n",
    "#    'dropout': [0.25, 0.50],\n",
    "#    'activation': ['leakyrelu', 'relu'],\n",
    "#    'hidden_layers': [0, 1, 2, 3, 4],\n",
    "#    'leaky_alpha': [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "#}\n",
    "\n",
    "hyper_parameter = {\n",
    "    'samples': [20000],\n",
    "    'epochs': [1],\n",
    "    'batch_size': [16, 32],# 64],\n",
    "    'optimizer': [Adam],\n",
    "    'lr': [5], #1, 2, 5],\n",
    "    'first_neuron':  [1024, 2048, 4096],\n",
    "    'dropout': [0.25, 0.50],\n",
    "    'activation': ['relu', 'leakyrelu'],\n",
    "    'hidden_layers': [0, 1, 2, 3, 4],\n",
    "    'leaky_alpha': [0.1] #Default bei LeakyReLU, sonst PReLU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Talos Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 1275s 1s/step - loss: 7220.8982 - mean_absolut_error: 69.6722 - val_loss: 8223.3989 - val_mean_absolut_error: 72.3856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                            | 1/120 [21:17<42:13:22, 1277.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:21:15.311438\n",
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 175s 175ms/step - loss: 6167.3713 - mean_absolut_error: 57.7478 - val_loss: 6625.2539 - val_mean_absolut_error: 59.2824\n",
      "Time taken: 0:02:54.935362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▎                                                                            | 2/120 [24:13<31:02:15, 946.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 175s 175ms/step - loss: 21587.5009 - mean_absolut_error: 100.5736 - val_loss: 21643.5121 - val_mean_absolut_error: 100.8988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▉                                                                            | 3/120 [27:09<23:15:37, 715.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:02:55.177202\n",
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 175s 175ms/step - loss: 6189.0875 - mean_absolut_error: 57.9220 - val_loss: 7635.4460 - val_mean_absolut_error: 62.5354\n",
      "Time taken: 0:02:55.274537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▌                                                                           | 4/120 [30:05<17:50:50, 553.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 174s 174ms/step - loss: 21585.1643 - mean_absolut_error: 100.5543 - val_loss: 21643.4773 - val_mean_absolut_error: 100.9001\n",
      "Time taken: 0:02:54.669606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▎                                                                          | 5/120 [33:01<14:04:09, 440.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 175s 175ms/step - loss: 7281.9125 - mean_absolut_error: 69.8271 - val_loss: 9111.0760 - val_mean_absolut_error: 74.8142\n",
      "Time taken: 0:02:55.240026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▉                                                                          | 6/120 [35:57<11:26:14, 361.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 174s 174ms/step - loss: 21582.7911 - mean_absolut_error: 100.5185 - val_loss: 21650.2276 - val_mean_absolut_error: 100.9718\n",
      "Time taken: 0:02:54.622490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▌                                                                          | 7/120 [38:53<9:35:23, 305.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 175s 175ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:02:54.818505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▎                                                                         | 8/120 [41:49<8:17:41, 266.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 175s 175ms/step - loss: 6159.7114 - mean_absolut_error: 57.6742 - val_loss: 359996.3993 - val_mean_absolut_error: 321.4796\n",
      "Time taken: 0:02:54.901159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|█████▉                                                                         | 9/120 [44:45<7:22:55, 239.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 174s 174ms/step - loss: 6162.3531 - mean_absolut_error: 57.6765 - val_loss: 362708.1940 - val_mean_absolut_error: 275.0461\n",
      "Time taken: 0:02:54.366609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▌                                                                       | 10/120 [47:40<6:43:43, 220.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 175s 175ms/step - loss: 7363.6378 - mean_absolut_error: 70.0644 - val_loss: 40953.1466 - val_mean_absolut_error: 125.5960\n",
      "Time taken: 0:02:55.246400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▏                                                                      | 11/120 [50:36<6:16:06, 207.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 174s 174ms/step - loss: 6117.0917 - mean_absolut_error: 57.6924 - val_loss: 419888.3775 - val_mean_absolut_error: 315.8065\n",
      "Time taken: 0:02:54.476753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████▊                                                                      | 12/120 [53:32<5:55:38, 197.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 175s 175ms/step - loss: 7308.0353 - mean_absolut_error: 69.9619 - val_loss: 135384.7760 - val_mean_absolut_error: 183.3708\n",
      "Time taken: 0:02:54.959054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▍                                                                     | 13/120 [56:28<5:40:48, 191.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 175s 175ms/step - loss: 21586.5527 - mean_absolut_error: 100.5452 - val_loss: 34894.0666 - val_mean_absolut_error: 137.3814\n",
      "Time taken: 0:02:54.809009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████                                                                     | 14/120 [59:24<5:29:32, 186.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 21587.9572 - mean_absolut_error: 100.5753 - val_loss: 90175.3003 - val_mean_absolut_error: 191.8222\n",
      "Time taken: 0:03:17.371686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▌                                                                  | 15/120 [1:02:42<5:32:41, 190.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 7327.2711 - mean_absolut_error: 69.9943 - val_loss: 7476.7264 - val_mean_absolut_error: 70.5421\n",
      "Time taken: 0:03:17.236535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████████▏                                                                 | 16/120 [1:06:00<5:33:45, 192.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22696.7278 - val_mean_absolut_error: 112.6598\n",
      "Time taken: 0:03:18.926699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████████▊                                                                 | 17/120 [1:09:20<5:34:21, 194.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 7268.7284 - mean_absolut_error: 69.8633 - val_loss: 8167.6378 - val_mean_absolut_error: 72.4524\n",
      "Time taken: 0:03:16.831545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|███████████▍                                                                | 18/120 [1:12:38<5:32:41, 195.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 7265.8261 - mean_absolut_error: 69.8104 - val_loss: 223369.9861 - val_mean_absolut_error: 214.0011\n",
      "Time taken: 0:03:16.907426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████████████                                                                | 19/120 [1:15:56<5:30:33, 196.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6263.0592 - mean_absolut_error: 58.1557 - val_loss: 94068.9873 - val_mean_absolut_error: 170.4177\n",
      "Time taken: 0:03:17.042245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|████████████▋                                                               | 20/120 [1:19:14<5:28:08, 196.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 196s 196ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.1864 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:16.554421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█████████████▎                                                              | 21/120 [1:22:32<5:25:12, 197.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:18.266467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█████████████▉                                                              | 22/120 [1:25:51<5:23:00, 197.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6173.5309 - mean_absolut_error: 57.7862 - val_loss: 402574.7819 - val_mean_absolut_error: 326.5245\n",
      "Time taken: 0:03:16.841099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|██████████████▌                                                             | 23/120 [1:29:09<5:19:46, 197.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6147.6400 - mean_absolut_error: 57.7671 - val_loss: 10734.1690 - val_mean_absolut_error: 72.6000\n",
      "Time taken: 0:03:17.637069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████▏                                                            | 24/120 [1:32:28<5:16:53, 198.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 7239.8113 - mean_absolut_error: 69.7367 - val_loss: 8314.6544 - val_mean_absolut_error: 72.8074\n",
      "Time taken: 0:03:18.562889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|███████████████▊                                                            | 25/120 [1:35:47<5:14:19, 198.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.1523 - val_mean_absolut_error: 112.6624\n",
      "Time taken: 0:03:17.083319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|████████████████▍                                                           | 26/120 [1:39:05<5:10:49, 198.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 21596.0034 - mean_absolut_error: 100.6550 - val_loss: 22806.1153 - val_mean_absolut_error: 109.9666\n",
      "Time taken: 0:03:18.117075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████                                                           | 27/120 [1:42:25<5:07:52, 198.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22696.9111 - val_mean_absolut_error: 112.6596\n",
      "Time taken: 0:03:17.359680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████████████▋                                                          | 28/120 [1:45:43<5:04:28, 198.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 6162.5686 - mean_absolut_error: 57.7024 - val_loss: 369205.0070 - val_mean_absolut_error: 307.0346\n",
      "Time taken: 0:03:17.649578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████████████▎                                                         | 29/120 [1:49:02<5:01:13, 198.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6303.6305 - mean_absolut_error: 58.2151 - val_loss: 112317.4178 - val_mean_absolut_error: 185.6264\n",
      "Time taken: 0:03:19.386816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████████████                                                         | 30/120 [1:52:22<4:58:44, 199.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 21861.5323 - mean_absolut_error: 103.1513 - val_loss: 21572.3645 - val_mean_absolut_error: 100.1973\n",
      "Time taken: 0:03:14.784687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|███████████████████▋                                                        | 31/120 [1:55:38<4:53:56, 198.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:14.722308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|████████████████████▎                                                       | 32/120 [1:58:54<4:49:35, 197.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 16174.0505 - mean_absolut_error: 90.7808 - val_loss: 19954.0938 - val_mean_absolut_error: 104.6911\n",
      "Time taken: 0:03:14.884728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|████████████████████▉                                                       | 33/120 [2:02:10<4:45:38, 196.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 387ms/step - loss: 15455.7827 - mean_absolut_error: 88.0989 - val_loss: 19540.3574 - val_mean_absolut_error: 101.9766\n",
      "Time taken: 0:03:14.023874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|█████████████████████▌                                                      | 34/120 [2:05:25<4:41:32, 196.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 387ms/step - loss: 22262.6602 - mean_absolut_error: 107.6718 - val_loss: 22350.1342 - val_mean_absolut_error: 108.8251\n",
      "Time taken: 0:03:14.133264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██████████████████████▏                                                     | 35/120 [2:08:40<4:37:44, 196.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: 14699.5597 - mean_absolut_error: 89.4647 - val_loss: 17666.2941 - val_mean_absolut_error: 98.7469\n",
      "Time taken: 0:03:13.880471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████▊                                                     | 36/120 [2:11:55<4:34:00, 195.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: 15721.5311 - mean_absolut_error: 90.2529 - val_loss: 17311.0029 - val_mean_absolut_error: 95.0140\n",
      "Time taken: 0:03:13.627605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████▍                                                    | 37/120 [2:15:10<4:30:19, 195.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 6607.3296 - mean_absolut_error: 59.2379 - val_loss: 7020.4706 - val_mean_absolut_error: 60.8063\n",
      "Time taken: 0:03:14.295995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████████████████                                                    | 38/120 [2:18:25<4:27:02, 195.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: 7603.1889 - mean_absolut_error: 70.7449 - val_loss: 7150.3855 - val_mean_absolut_error: 69.6135\n",
      "Time taken: 0:03:13.508414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████████████████▋                                                   | 39/120 [2:21:40<4:23:26, 195.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 13158.4407 - mean_absolut_error: 81.2686 - val_loss: 5841.8514 - val_mean_absolut_error: 56.6944\n",
      "Time taken: 0:03:14.934319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████████████████▎                                                  | 40/120 [2:24:56<4:20:32, 195.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.1359 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:13.480846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|█████████████████████████▉                                                  | 41/120 [2:28:10<4:16:56, 195.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 6574.1537 - mean_absolut_error: 59.2212 - val_loss: 6039.4172 - val_mean_absolut_error: 57.3403\n",
      "Time taken: 0:03:14.833026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████████████████▌                                                 | 42/120 [2:31:26<4:13:59, 195.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 15128.6958 - mean_absolut_error: 86.3957 - val_loss: 14089.3608 - val_mean_absolut_error: 83.8416\n",
      "Time taken: 0:03:14.386552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████████████████▏                                                | 43/120 [2:34:42<4:10:45, 195.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:13.267164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███████████████████████████▊                                                | 44/120 [2:37:56<4:07:06, 195.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: 7676.7372 - mean_absolut_error: 70.9170 - val_loss: 6909.4308 - val_mean_absolut_error: 68.9800\n",
      "Time taken: 0:03:13.386766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████▌                                               | 45/120 [2:41:10<4:03:37, 194.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.0848 - val_mean_absolut_error: 112.6624\n",
      "Time taken: 0:03:13.732569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████▏                                              | 46/120 [2:44:25<4:00:20, 194.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 17530.3046 - mean_absolut_error: 73.2458 - val_loss: 5853.8952 - val_mean_absolut_error: 56.6611\n",
      "Time taken: 0:03:14.303344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|█████████████████████████████▊                                              | 47/120 [2:47:41<3:57:16, 195.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 21739.2360 - mean_absolut_error: 102.1778 - val_loss: 21768.1446 - val_mean_absolut_error: 102.3043\n",
      "Time taken: 0:03:14.341984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████▍                                             | 48/120 [2:50:56<3:54:10, 195.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 6526.6619 - mean_absolut_error: 59.0187 - val_loss: 5934.4890 - val_mean_absolut_error: 57.0986\n",
      "Time taken: 0:03:14.870748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████                                             | 49/120 [2:54:12<3:51:12, 195.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 23164.9021 - mean_absolut_error: 106.2031 - val_loss: 22697.2094 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:14.304294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|███████████████████████████████▋                                            | 50/120 [2:57:27<3:47:57, 195.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 12519.2494 - mean_absolut_error: 77.7440 - val_loss: 5806.1197 - val_mean_absolut_error: 56.6824\n",
      "Time taken: 0:03:14.505613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████▎                                           | 51/120 [3:00:43<3:44:46, 195.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 195s 390ms/step - loss: 7777.7165 - mean_absolut_error: 71.1900 - val_loss: 7282.4841 - val_mean_absolut_error: 69.9882\n",
      "Time taken: 0:03:15.234721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████▉                                           | 52/120 [3:03:59<3:41:48, 195.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 7653.4633 - mean_absolut_error: 70.9681 - val_loss: 6994.9855 - val_mean_absolut_error: 69.2089\n",
      "Time taken: 0:03:14.862768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████████████████████▌                                          | 53/120 [3:07:15<3:38:37, 195.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 16185.8322 - mean_absolut_error: 90.0808 - val_loss: 17840.5813 - val_mean_absolut_error: 95.6867\n",
      "Time taken: 0:03:14.318809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████████████████████▏                                         | 54/120 [3:10:31<3:35:14, 195.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: 22689.3359 - mean_absolut_error: 112.6963 - val_loss: 22697.2093 - val_mean_absolut_error: 112.6631\n",
      "Time taken: 0:03:13.510699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|██████████████████████████████████▊                                         | 55/120 [3:13:45<3:31:37, 195.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 196s 392ms/step - loss: 68105.3649 - mean_absolut_error: 95.6142 - val_loss: 18218.5794 - val_mean_absolut_error: 100.2888\n",
      "Time taken: 0:03:16.641383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████▍                                        | 56/120 [3:17:03<3:29:08, 196.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 267677.1668 - mean_absolut_error: 90.8308 - val_loss: 5985.6452 - val_mean_absolut_error: 57.3787\n",
      "Time taken: 0:03:14.728037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████████████████████                                        | 57/120 [3:20:19<3:25:47, 195.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 7591.6458 - mean_absolut_error: 70.7578 - val_loss: 7103.9954 - val_mean_absolut_error: 69.5759\n",
      "Time taken: 0:03:14.868757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████████████████████▋                                       | 58/120 [3:23:35<3:22:31, 195.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: 12971.5817 - mean_absolut_error: 75.3040 - val_loss: 6661.0942 - val_mean_absolut_error: 59.4608\n",
      "Time taken: 0:03:13.616323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████████████████████▎                                      | 59/120 [3:26:49<3:18:52, 195.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 30390.1510 - mean_absolut_error: 80.2500 - val_loss: 9004.7002 - val_mean_absolut_error: 67.0619\n",
      "Time taken: 0:03:14.350932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████                                      | 60/120 [3:30:05<3:15:33, 195.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6234.1388 - mean_absolut_error: 58.0456 - val_loss: 58316.9563 - val_mean_absolut_error: 141.7157\n",
      "Time taken: 0:03:17.917262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|██████████████████████████████████████▋                                     | 61/120 [3:33:24<3:13:19, 196.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6157.9592 - mean_absolut_error: 57.7429 - val_loss: 287436.3508 - val_mean_absolut_error: 279.8763\n",
      "Time taken: 0:03:18.438136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|███████████████████████████████████████▎                                    | 62/120 [3:36:43<3:10:53, 197.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6209.7759 - mean_absolut_error: 57.8986 - val_loss: 595369.3143 - val_mean_absolut_error: 418.8795\n",
      "Time taken: 0:03:19.894197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|███████████████████████████████████████▉                                    | 63/120 [3:40:04<3:08:36, 198.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6166.5574 - mean_absolut_error: 57.8364 - val_loss: 8317.0679 - val_mean_absolut_error: 65.1363\n",
      "Time taken: 0:03:19.678559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|████████████████████████████████████████▌                                   | 64/120 [3:43:25<3:05:55, 199.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6159.5966 - mean_absolut_error: 57.7737 - val_loss: 246741.6615 - val_mean_absolut_error: 265.8851\n",
      "Time taken: 0:03:19.707136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████████████████████████▏                                  | 65/120 [3:46:46<3:03:03, 199.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6226.9650 - mean_absolut_error: 57.8839 - val_loss: 224593.6239 - val_mean_absolut_error: 243.5484\n",
      "Time taken: 0:03:18.535734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████▊                                  | 66/120 [3:50:06<2:59:42, 199.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6189.9066 - mean_absolut_error: 57.8858 - val_loss: 6557.2911 - val_mean_absolut_error: 59.1446\n",
      "Time taken: 0:03:19.881323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████████████████████▍                                 | 67/120 [3:53:27<2:56:43, 200.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6208.8797 - mean_absolut_error: 57.8990 - val_loss: 469318.2647 - val_mean_absolut_error: 342.3684\n",
      "Time taken: 0:03:19.298944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|███████████████████████████████████████████                                 | 68/120 [3:56:47<2:53:29, 200.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6154.5240 - mean_absolut_error: 57.7738 - val_loss: 596329.9417 - val_mean_absolut_error: 379.5760\n",
      "Time taken: 0:03:18.268912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|███████████████████████████████████████████▋                                | 69/120 [4:00:06<2:49:57, 199.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6190.3952 - mean_absolut_error: 57.9677 - val_loss: 570960.2957 - val_mean_absolut_error: 380.4344\n",
      "Time taken: 0:03:19.501637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|████████████████████████████████████████████▎                               | 70/120 [4:03:27<2:46:47, 200.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6168.8821 - mean_absolut_error: 57.8088 - val_loss: 532295.2005 - val_mean_absolut_error: 387.1967\n",
      "Time taken: 0:03:18.064184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████▉                               | 71/120 [4:06:46<2:43:12, 199.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6175.5799 - mean_absolut_error: 57.8034 - val_loss: 428733.4967 - val_mean_absolut_error: 334.8953\n",
      "Time taken: 0:03:19.116893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████▌                              | 72/120 [4:10:07<2:39:58, 199.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6181.6607 - mean_absolut_error: 58.0105 - val_loss: 213003.8056 - val_mean_absolut_error: 236.5976\n",
      "Time taken: 0:03:18.854678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████████████████████████████████████████████▏                             | 73/120 [4:13:26<2:36:38, 199.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6255.8495 - mean_absolut_error: 58.1857 - val_loss: 26988.4927 - val_mean_absolut_error: 105.1179\n",
      "Time taken: 0:03:19.001732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████████████████████████▊                             | 74/120 [4:16:47<2:33:20, 200.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 201s 201ms/step - loss: 6255.1739 - mean_absolut_error: 58.1729 - val_loss: 74585.0601 - val_mean_absolut_error: 161.2166\n",
      "Time taken: 0:03:21.045414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████▌                            | 75/120 [4:20:09<2:30:29, 200.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6277.5218 - mean_absolut_error: 58.0714 - val_loss: 95305.4629 - val_mean_absolut_error: 174.5154\n",
      "Time taken: 0:03:20.564478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|████████████████████████████████████████████████▏                           | 76/120 [4:23:30<2:27:22, 200.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 198s 198ms/step - loss: 6281.3498 - mean_absolut_error: 58.0488 - val_loss: 21922.3789 - val_mean_absolut_error: 98.2555\n",
      "Time taken: 0:03:18.618393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████▊                           | 77/120 [4:26:50<2:23:45, 200.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6172.2898 - mean_absolut_error: 57.7019 - val_loss: 409165.8939 - val_mean_absolut_error: 306.2223\n",
      "Time taken: 0:03:19.220155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████▍                          | 78/120 [4:30:11<2:20:22, 200.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6277.7823 - mean_absolut_error: 58.1161 - val_loss: 33282.4112 - val_mean_absolut_error: 112.4872\n",
      "Time taken: 0:03:19.947160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████████████████████████████                          | 79/120 [4:33:32<2:17:08, 200.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 199s 199ms/step - loss: 6282.0196 - mean_absolut_error: 58.2019 - val_loss: 104736.1584 - val_mean_absolut_error: 184.5378\n",
      "Time taken: 0:03:19.228885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████▋                         | 80/120 [4:36:52<2:13:43, 200.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 201s 201ms/step - loss: 6217.9236 - mean_absolut_error: 57.9194 - val_loss: 369794.4955 - val_mean_absolut_error: 322.2568\n",
      "Time taken: 0:03:20.940592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████▎                        | 81/120 [4:40:14<2:10:40, 201.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6290.5617 - mean_absolut_error: 58.2611 - val_loss: 87107.1297 - val_mean_absolut_error: 166.9010\n",
      "Time taken: 0:03:20.115085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████▉                        | 82/120 [4:43:35<2:07:21, 201.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6211.0813 - mean_absolut_error: 57.8635 - val_loss: 365329.9800 - val_mean_absolut_error: 314.9095\n",
      "Time taken: 0:03:20.530087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████▌                       | 83/120 [4:46:57<2:04:06, 201.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 201s 201ms/step - loss: 6287.5361 - mean_absolut_error: 58.2660 - val_loss: 19305.8160 - val_mean_absolut_error: 92.5833\n",
      "Time taken: 0:03:20.879086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████▏                      | 84/120 [4:50:19<2:00:53, 201.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6106.1385 - mean_absolut_error: 57.6070 - val_loss: 585456.1984 - val_mean_absolut_error: 390.4261\n",
      "Time taken: 0:03:20.580807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████▊                      | 85/120 [4:53:41<1:57:34, 201.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 202s 202ms/step - loss: 6092.8767 - mean_absolut_error: 57.5637 - val_loss: 580771.3644 - val_mean_absolut_error: 384.9241\n",
      "Time taken: 0:03:22.412015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████▍                     | 86/120 [4:57:04<1:54:33, 202.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 201s 201ms/step - loss: 6297.7032 - mean_absolut_error: 58.2296 - val_loss: 47745.7759 - val_mean_absolut_error: 134.8643\n",
      "Time taken: 0:03:20.964605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████████████████                     | 87/120 [5:00:26<1:51:10, 202.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6268.4080 - mean_absolut_error: 58.1924 - val_loss: 45541.5190 - val_mean_absolut_error: 129.8097\n",
      "Time taken: 0:03:20.447287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████▋                    | 88/120 [5:03:48<1:47:43, 201.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6179.6383 - mean_absolut_error: 57.8642 - val_loss: 427701.8444 - val_mean_absolut_error: 334.5666\n",
      "Time taken: 0:03:20.489813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████▎                   | 89/120 [5:07:10<1:44:18, 201.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 16, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 1000, Validation Steps: 250\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 250 steps\n",
      "1000/1000 [==============================] - 200s 200ms/step - loss: 6162.5035 - mean_absolut_error: 57.7752 - val_loss: 960226.0535 - val_mean_absolut_error: 485.3871\n",
      "Time taken: 0:03:20.313579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████                   | 90/120 [5:10:31<1:40:52, 201.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 633297.7116 - mean_absolut_error: 113.8127 - val_loss: 11409.3713 - val_mean_absolut_error: 77.6426\n",
      "Time taken: 0:03:14.513202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|█████████████████████████████████████████████████████████▋                  | 91/120 [5:13:47<1:36:37, 199.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 387ms/step - loss: 33610.5743 - mean_absolut_error: 98.4197 - val_loss: 16320.2973 - val_mean_absolut_error: 89.7128\n",
      "Time taken: 0:03:13.974047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|██████████████████████████████████████████████████████████▎                 | 92/120 [5:17:02<1:32:37, 198.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 195s 390ms/step - loss: 177494.3296 - mean_absolut_error: 88.5684 - val_loss: 9956.7222 - val_mean_absolut_error: 85.4794\n",
      "Time taken: 0:03:15.228463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████▉                 | 93/120 [5:20:18<1:29:02, 197.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 17889.5872 - mean_absolut_error: 76.8639 - val_loss: 6103.7313 - val_mean_absolut_error: 57.5585\n",
      "Time taken: 0:03:14.956480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████████████████████████████████████████████████████████▌                | 94/120 [5:23:34<1:25:30, 197.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 17278.7329 - mean_absolut_error: 90.2517 - val_loss: 15953.0895 - val_mean_absolut_error: 89.4634\n",
      "Time taken: 0:03:14.267646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████▏               | 95/120 [5:26:50<1:21:59, 196.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 400727.0137 - mean_absolut_error: 95.5394 - val_loss: 9961.0373 - val_mean_absolut_error: 69.5017\n",
      "Time taken: 0:03:14.742559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████▊               | 96/120 [5:30:06<1:18:36, 196.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 24003.0816 - mean_absolut_error: 88.0807 - val_loss: 12547.0246 - val_mean_absolut_error: 78.0799\n",
      "Time taken: 0:03:14.776120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████▍              | 97/120 [5:33:22<1:15:15, 196.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 3359061.8618 - mean_absolut_error: 86.3452 - val_loss: 8828.7136 - val_mean_absolut_error: 66.0330\n",
      "Time taken: 0:03:14.311738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████████████████████              | 98/120 [5:36:37<1:11:53, 196.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 191s 383ms/step - loss: 953391.4001 - mean_absolut_error: 114.7156 - val_loss: 21064.6330 - val_mean_absolut_error: 108.6937\n",
      "Time taken: 0:03:11.831694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████████████████████▋             | 99/120 [5:39:50<1:08:18, 195.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: 246355.3710 - mean_absolut_error: 94.3220 - val_loss: 13356.0432 - val_mean_absolut_error: 80.9607\n",
      "Time taken: 0:03:13.913312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|██████████████████████████████████████████████████████████████▌            | 100/120 [5:43:05<1:05:02, 195.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 1207224.1533 - mean_absolut_error: 111.9242 - val_loss: 7739.6058 - val_mean_absolut_error: 62.0649\n",
      "Time taken: 0:03:14.791791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████████████████████████████████▏           | 101/120 [5:46:21<1:01:52, 195.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 195s 390ms/step - loss: 20586.7853 - mean_absolut_error: 83.6676 - val_loss: 10503.2069 - val_mean_absolut_error: 72.9217\n",
      "Time taken: 0:03:15.323449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████▍           | 102/120 [5:49:38<58:42, 195.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 59415623.8337 - mean_absolut_error: 182.9536 - val_loss: 23571.8680 - val_mean_absolut_error: 115.6694\n",
      "Time taken: 0:03:14.583852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████████████████████████████████████           | 103/120 [5:52:53<55:27, 195.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 195s 389ms/step - loss: 67266.0877 - mean_absolut_error: 86.2533 - val_loss: 18300.0456 - val_mean_absolut_error: 101.8650\n",
      "Time taken: 0:03:15.001353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|██████████████████████████████████████████████████████████████████▋          | 104/120 [5:56:10<52:13, 195.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: 114048.0865 - mean_absolut_error: 83.9671 - val_loss: 9364.7964 - val_mean_absolut_error: 68.2281\n",
      "Time taken: 0:03:13.763640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|███████████████████████████████████████████████████████████████████▍         | 105/120 [5:59:24<48:53, 195.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 195s 389ms/step - loss: 31142459.8533 - mean_absolut_error: 148.1144 - val_loss: 9226.6911 - val_mean_absolut_error: 60.5164\n",
      "Time taken: 0:03:15.148343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████         | 106/120 [6:02:41<45:41, 195.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 387ms/step - loss: 2368363.1402 - mean_absolut_error: 128.4495 - val_loss: 22797.7601 - val_mean_absolut_error: 109.6091\n",
      "Time taken: 0:03:14.163420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████████████████████████████████████████████████████████████████▋        | 107/120 [6:05:56<42:23, 195.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: 366681.7020 - mean_absolut_error: 95.7035 - val_loss: 22457.9195 - val_mean_absolut_error: 112.6133\n",
      "Time taken: 0:03:13.608203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████▎       | 108/120 [6:09:11<39:04, 195.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: 19725.7855 - mean_absolut_error: 84.1904 - val_loss: 17944.8175 - val_mean_absolut_error: 99.7168\n",
      "Time taken: 0:03:13.956245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████▉       | 109/120 [6:12:26<35:48, 195.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 387ms/step - loss: 111884.8606 - mean_absolut_error: 89.8688 - val_loss: 22150.5087 - val_mean_absolut_error: 111.6525\n",
      "Time taken: 0:03:14.046487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|██████████████████████████████████████████████████████████████████████▌      | 110/120 [6:15:41<32:32, 195.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 195s 389ms/step - loss: 20350.3381 - mean_absolut_error: 79.8277 - val_loss: 6315.0821 - val_mean_absolut_error: 57.9925\n",
      "Time taken: 0:03:14.978677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████████▏     | 111/120 [6:18:57<29:19, 195.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 134900308.5956 - mean_absolut_error: 231.4797 - val_loss: 24040.1658 - val_mean_absolut_error: 126.8531\n",
      "Time taken: 0:03:14.626667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████████████████████████████████▊     | 112/120 [6:22:13<26:05, 195.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 389ms/step - loss: 27083105.5762 - mean_absolut_error: 188.2963 - val_loss: 23427.1887 - val_mean_absolut_error: 116.9999\n",
      "Time taken: 0:03:14.801416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|████████████████████████████████████████████████████████████████████████▌    | 113/120 [6:25:29<22:50, 195.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 77174.5770 - mean_absolut_error: 77.4360 - val_loss: 5805.8701 - val_mean_absolut_error: 56.7347\n",
      "Time taken: 0:03:14.226778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████▏   | 114/120 [6:28:45<19:33, 195.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 2048, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: 65803.9417 - mean_absolut_error: 71.7026 - val_loss: 9447.6885 - val_mean_absolut_error: 68.6024\n",
      "Time taken: 0:03:13.912459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████████████████████████████████████████████████████████████████████▊   | 115/120 [6:32:00<16:17, 195.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 0, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: 906229.2528 - mean_absolut_error: 95.1046 - val_loss: 22565.4507 - val_mean_absolut_error: 111.0194\n",
      "Time taken: 0:03:13.755746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|██████████████████████████████████████████████████████████████████████████▍  | 116/120 [6:35:15<13:01, 195.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 1, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 386ms/step - loss: 261395.8358 - mean_absolut_error: 111.3787 - val_loss: 24056.6979 - val_mean_absolut_error: 118.1956\n",
      "Time taken: 0:03:13.389869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|███████████████████████████████████████████████████████████████████████████  | 117/120 [6:38:29<09:45, 195.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 2, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 193s 387ms/step - loss: 9541838.8716 - mean_absolut_error: 131.6977 - val_loss: 9156.1928 - val_mean_absolut_error: 78.9623\n",
      "Time taken: 0:03:13.872213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|███████████████████████████████████████████████████████████████████████████▋ | 118/120 [6:41:44<06:30, 195.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 3, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 540281.4248 - mean_absolut_error: 94.7323 - val_loss: 10440.1673 - val_mean_absolut_error: 92.6779\n",
      "Time taken: 0:03:14.662952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|████████████████████████████████████████████████████████████████████████████▎| 119/120 [6:45:00<03:15, 195.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.5, 'epochs': 1, 'first_neuron': 4096, 'hidden_layers': 4, 'leaky_alpha': 0.1, 'lr': 2, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'samples': 20000}\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Steps per Epoch: 500, Validation Steps: 125\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate for 125 steps\n",
      "500/500 [==============================] - 194s 388ms/step - loss: 5978981.6173 - mean_absolut_error: 118.5750 - val_loss: 19549.0384 - val_mean_absolut_error: 101.3556\n",
      "Time taken: 0:03:14.463941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 120/120 [6:48:16<00:00, 204.14s/it]\n"
     ]
    }
   ],
   "source": [
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "            t = ta.Scan(\n",
    "                x = dummy_x,\n",
    "                y = dummy_y,\n",
    "                model = gen_net,\n",
    "                params = hyper_parameter,\n",
    "                experiment_name = '{}'.format(Global.dataset),\n",
    "                #shuffle=False,\n",
    "                reduction_metric = Global.reduction_metric,\n",
    "                disable_progress_bar = False,\n",
    "                print_params = True,\n",
    "                clear_session = True,\n",
    "                save_weights = False\n",
    "            )\n",
    "        \n",
    "\n",
    "#t.data.to_csv(Global.target_dir + Global.results, index = True)\n",
    "t.data.to_csv(Global.target_dir + Global.results, index = True, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
