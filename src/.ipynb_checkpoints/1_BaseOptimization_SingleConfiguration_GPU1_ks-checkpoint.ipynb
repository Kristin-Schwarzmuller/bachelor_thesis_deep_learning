{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Base Training on GPU1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Links <a name = \"Top\"></a>\n",
    "<ol>\n",
    "    <li><p><a href = #setup>Set Up</a></p></li>\n",
    "    <li><p><a href = #Base>Base Training</a></p></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Conda Environment: tf_ks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "print('Current Conda Environment: {}'.format(os.environ['CONDA_DEFAULT_ENV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed version of TensorFlow 2.1.0 includes GPU support.\n",
      "\n",
      "Num GPUs Available:  2 \n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9789843453407631349\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9105744200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14465286960776645484\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9104897474\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17025568015009063669\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import talos as ta\n",
    "from talos.model import lr_normalizer, early_stopper, hidden_layers\n",
    "\n",
    "import tensorflow as tf\n",
    "  \n",
    "available_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "built_with_cuda = tf.test.is_built_with_cuda()\n",
    "\n",
    "if not (not available_gpus) & built_with_cuda:\n",
    "    print(\"The installed version of TensorFlow {} includes GPU support.\\n\".format(tf.__version__))\n",
    "    print(\"Num GPUs Available: \", len(available_gpus), \"\\n\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow {} does not include GPU support.\\n\".format(tf.__version__))\n",
    "    \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "from tensorflow.compat.v1.keras import callbacks, backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.compat.v1.Session(config = config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "\n",
    "* <i>SSD</i>, falls genug Speicher auf SSD im SymLink <i>fast_output</i> verfügbar ist\n",
    "* <i>HDD</i>, falls möglicherweise zu wenig SSD-Speicher verfügbar ist $\\rightarrow$ <i>output</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class OutputDirectory(IntEnum):\n",
    "    HDD = 0\n",
    "    SSD = 1\n",
    "    \n",
    "output_path = ['output', 'fast_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Label_Type into suitable label names.\n",
    "$\\Rightarrow$ Angular / Normalized $\\rightarrow$ ['Elevation', 'Azimuth']\n",
    "\n",
    "$\\Rightarrow$ Stereographic $\\rightarrow$ ['S_x', 'S_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Label_Names(label_type):\n",
    "    if label_type == 'Angular' or label_type == 'Normalized':\n",
    "        return ['Elevation', 'Azimuth']\n",
    "    elif label_type == 'Stereographic':\n",
    "        return ['S_x', 'S_y']\n",
    "    else:\n",
    "        assert(True, 'LabelType Invalid')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benutzerdefinierte Kostenfunktion & Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mse(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype = 'float32')\n",
    "    return K.mean(K.square(K.minimum(K.abs(y_pred - y_true), max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def circular_mae(y_true, y_pred):\n",
    "    max_error = tf.constant(360, dtype = 'float32')\n",
    "    return K.mean(K.minimum(K.abs(y_pred - y_true), K.abs(max_error - K.abs(y_pred - y_true))), axis = -1)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String into Reduction Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reduction_Metric(metric):\n",
    "    \n",
    "    if metric == 'custom_mae':\n",
    "        return [custom_mae]\n",
    "    elif metric == 'tf.keras.metrics.MeanAbsoluteError()':\n",
    "        return [tf.keras.metrics.MeanAbsoluteError()]\n",
    "    elif metric == 'circular_mae':\n",
    "        return [circular_mae]\n",
    "    elif metric == 'mean_squared_error':\n",
    "        return ['mean_squared_error']\n",
    "    else:\n",
    "        assert(False, 'Metric yet unknown - Please modify get_Reduction_Metric to meet your requirements')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Bottleneck-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bottleneck_features(train_generator, valid_generator, bottleneck):\n",
    "        \n",
    "    print('Creating bottleneck features...')\n",
    "    model = VGG16(include_top = False, weights = 'imagenet')\n",
    "        \n",
    "    #bottleneck_features_train = model.predict_generator(train_generator, train_generator.n // train_generator.batch_size)\n",
    "    bottleneck_features_train = model.predict(train_generator)\n",
    "    np.save(open(_LOG_DIR + 'Train_' + bottleneck, 'wb'), bottleneck_features_train)\n",
    "        \n",
    "    #bottleneck_features_valid = model.predict_generator(valid_generator, valid_generator.n // valid_generator.batch_size)\n",
    "    bottleneck_features_valid = model.predict(valid_generator)\n",
    "    np.save(open(_LOG_DIR + 'Valid_'  + bottleneck, 'wb'), bottleneck_features_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Datenpipeline (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch_size, num_samples, label_type):\n",
    "    #print(_CSV_FILE)\n",
    "    df = pd.read_csv(_CSV_FILE)\n",
    "    df_shuffled = df.sample(frac = 1, random_state = 1)\n",
    "    df_train = df_shuffled[0 : int(num_samples * 0.8 // batch_size * batch_size)]   \n",
    "    df_valid = df_shuffled.drop(df_shuffled.index[0: df_train.shape[0]])[0 : int(num_samples * 0.2 // batch_size * batch_size)]\n",
    "    \n",
    "    train_labels = df_train.drop(['Filename RGB'], axis = 1).values\n",
    "    valid_labels = df_valid.drop(['Filename RGB'], axis = 1).values\n",
    "    \n",
    "    bottleneck = 'Bottleneck_Features_{}_{}.npy'.format(str(num_samples), str(batch_size))                                       \n",
    "    if not os.path.exists(_LOG_DIR + 'Train_' + bottleneck):\n",
    "        \n",
    "        if _USE_DATA_AUGMENTATION:\n",
    "            train_data_generator = ImageDataGenerator(\n",
    "                rescale = 1./255, \n",
    "                width_shift_range = 0.1,\n",
    "                height_shift_range = 0.1, \n",
    "                zoom_range = 0.1,\n",
    "                brightness_range = (0.25, 0.75),\n",
    "                fill_mode = 'nearest'\n",
    "            )\n",
    "        else:\n",
    "            train_data_generator = ImageDataGenerator(\n",
    "                rescale = 1./255\n",
    "            )\n",
    "            \n",
    "        valid_data_generator = ImageDataGenerator(\n",
    "            rescale = 1./255\n",
    "        )\n",
    "    \n",
    "        print('Y-Col: {}'.format(get_Label_Names(label_type)))\n",
    "        \n",
    "        train_generator = train_data_generator.flow_from_dataframe(\n",
    "            dataframe = df_train,\n",
    "            directory = _IMAGE_DIR,\n",
    "            x_col = 'Filename RGB',\n",
    "            y_col = get_Label_Names(label_type),\n",
    "            class_mode = 'raw',\n",
    "            target_size = (224, 224),\n",
    "            color_mode = 'rgb',\n",
    "            shuffle = False,\n",
    "            seed = 1,\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "    \n",
    "        valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "            dataframe = df_valid,\n",
    "            directory = _IMAGE_DIR,\n",
    "            x_col = 'Filename RGB',\n",
    "            y_col = get_Label_Names(label_type),\n",
    "            class_mode = 'raw',\n",
    "            target_size = (224, 224),\n",
    "            color_mode = 'rgb',\n",
    "            shuffle = False,\n",
    "            seed = 1,\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "                                                        \n",
    "        create_bottleneck_features(train_generator, valid_generator, bottleneck)\n",
    "        \n",
    "    train_features = np.load(open(_LOG_DIR + 'Train_' + bottleneck, 'rb'))\n",
    "    valid_features = np.load(open(_LOG_DIR + 'Valid_' + bottleneck, 'rb'))\n",
    "                                                        \n",
    "    return train_labels, valid_labels, train_features, valid_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Modell (Angepasst für Talos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_model_base(x, y, x_val, y_val, params):\n",
    "    \n",
    "    global _COUNTER    \n",
    "    K.clear_session()\n",
    "\n",
    "    train_labels, valid_labels, train_features, valid_features = prepare_data(params['batch_size'], params['samples'], params['label_type'])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = train_features.shape[1 :])) # (7, 7, 512)\n",
    "    \n",
    "    dropout_rate = params['dropout']\n",
    "    first_neuron = params['first_neuron']\n",
    "    \n",
    "    if params['activation'] == 'leakyrelu':\n",
    "        activation_layer = LeakyReLU(alpha = 0.1)\n",
    "    elif params['activation'] == 'relu':\n",
    "        activation_layer = ReLU()  \n",
    "    \n",
    "    model.add(Dense(units = first_neuron, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "    model.add(activation_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        model.add(Dropout(rate = dropout_rate))\n",
    "        \n",
    "    hidden_neuron_fraction = first_neuron\n",
    "    for i in range(params['hidden_layers']):\n",
    "        hidden_neuron_fraction = hidden_neuron_fraction // 2\n",
    "        model.add(Dense(units = hidden_neuron_fraction, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "        model.add(activation_layer)\n",
    "        if dropout_rate > 0.0:\n",
    "            model.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    model.add(Dense(units = 2, kernel_initializer = glorot_uniform(seed = 1)))\n",
    "\n",
    "    print('Using Loss: {} \\nand Reduction Metric: {}'.format(\n",
    "        params['loss_function'], \n",
    "        get_Reduction_Metric(params['reduction_metric'])))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = params['optimizer'](lr = lr_normalizer(params['lr'], params['optimizer'])), \n",
    "        loss = params['loss_function'], \n",
    "        metrics = get_Reduction_Metric(params['reduction_metric'])\n",
    "    )\n",
    "    \n",
    "    print('Monitor: {}'.format(params['monitor_value']))\n",
    "    \n",
    "    checkpointer = callbacks.ModelCheckpoint(\n",
    "        filepath = _LOG_DIR + 'Best_Weights_FC_{}.hdf5'.format(_COUNTER),\n",
    "        monitor = params['monitor_value'],\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    out = model.fit(\n",
    "        x = train_features,\n",
    "        y = train_labels,\n",
    "        epochs = params['epochs'],\n",
    "        validation_data = (valid_features, valid_labels),\n",
    "        steps_per_epoch = int(params['samples'] * 0.8 // params['batch_size']),\n",
    "        validation_steps = int(params['samples'] * 0.2 // params['batch_size']),\n",
    "        callbacks = [checkpointer]\n",
    "    )\n",
    "    print(\"Time taken:\", datetime.now() - startTime)\n",
    "    \n",
    "    _COUNTER = _COUNTER + 1\n",
    "     \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter <a name = \"setup\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSerach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Adam = RMSprop + Momentum (lr=0.001)\n",
    "#     Nadam = Adam RMSprop + Nesterov-Momentum (lr=0.002)\n",
    "#     RMSprop = (lr=0.001)\n",
    "#     SGD = (lr=0.01)\n",
    "#     Adagrad\n",
    "\n",
    "hyper_parameter = {\n",
    "    'samples': [20000],\n",
    "    'epochs': [1],\n",
    "    'batch_size': [32, 64],\n",
    "    'optimizer': [Adam],\n",
    "    'lr': [1, 2, 5],\n",
    "    'first_neuron': [1024, 2048, 4096],\n",
    "    'dropout': [0.25, 0.50],\n",
    "    'activation': ['leakyrelu', 'relu'],\n",
    "    'hidden_layers': [0, 1, 2, 3, 4],\n",
    "    # beginning from here, Values should only contain a single entry:\n",
    "    # ===============================================================\n",
    "    'label_type': ['Angular'], # Stereographic, Angular, Normalized\n",
    "    'loss_function': ['mean_squared_error'], # circular_mse\n",
    "    'reduction_metric': ['custom_mae'], # tf.keras.metrics.MeanAbsoluteError(), circular_mae, custom_mae, mean_squared_error\n",
    "    'monitor_value': ['val_custom_mae'] # val_custom_mae, val_mean_absolute_error, val_circular_mae, val_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():  \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dateisystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RUN = 'SYNTH'\n",
    "_LOSS = 'MSE'\n",
    "_DATASET_NAME = '201019_2253_final'#'2020-05-28'\n",
    "_DEVICE = 'GeForce_RTX_2080_Ti' #\"/device:GPU:1\" #'TITAN_GPU1'\n",
    "\n",
    "storage = OutputDirectory.SSD # 'fast_output' if ssd storage may suffice, 'output' otherwise\n",
    "\n",
    "#APPENDIX = 'Stereographic'\n",
    "\n",
    "#FUNCTION_OVERRIDE = ['mean_squared_error', [custom_mae], 'val_custom_mae'] # None, or e. g. ['mean_squared_error', [circular_mae], 'val_circular_mae']\n",
    "\n",
    "if hyper_parameter['label_type'][0] == 'Stereographic':\n",
    "    _CSV_FILE_NAME = 'images_synthetisch_stereographic.csv'\n",
    "    _STEREOGRAPHIC = True\n",
    "elif hyper_parameter['label_type'][0] == 'Angular':\n",
    "    _CSV_FILE_NAME = 'labels_ks.csv' #'images_synthetisch.csv'\n",
    "    _STEREOGRAPHIC = False\n",
    "elif hyper_parameter['label_type'][0] == 'Normalized':\n",
    "    _CSV_FILE_NAME = 'images_synthetisch_normalized.csv'\n",
    "    _STEREOGRAPHIC = False\n",
    "else:\n",
    "    assert(True, 'LabelType Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_USE_DATA_AUGMENTATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODEL_NAME = '{}_Optimierung_Hyperparameter_v{}'.format(_DATASET_NAME, _RUN)\n",
    "_IMAGE_DIR = '..\\\\..\\\\data_generation\\\\dataset\\\\{}\\\\'.format(_DATASET_NAME) # '..\\\\dataset\\\\{}\\\\'.format(_DATASET_NAME)\n",
    "_CSV_FILE = _IMAGE_DIR + _CSV_FILE_NAME\n",
    "\n",
    "_COUNTER = 0\n",
    "\n",
    "_note = '_Custom-MAE'\n",
    "\n",
    "_NET_DIR = '{}_Regression_{}\\\\{}_{}_Base{}\\\\'.format(_RUN, _LOSS, _DATASET_NAME, hyper_parameter['label_type'][0], _note)\n",
    "_LOG_DIR = '..\\\\{}\\\\{}'.format(output_path[storage], _NET_DIR)\n",
    "\n",
    "_RESULTS_FILE = '\\\\..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, hyper_parameter['label_type'][0], _note)\n",
    "\n",
    "if(not os.path.exists(_LOG_DIR)):\n",
    "    os.makedirs(_LOG_DIR)\n",
    "else:\n",
    "    input('Directory >>| {} |<< existiert bereits. Fortsetzen auf eigene Gefahr! (Weiter mit Enter)'.format(_LOG_DIR))\n",
    "\n",
    "device_file = open(_LOG_DIR + '{}'.format(_DEVICE), \"a+\")\n",
    "device_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ausführung GridSearch mit Talos <a name = \"Base\"></a><a href = #Top>Up</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/360 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Y-Col: ['Elevation', 'Azimuth']\n",
      "Found 16000 validated image filenames.\n",
      "Found 4000 validated image filenames.\n",
      "Creating bottleneck features...\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000188A541FC18>]\n",
      "Monitor: val_custom_mae\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "15840/16000 [============================>.] - ETA: 0s - loss: 3808.9903 - custom_mae: 39.1295\n",
      "Epoch 00001: val_custom_mae improved from inf to 31.90724, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Base_Custom-MAE\\Best_Weights_FC_0.hdf5\n",
      "16000/16000 [==============================] - 5s 333us/sample - loss: 3799.5094 - custom_mae: 39.0489 - val_loss: 2938.1041 - val_custom_mae: 31.9072\n",
      "Time taken: 0:00:07.004152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                               | 1/360 [01:33<9:19:54, 93.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000188A541FC18>]\n",
      "Monitor: val_custom_mae\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "15744/16000 [============================>.] - ETA: 0s - loss: 3727.3012 - custom_mae: 38.5636\n",
      "Epoch 00001: val_custom_mae improved from inf to 32.56684, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Base_Custom-MAE\\Best_Weights_FC_1.hdf5\n",
      "16000/16000 [==============================] - 4s 273us/sample - loss: 3709.7922 - custom_mae: 38.4279 - val_loss: 3074.7211 - val_custom_mae: 32.5668\n",
      "Time taken: 0:00:05.864313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                               | 2/360 [01:41<6:44:20, 67.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 0, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000188A541FC18>]\n",
      "Monitor: val_custom_mae\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "15744/16000 [============================>.] - ETA: 0s - loss: 3831.2588 - custom_mae: 38.8275\n",
      "Epoch 00001: val_custom_mae improved from inf to 31.58516, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Base_Custom-MAE\\Best_Weights_FC_2.hdf5\n",
      "16000/16000 [==============================] - 4s 273us/sample - loss: 3809.6392 - custom_mae: 38.6761 - val_loss: 2971.0649 - val_custom_mae: 31.5852\n",
      "Time taken: 0:00:05.862015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                               | 3/360 [01:48<4:55:41, 49.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000188A541FC18>]\n",
      "Monitor: val_custom_mae\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "15968/16000 [============================>.] - ETA: 0s - loss: 3967.5240 - custom_mae: 40.7691\n",
      "Epoch 00001: val_custom_mae improved from inf to 32.05215, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Base_Custom-MAE\\Best_Weights_FC_3.hdf5\n",
      "16000/16000 [==============================] - 5s 284us/sample - loss: 3966.9972 - custom_mae: 40.7551 - val_loss: 2944.0792 - val_custom_mae: 32.0522\n",
      "Time taken: 0:00:06.034188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                               | 4/360 [01:56<3:40:08, 37.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000188A541FC18>]\n",
      "Monitor: val_custom_mae\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "15680/16000 [============================>.] - ETA: 0s - loss: 3952.8707 - custom_mae: 40.7421\n",
      "Epoch 00001: val_custom_mae improved from inf to 29.60960, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Base_Custom-MAE\\Best_Weights_FC_4.hdf5\n",
      "16000/16000 [==============================] - 5s 289us/sample - loss: 3924.4756 - custom_mae: 40.5453 - val_loss: 2620.7675 - val_custom_mae: 29.6096\n",
      "Time taken: 0:00:06.122669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█                                                                               | 5/360 [02:04<2:47:31, 28.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 1, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 5, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000188A541FC18>]\n",
      "Monitor: val_custom_mae\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "15776/16000 [============================>.] - ETA: 0s - loss: 4654.4504 - custom_mae: 43.3232\n",
      "Epoch 00001: val_custom_mae improved from inf to 35.27819, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Base_Custom-MAE\\Best_Weights_FC_5.hdf5\n",
      "16000/16000 [==============================] - 5s 298us/sample - loss: 4635.3034 - custom_mae: 43.2185 - val_loss: 3196.0328 - val_custom_mae: 35.2782\n",
      "Time taken: 0:00:06.303649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▎                                                                              | 6/360 [02:12<2:11:13, 22.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 1, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000188A541FC18>]\n",
      "Monitor: val_custom_mae\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "15936/16000 [============================>.] - ETA: 0s - loss: 4162.9268 - custom_mae: 42.7435\n",
      "Epoch 00001: val_custom_mae improved from inf to 31.43874, saving model to ..\\fast_output\\SYNTH_Regression_MSE\\201019_2253_final_Angular_Base_Custom-MAE\\Best_Weights_FC_6.hdf5\n",
      "16000/16000 [==============================] - 5s 297us/sample - loss: 4160.5934 - custom_mae: 42.7122 - val_loss: 2816.3410 - val_custom_mae: 31.4387\n",
      "Time taken: 0:00:06.295776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▌                                                                              | 7/360 [02:20<1:45:45, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'leakyrelu', 'batch_size': 32, 'dropout': 0.25, 'epochs': 1, 'first_neuron': 1024, 'hidden_layers': 2, 'label_type': 'Angular', 'loss_function': 'mean_squared_error', 'lr': 2, 'monitor_value': 'val_custom_mae', 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 'reduction_metric': 'custom_mae', 'samples': 20000}\n",
      "Using Loss: mean_squared_error \n",
      "and Reduction Metric: [<function custom_mae at 0x00000188A541FC18>]\n",
      "Monitor: val_custom_mae\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      " 3264/16000 [=====>........................] - ETA: 3s - loss: 5868.7045 - custom_mae: 53.4883"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dummy_x = np.empty((1, 2, 3, 224, 224))\n",
    "dummy_y = np.empty((1, 2))\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    \n",
    "        t = ta.Scan(\n",
    "            x = dummy_x,\n",
    "            y = dummy_y,\n",
    "            model = grid_model_base,\n",
    "            params = hyper_parameter,\n",
    "            experiment_name = '{}'.format(_DATASET_NAME),\n",
    "            #shuffle=False,\n",
    "            reduction_metric = hyper_parameter['reduction_metric'][0],\n",
    "            disable_progress_bar = False,\n",
    "            print_params = True,\n",
    "            clear_session = True,\n",
    "            save_weights = False\n",
    "        )\n",
    "        \n",
    "\n",
    "t.data.to_csv(_LOG_DIR + _RESULTS_FILE, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy best 10 Results to NAS if SSD Directory was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = _LOG_DIR + '..\\\\{}_{}_Base{}_Results.csv'.format(_DATASET_NAME, hyper_parameter['label_type'][0], _note)\n",
    "df = pd.read_csv(best_results).drop(columns = ['round_epochs', 'samples', 'epochs'], axis = 0)\n",
    "df = df.sort_values(hyper_parameter['monitor_value'][0], axis = 0, ascending = True, inplace = False, kind = 'quicksort', na_position = 'last')\n",
    "print('Displaying: {}'.format(best_results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_base_directory(src, dst, data, symlinks = False, ignore = None):\n",
    "    maxLen = 0\n",
    "    message = ''\n",
    "    \n",
    "    networks_to_copy = data.head(10).index\n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        \n",
    "        message = 'Creating Path: {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        os.makedirs(dst)\n",
    "        \n",
    "    for item in os.listdir(src):\n",
    "        \n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        \n",
    "        if os.path.isdir(s):\n",
    "            \n",
    "            message = 'Copying Directory: {}'.format(s)\n",
    "            maxLen = max(maxLen, len(message))\n",
    "            print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "            \n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if not os.path.exists(d): #or os.stat(s).st_mtime - os.stat(d).st_mtime > 1:\n",
    "                for idx in networks_to_copy:\n",
    "                    net = '_FC_{}.hdf5'.format(idx)\n",
    "                    if net in item or '_Bottleneck_' in item or '_GPU' in item:\n",
    "                        message = 'Copying File: {}'.format(s)\n",
    "                        maxLen = max(maxLen, len(message))\n",
    "                        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "                \n",
    "                        shutil.copy2(s, d)\n",
    "        \n",
    "        time.sleep(.1)    \n",
    "    \n",
    "    src_csv = src + _RESULTS_FILE\n",
    "    dst_csv = dst + _RESULTS_FILE\n",
    "    if not os.path.exists(dst_csv):\n",
    "        message = 'Copying File: {}'.format(src_csv)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        shutil.copy2(src_csv, dst_csv)\n",
    "     \n",
    "    message = 'Coyping... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "\n",
    "    \n",
    "def delete_directory(src, terminator = '\\n'):\n",
    "    message = ''\n",
    "    maxLen = 0\n",
    "    \n",
    "    try:\n",
    "        message = 'Deleting {}'.format(src)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\r')\n",
    "        \n",
    "        if os.path.isdir(src):\n",
    "            shutil.rmtree(src)\n",
    "            \n",
    "        elif os.path.isfile(src):\n",
    "            os.remove(src)\n",
    "        \n",
    "    except OSError as e:\n",
    "        message = 'Error: {} : {}'.format(src, e.strerror)\n",
    "        maxLen = max(maxLen, len(message))\n",
    "        print(message + ' ' * (maxLen - len(message)), end = '\\n')\n",
    "        return\n",
    "    \n",
    "    message = 'Deleting... Done'\n",
    "    maxLen = max(maxLen, len(message))\n",
    "    print(message + ' ' * (maxLen - len(message)), end = terminator)\n",
    "\n",
    "    \n",
    "def copy_base_training(src, dst):\n",
    "    copy_base_directory(src, dst, df)\n",
    "    \n",
    "    delete_directory(src, terminator = '\\r')\n",
    "    delete_directory(src + _RESULTS_FILE, terminator = '\\r')\n",
    "    if not os.listdir(src + '..\\\\'):\n",
    "        delete_directory(src + '..\\\\', terminator = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(storage == OutputDirectory.SSD):\n",
    "    _COPY_DIR = '..\\\\output\\\\{}'.format(_NET_DIR)\n",
    "    copy_base_training(_LOG_DIR, _COPY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_ks]",
   "language": "python",
   "name": "conda-env-tf_ks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
